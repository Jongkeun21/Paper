{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 layers, pre-trained weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from PIL import Image as PI\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1111 20:42:35.582975  5752 deprecation.py:323] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++ DONE ++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess_ :\n",
    "    saver = tf.train.import_meta_graph('D:/Backup_data/jongkeun/paper/190524/save_dir-494.meta')\n",
    "    saver.restore(sess_, tf.train.latest_checkpoint('D:/Backup_data/jongkeun/paper/190524/'))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "#     print(graph.get_tensor_by_name('layer0_conv2d_1/kernel:0'))\n",
    "    weight0_1, weight0_2 = sess_.run([graph.get_tensor_by_name('layer0_conv2d_1/kernel:0'), \n",
    "                                      graph.get_tensor_by_name('layer0_conv2d_2/kernel:0')])\n",
    "    weight1, weight1_1, weight1_2 = sess_.run([graph.get_tensor_by_name('layer1_conv2d/kernel:0'), \n",
    "                                               graph.get_tensor_by_name('layer1_conv2d_1/kernel:0'),\n",
    "                                               graph.get_tensor_by_name('layer1_conv2d_2/kernel:0')])\n",
    "    weight2, weight2_1, weight2_2 = sess_.run([graph.get_tensor_by_name('layer2_conv2d/kernel:0'), \n",
    "                                               graph.get_tensor_by_name('layer2_conv2d_1/kernel:0'),\n",
    "                                               graph.get_tensor_by_name('layer2_conv2d_2/kernel:0')])\n",
    "    weight3, weight3_1, weight3_2 = sess_.run([graph.get_tensor_by_name('layer3_conv2d/kernel:0'), \n",
    "                                               graph.get_tensor_by_name('layer3_conv2d_1/kernel:0'),\n",
    "                                               graph.get_tensor_by_name('layer3_conv2d_2/kernel:0')])\n",
    "    \n",
    "    \n",
    "#     weight_ = weight[:,:,:,:]\n",
    "#     print(weight_)\n",
    "\n",
    "print(\"++++++++++++++++++ DONE ++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 500\n",
    "batch_size = 20\n",
    "image_resize = 128\n",
    "image_width = image_resize\n",
    "image_height = image_resize\n",
    "initial_rate = 1e-5\n",
    "label_len = 3\n",
    "\n",
    "home_dir = \"D:/Backup_data/jongkeun\"\n",
    "\n",
    "image_file = \"/dir_images/190819_newTraining/\"\n",
    "test_image_file = \"/dir_images/190309_new_test/\"\n",
    "\n",
    "label_file = \"/csv/190819_newTraining.csv\"\n",
    "test_label_file = \"/csv/190309_new_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9182\n",
      "1018\n"
     ]
    }
   ],
   "source": [
    "label_dir = home_dir + label_file\n",
    "test_label_dir = home_dir + test_label_file\n",
    "\n",
    "# label_dir = os.getcwd() + \"/csv/181126.csv\"\n",
    "# test_label_dir = os.getcwd() + \"/csv/181126_test.csv\"\n",
    "\n",
    "f_label = open(label_dir, 'r')\n",
    "f_test_label = open(test_label_dir, 'r')\n",
    "\n",
    "label_data = f_label.readlines()\n",
    "test_label_data = f_test_label.readlines()\n",
    "\n",
    "f_label.close()\n",
    "f_test_label.close()\n",
    "\n",
    "label_array = np.array(np.reshape(label_data, [-1, label_len]), dtype=np.int32)\n",
    "test_label_array = np.array(np.reshape(test_label_data, [-1, label_len]), dtype=np.int32)\n",
    "\n",
    "print(len(label_array))\n",
    "print(len(test_label_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9182\n",
      "1018\n"
     ]
    }
   ],
   "source": [
    "image_dir = home_dir + image_file\n",
    "test_image_dir = home_dir + test_image_file\n",
    "\n",
    "# image_dir = os.getcwd() + \"/dir_images/181126/\"\n",
    "# test_image_dir = os.getcwd() + \"/dir_images/181126_test/\"\n",
    "\n",
    "image_list = os.listdir(image_dir)\n",
    "test_image_list = os.listdir(test_image_dir)\n",
    "\n",
    "image_list = sorted(image_list, key=lambda x: (int(re.sub('\\D','',x)),x))\n",
    "test_image_list = sorted(test_image_list, key=lambda y: (int(re.sub('\\D','',y)),y))\n",
    "\n",
    "image_array = []\n",
    "test_image_array = []\n",
    "\n",
    "for i in range(len(image_list)) :\n",
    "    image_list[i] = image_dir + image_list[i]\n",
    "    image_array.append(np.array(PI.open(image_list[i]).resize((image_resize, image_resize)).convert('L')))\n",
    "\n",
    "for j in range(len(test_image_list)) :\n",
    "    test_image_list[j] = test_image_dir + test_image_list[j]\n",
    "    test_image_array.append(np.array(PI.open(test_image_list[j]).resize((image_resize, image_resize)).convert('L')))\n",
    "\n",
    "image_array = np.reshape(image_array, [-1, image_width*image_height])\n",
    "test_image_array = np.reshape(test_image_array, [-1, image_width*image_height])\n",
    "\n",
    "image_array = np.array(np.reshape(image_array, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "test_image_array = np.array(np.reshape(test_image_array, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "\n",
    "print(len(image_array))\n",
    "print(len(test_image_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, image_width, image_height, 1], name='input_image')\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[None, label_len], name='input_label')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "\n",
    "# W_pre_trained = tf.placeholder(dtype=tf.float32, shape=[3,3,8,128], name='pre_trained_weight')\n",
    "\n",
    "is_training = tf.placeholder(dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h0_1 = tf.placeholder(dtype=tf.float32, shape=[3,3,1,8], name='W_h0_1')\n",
    "W_h0_2 = tf.placeholder(dtype=tf.float32, shape=[3,3,8,128], name='W_h0_2')\n",
    "\n",
    "W_h1 = tf.placeholder(dtype=tf.float32, shape=[3,3,128,256], name='W_h1')\n",
    "W_h1_1 = tf.placeholder(dtype=tf.float32, shape=[3,3,256, 128], name='W_h1_1')\n",
    "W_h1_2 = tf.placeholder(dtype=tf.float32, shape=[3,3,128,256], name='W_h1_2')\n",
    "\n",
    "W_h2 = tf.placeholder(dtype=tf.float32, shape=[3,3,256,512], name='W_h2')\n",
    "W_h2_1 = tf.placeholder(dtype=tf.float32, shape=[3,3,512,256], name='W_h2_1')\n",
    "W_h2_2 = tf.placeholder(dtype=tf.float32, shape=[3,3,256,512], name='W_h2_2')\n",
    "\n",
    "W_h3 = tf.placeholder(dtype=tf.float32, shape=[3,3,512,512], name='W_h3')\n",
    "W_h3_1 = tf.placeholder(dtype=tf.float32, shape=[3,3,512,1024], name='W_h3_1')\n",
    "W_h3_2 = tf.placeholder(dtype=tf.float32, shape=[3,3,1024,512], name='W_h3_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1111 20:46:18.505731  5752 deprecation.py:323] From <ipython-input-8-23d39681c4d3>:2: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W1111 20:46:18.633391  5752 deprecation.py:323] From <ipython-input-8-23d39681c4d3>:3: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W1111 20:46:18.701236  5752 deprecation.py:323] From <ipython-input-8-23d39681c4d3>:8: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W1111 20:46:18.703224  5752 deprecation.py:506] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "def pooling(input_, dropout, is_training) :\n",
    "    pooling = tf.layers.max_pooling2d(input_, [2, 2], [2, 2], padding='SAME')\n",
    "    pooling = tf.layers.dropout(pooling, dropout, is_training)\n",
    "    \n",
    "    return pooling\n",
    "\n",
    "def learning_layer(input_data, size, dropout, is_training) :\n",
    "    layer = tf.layers.conv2d(input_data, size, [3, 3], padding='SAME')\n",
    "    layer = tf.layers.max_pooling2d(layer, [2, 2], [2, 2], padding='SAME')\n",
    "    layer = tf.layers.dropout(layer, dropout, is_training)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def convolutional(input_, weight_) :\n",
    "    conv_ = tf.nn.conv2d(input_, weight_, strides=[1,1,1,1], padding='SAME')\n",
    "    hidden_ = tf.nn.relu(conv_)\n",
    "    \n",
    "    return hidden_\n",
    "\n",
    "with tf.device('/gpu:0') :\n",
    "    layer0_1 = convolutional(x, W_h0_1)\n",
    "    layer0_2 = convolutional(layer0_1, W_h0_2)\n",
    "    layer1 = convolutional(layer0_2, W_h1)\n",
    "    layer1_ = pooling(layer1, dropout=0.7, is_training=is_training)\n",
    "    \n",
    "    layer1_1 = convolutional(layer1_, W_h1_1)\n",
    "    layer1_2 = convolutional(layer1_1, W_h1_2)\n",
    "    layer2 = convolutional(layer1_2, W_h2)\n",
    "    layer2_ = pooling(layer2, dropout=0.7, is_training=is_training)\n",
    "    \n",
    "    layer2_1 = convolutional(layer2_, W_h2_1)\n",
    "    layer2_2 = convolutional(layer2_1, W_h2_2)\n",
    "    layer2_3 = convolutional(layer2_2, W_h3)\n",
    "    layer2_4 = convolutional(layer2_3, W_h3_1)\n",
    "    layer2_5 = convolutional(layer2_4, W_h3_2)\n",
    "    layer3 = learning_layer(input_data=layer2_5, size=1024, dropout=0.7, is_training=is_training)\n",
    "    \n",
    "    layer3_1 = tf.layers.conv2d(layer3, 128, [3,3], padding='SAME')\n",
    "    layer3_2 = tf.layers.conv2d(layer3_1, 512, [3,3], padding='SAME')\n",
    "    layer3_3 = tf.layers.conv2d(layer3_2, 256, [3,3], padding='SAME')\n",
    "    layer3_4 = tf.layers.conv2d(layer3_3, 1024, [3,3], padding='SAME')\n",
    "    layer3_5 = tf.layers.conv2d(layer3_4, 128, [3,3], padding='SAME')\n",
    "    layer3_6 = tf.layers.conv2d(layer3_5, 512, [3,3], padding='SAME')\n",
    "    layer3_7 = tf.layers.conv2d(layer3_6, 256, [3,3], padding='SAME')\n",
    "    layer3_8 = tf.layers.conv2d(layer3_7, 1024, [3,3], padding='SAME')\n",
    "    layer4 = learning_layer(input_data=layer3_8, size=1024, dropout=0.7, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1111 20:46:22.363440  5752 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1111 20:46:22.364433  5752 deprecation.py:323] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W1111 20:46:22.591835  5752 deprecation.py:323] From <ipython-input-9-f9981dd3a3a2>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "fully_layer = tf.contrib.layers.flatten(layer4)\n",
    "fully_layer = tf.layers.dense(fully_layer, 1024, activation=tf.nn.relu, name='fully_connected_layer')\n",
    "fully_layer = tf.layers.dropout(fully_layer, 0.5, is_training, name='fully_connected_dropout')\n",
    "\n",
    "fully_layer2 = tf.layers.dense(fully_layer, 512, activation=tf.nn.relu, name='fully_connected_layer2')\n",
    "fully_layer2 = tf.layers.dropout(fully_layer2, 0.5, is_training, name='fully_connected_dropout2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fully_layer2, label_len, activation=None)\n",
    "\n",
    "with tf.name_scope('pred') : \n",
    "    pred = tf.nn.softmax(logits)\n",
    "with tf.name_scope('loss') :\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_))\n",
    "    \n",
    "with tf.device('/gpu:0') :\n",
    "    with tf.name_scope('train') :\n",
    "            train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "label_output = tf.argmax(pred, 1)\n",
    "label_input = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: the launch timed out and was terminated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4cb70f9ea11e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcoord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mthread\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_queue_runners\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \"\"\"\n\u001b[1;32m-> 1570\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: the launch timed out and was terminated"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(log_device_placement = True, allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess :\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_batch = int(len(image_list) / batch_size)\n",
    "    correct_list = []\n",
    "    incorrect_list = []\n",
    "    cost_list = []\n",
    "    cost_sum_list = []\n",
    "    pred_list = []\n",
    "    total_acc = 0\n",
    "    cost_sum = 0\n",
    "    cost_flag = 0\n",
    "    rate_ = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for epoch in range(training_epochs) :\n",
    "        total_cost = 0\n",
    "        \n",
    "        if epoch%50 == 49 :\n",
    "                rate_ *= 0.8\n",
    "                \n",
    "        if epoch%10 == 9 :\n",
    "#             rate_ *= 0.8\n",
    "            midpoint = int(time.time() - start_time)\n",
    "        \n",
    "            print(\"\")\n",
    "            print(\"=======================================================================================\")\n",
    "            print('{:03d}:{:02d}:{:02d}'.format(midpoint//3600, (midpoint%3600//60), midpoint%60))\n",
    "            print(\"=======================================================================================\")\n",
    "            print(\"\")\n",
    "        \n",
    "        batch_index = np.random.choice(len(image_array), total_batch, replace=False)\n",
    "                \n",
    "        for i in range(total_batch) :                \n",
    "            _, _loss = sess.run([train, loss], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]], \n",
    "                                                          is_training: True, learning_rate: initial_rate*rate_,\n",
    "                                                          W_h0_1: weight0_1, W_h0_2: weight0_2,\n",
    "                                                          W_h1: weight1, W_h1_1: weight1_1, W_h1_2: weight1_2,\n",
    "                                                          W_h2: weight2, W_h2_1: weight2_1, W_h2_2: weight2_2,\n",
    "                                                          W_h3: weight3, W_h3_1: weight3_1, W_h3_2: weight3_2})                 \n",
    "            total_cost += _loss\n",
    "\n",
    "        avg_cost = total_cost/total_batch\n",
    "        print('Epoch : ', '%4d' % (epoch + 1), '    Avg. cost = ', '{:.4f}'.format(avg_cost))            \n",
    "        \n",
    "#         train_writer.add_summary(_merge, epoch)\n",
    "               \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"================================     Training done     ================================\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    for test in range(len(test_image_array)) :\n",
    "        _acc, _label_input, _label_output, _pred = sess.run([acc, label_input, label_output, pred], \n",
    "                             feed_dict={x: test_image_array[[test]], y_: test_label_array[[test]], is_training: False,\n",
    "                                        W_h0_1: weight0_1, W_h0_2: weight0_2,\n",
    "                                        W_h1: weight1, W_h1_1: weight1_1, W_h1_2: weight1_2,\n",
    "                                        W_h2: weight2, W_h2_1: weight2_1, W_h2_2: weight2_2,\n",
    "                                        W_h3: weight3, W_h3_1: weight3_1, W_h3_2: weight3_2})\n",
    "\n",
    "        total_acc += _acc\n",
    "        pred_list.append(list(_pred[0]))\n",
    "        \n",
    "        print(\"n: \", test, \"    label_input: \", _label_input, \"    label_output: \", _label_output)\n",
    "        \n",
    "        if _label_input == _label_output :\n",
    "            correct_list.append(test)   \n",
    "        else :\n",
    "            incorrect_list.append(test)\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"          TOTAL ACC    : \", '{:.5f}'.format(total_acc / len(test_image_array)))\n",
    "    print(\"\")\n",
    "    print(pred_list)\n",
    "    \n",
    "    end_time = int(time.time() - start_time)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print('{:03d}:{:02d}:{:02d}'.format(end_time//3600, (end_time%3600//60), end_time%60))\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
