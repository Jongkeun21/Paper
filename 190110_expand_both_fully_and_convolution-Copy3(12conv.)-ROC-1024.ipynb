{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from PIL import Image as PI\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    LABEL\n",
    "        0 -> NORMAL (3,319)\n",
    "        1 -> PNEUMOTHORAX (2,084)\n",
    "        2 -> PNEUMONIA (4,277)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 500\n",
    "batch_size = 20\n",
    "image_resize = 1024\n",
    "image_width = image_resize\n",
    "image_height = image_resize\n",
    "initial_rate = 1e-5\n",
    "label_len = 3\n",
    "\n",
    "home_dir = \"D:/Backup_data/jongkeun\"\n",
    "\n",
    "image_file = \"/dir_images/190819_newTraining/\"\n",
    "test_image_file = \"/dir_images/190309_new_test/\"\n",
    "\n",
    "label_file = \"/csv/190819_newTraining.csv\"\n",
    "test_label_file = \"/csv/190309_new_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9182\n",
      "1018\n"
     ]
    }
   ],
   "source": [
    "label_dir = home_dir + label_file\n",
    "test_label_dir = home_dir + test_label_file\n",
    "\n",
    "# label_dir = os.getcwd() + \"/csv/181126.csv\"\n",
    "# test_label_dir = os.getcwd() + \"/csv/181126_test.csv\"\n",
    "\n",
    "f_label = open(label_dir, 'r')\n",
    "f_test_label = open(test_label_dir, 'r')\n",
    "\n",
    "label_data = f_label.readlines()\n",
    "test_label_data = f_test_label.readlines()\n",
    "\n",
    "f_label.close()\n",
    "f_test_label.close()\n",
    "\n",
    "label_array = np.array(np.reshape(label_data, [-1, label_len]), dtype=np.int32)\n",
    "test_label_array = np.array(np.reshape(test_label_data, [-1, label_len]), dtype=np.int32)\n",
    "\n",
    "print(len(label_array))\n",
    "print(len(test_label_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image, test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ec95bf20589d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mimage_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mimage_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_resize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_resize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#         image_array.append(np.array(PI.open(image_list[i]).convert('L')))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_dir = home_dir + image_file\n",
    "test_image_dir = home_dir + test_image_file\n",
    "\n",
    "image_list = os.listdir(image_dir)\n",
    "test_image_list = os.listdir(test_image_dir)\n",
    "\n",
    "image_list = sorted(image_list, key=lambda x: (int(re.sub('\\D','',x)),x))\n",
    "test_image_list = sorted(test_image_list, key=lambda y: (int(re.sub('\\D','',y)),y))\n",
    "\n",
    "image_array = []\n",
    "test_image_array = []\n",
    "\n",
    "with tf.device('/gpu:0') :\n",
    "    for i in range(len(image_list)) :\n",
    "        image_list[i] = image_dir + image_list[i]\n",
    "        image_array.append(np.array(PI.open(image_list[i]).resize((image_resize, image_resize)).convert('L')))\n",
    "#         image_array.append(np.array(PI.open(image_list[i]).convert('L')))\n",
    "\n",
    "    for j in range(len(test_image_list)) :\n",
    "        test_image_list[j] = test_image_dir + test_image_list[j]\n",
    "        test_image_array.append(np.array(PI.open(test_image_list[j]).resize((image_resize, image_resize)).convert('L')))\n",
    "#         test_image_array.append(np.array(PI.open(test_image_list[j]).convert('L')))\n",
    "\n",
    "    image_reshped = np.reshape(image_array, [-1, image_width*image_height])\n",
    "    test_image_reshaped = np.reshape(test_image_array, [-1, image_width*image_height])\n",
    "\n",
    "    image_array = np.array(np.reshape(image_reshped, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "    test_image_array = np.array(np.reshape(test_image_reshaped, [-1, image_width, image_height, 1]), dtype=np.float32)\n",
    "\n",
    "print(len(image_array))\n",
    "print(len(test_image_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feeding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, image_width, image_height, 1], name='input_image')\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[None, label_len], name='input_label')\n",
    "learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "\n",
    "is_training = tf.placeholder(dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_layer(input_data, size, dropout, is_training) :\n",
    "    layer = tf.layers.conv2d(input_data, size, [3, 3], padding='SAME')\n",
    "    layer = tf.layers.max_pooling2d(layer, [2, 2], [2, 2], padding='SAME')\n",
    "    layer = tf.layers.dropout(layer, dropout, is_training)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "with tf.device('/gpu:0') :\n",
    "    layer0_1 = tf.layers.conv2d(x, 8, [3,3], padding='SAME')\n",
    "    layer0_2 = tf.layers.conv2d(layer0_1, 128, [3,3], padding='SAME')\n",
    "    layer1 = learning_layer(input_data=layer0_2, size=256, dropout=0.7, is_training=is_training)\n",
    "#     layer0_3 = tf.layers.conv2d(layer0_2, 256, [3,3], padding='SAME')\n",
    "#     layer1 = learning_layer(input_data=layer0_2, size=256, drop_out=0.7, is_training=is_training, name_conv2d=\"layer1_conv2d\", \n",
    "#                    name_maxpooling=\"layer1_maxpooling\", name_dropout=\"layer1_dropout\")\n",
    "    layer1_1 = tf.layers.conv2d(layer1, 128, [3,3], padding='SAME')\n",
    "    layer1_2 = tf.layers.conv2d(layer1_1, 256, [3,3], padding='SAME')\n",
    "    layer2 = learning_layer(input_data=layer1_2, size=512, dropout=0.7, is_training=is_training)\n",
    "    layer2_1 = tf.layers.conv2d(layer2, 256, [3,3], padding='SAME')\n",
    "    layer2_2 = tf.layers.conv2d(layer2_1, 128, [3,3], padding='SAME')\n",
    "#     layer2_3 = tf.layers.conv2d(layer2_2, 64, [3,3], padding='SAME')\n",
    "    layer3 = learning_layer(input_data=layer2_2, size=64, dropout=0.7, is_training=is_training)\n",
    "#     layer3 = learning_layer(input_data=layer2_2, size=64, drop_out=0.7, is_training=is_training, name_conv2d=\"layer3_conv2d\", \n",
    "#                    name_maxpooling=\"layer3_maxpooling\", name_dropout=\"layer3_dropout\")\n",
    "    layer3_1 = tf.layers.conv2d(layer3, 128, [3,3], padding='SAME')\n",
    "    layer3_2 = tf.layers.conv2d(layer3_1, 256, [3,3], padding='SAME')\n",
    "    layer4 = learning_layer(input_data=layer3_2, size=64, dropout=0.7, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_layer = tf.contrib.layers.flatten(layer4)\n",
    "fully_layer = tf.layers.dense(fully_layer, 1024, activation=tf.nn.relu, name='fully_connected_layer')\n",
    "fully_layer = tf.layers.dropout(fully_layer, 0.5, is_training, name='fully_connected_dropout')\n",
    "\n",
    "fully_layer2 = tf.layers.dense(fully_layer, 512, activation=tf.nn.relu, name='fully_connected_layer2')\n",
    "fully_layer2 = tf.layers.dropout(fully_layer2, 0.5, is_training, name='fully_connected_dropout2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fully_layer2, label_len, activation=None)\n",
    "\n",
    "with tf.name_scope('pred') : \n",
    "    pred = tf.nn.softmax(logits)\n",
    "with tf.name_scope('loss') :\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_))\n",
    "    \n",
    "with tf.device('/gpu:0') :\n",
    "    with tf.name_scope('train') :\n",
    "            train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "label_output = tf.argmax(pred, 1)\n",
    "label_input = tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_output)\n",
    "print(label_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.summary.scalar('scalar_loss', loss)\n",
    "\n",
    "image_shaped_input = tf.reshape(x, [-1, image_resize, image_resize, 1])\n",
    "tf.summary.image('input', image_shaped_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(log_device_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess :\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "#     f = open(log_dir+log_file, 'w')\n",
    "    \n",
    "#     saver = tf.train.import_meta_graph('meta_data_name')\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('saved'))\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     for var in tf.trainable_variables() :\n",
    "#         tf.summary.histogram(var.name, var)\n",
    "\n",
    "#     merged_summary = tf.summary.merge_all()  \n",
    "#     train_writer = tf.summary.FileWriter(merge_dir, sess.graph)\n",
    "    \n",
    "    total_batch = int(len(image_list) / batch_size)\n",
    "    correct_list = []\n",
    "    incorrect_list = []\n",
    "    cost_list = []\n",
    "    cost_sum_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "#     sensitivity_acc = 0\n",
    "#     specificity_acc = 0\n",
    "    total_acc = 0\n",
    "    cost_sum = 0\n",
    "    cost_flag = 0\n",
    "    rate_ = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for epoch in range(training_epochs) :\n",
    "        total_cost = 0\n",
    "        \n",
    "        if epoch%50 == 49 :\n",
    "                rate_ *= 0.8\n",
    "                \n",
    "        if epoch%10 == 9 :\n",
    "#             rate_ *= 0.8\n",
    "            midpoint = int(time.time() - start_time)\n",
    "        \n",
    "            print(\"\")\n",
    "            print(\"=======================================================================================\")\n",
    "            print('{:03d}:{:02d}:{:02d}'.format(midpoint//3600, (midpoint%3600//60), midpoint%60))\n",
    "            print(\"=======================================================================================\")\n",
    "            print(\"\")\n",
    "        \n",
    "        batch_index = np.random.choice(len(image_array), total_batch, replace=False)\n",
    "                \n",
    "        for i in range(total_batch) :                \n",
    "            _, _loss = sess.run([train, loss], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]], \n",
    "                                                          is_training: True, learning_rate: initial_rate*rate_})    \n",
    "\n",
    "#             _, _loss, _merge = sess.run([train, loss, merged_summary], feed_dict={x: image_array[[batch_index[i]]], y_: label_array[[batch_index[i]]],\n",
    "#                                                                                   is_training: True, learning_rate: initial_rate*rate_})\n",
    "            \n",
    "#             saver.save(sess, save_dir, global_step=total_batch, write_meta_graph=False)\n",
    "            \n",
    "            total_cost += _loss\n",
    "\n",
    "        avg_cost = total_cost/total_batch\n",
    "        print('Epoch : ', '%4d' % (epoch + 1), '    Avg. cost = ', '{:.4f}'.format(avg_cost))            \n",
    "        \n",
    "#         train_writer.add_summary(_merge, epoch)\n",
    "               \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"================================     Training done     ================================\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "    for test in range(len(test_image_array)) :\n",
    "        _acc, _label_input, _label_output, _pred = sess.run([acc, label_input, label_output, pred], \n",
    "                             feed_dict={x: test_image_array[[test]], y_: test_label_array[[test]], is_training: False})\n",
    "\n",
    "        total_acc += _acc\n",
    "        pred_list.append(list(_pred[0]))\n",
    "        \n",
    "        print(\"n: \", test, \"    label_input: \", _label_input, \"    label_output: \", _label_output)\n",
    "        \n",
    "        if _label_input == _label_output :\n",
    "            correct_list.append(test)   \n",
    "        else :\n",
    "            incorrect_list.append(test)\n",
    "        \n",
    "    print(\"\")\n",
    "#     print(\"       SENSITIVITY ACC : \", '{:.5f}'.format(sensitivity_acc / 200))\n",
    "#     print(\"       SPECIFICITY ACC : \", '{:.5f}'.format(specificity_acc / 100))\n",
    "    print(\"          TOTAL ACC    : \", '{:.5f}'.format(total_acc / len(test_image_array)))\n",
    "    print(\"\")\n",
    "    print(pred_list)\n",
    "    \n",
    "#     for cnt in range(len(correct_list)) :\n",
    "#         data = correct_list[cnt]\n",
    "#         f.write(str(data)+\"\\n\")\n",
    "#         print(data)\n",
    "    \n",
    "#     tf.train.write_graph(sess.graph, './', pb_, as_text=False)\n",
    "#     tf.train.write_graph(sess.graph, './', txt_, as_text=True)\n",
    "#     train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    end_time = int(time.time() - start_time)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=======================================================================================\")\n",
    "    print('{:03d}:{:02d}:{:02d}'.format(end_time//3600, (end_time%3600//60), end_time%60))\n",
    "    print(\"=======================================================================================\")\n",
    "    print(\"\")\n",
    "    \n",
    "#     for cnt_incorrect in range(len(incorrect_list)) :\n",
    "#         print(incorrect_list[cnt_incorrect])\n",
    "    \n",
    "#     f.close()\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
