{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras, os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9182 images belonging to 3 classes.\n",
      "Found 1018 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory='D:/Backup_data/jongkeun/dir_images/traindata/', target_size=(128,128))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory='D:/Backup_data/jongkeun/dir_images/testdata/', target_size=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0107 14:39:09.721889  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 14:39:10.594562  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0107 14:39:10.604536  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0107 14:39:10.632461  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(input_shape=(128,128,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 14:39:11.286739  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0107 14:39:11.292723  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=1e-5)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 12291     \n",
      "=================================================================\n",
      "Total params: 65,066,819\n",
      "Trainable params: 65,066,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 14:39:12.402726  6596 deprecation.py:323] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0107 14:39:12.509441  6596 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 138s 1s/step - loss: 0.6892 - acc: 0.6991 - val_loss: 0.5048 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79688, saving model to 191118_vgg16\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 104s 1s/step - loss: 0.5692 - acc: 0.7556 - val_loss: 0.4988 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79688\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.5660 - acc: 0.7461 - val_loss: 0.4418 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79688\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.5391 - acc: 0.7694 - val_loss: 0.4633 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79688 to 0.83439, saving model to 191118_vgg16\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.5028 - acc: 0.7941 - val_loss: 0.4454 - val_acc: 0.8031\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.83439\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.4990 - acc: 0.8011 - val_loss: 0.3900 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.83439 to 0.84062, saving model to 191118_vgg16\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.4810 - acc: 0.8069 - val_loss: 0.3761 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84062 to 0.87580, saving model to 191118_vgg16\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.4653 - acc: 0.8113 - val_loss: 0.4201 - val_acc: 0.8031\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87580\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.4377 - acc: 0.8249 - val_loss: 0.3974 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87580\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.4391 - acc: 0.8281 - val_loss: 0.4299 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87580\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.4261 - acc: 0.8337 - val_loss: 0.4022 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87580\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.4152 - acc: 0.8402 - val_loss: 0.3408 - val_acc: 0.8781\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.87580 to 0.87813, saving model to 191118_vgg16\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.3986 - acc: 0.8481 - val_loss: 0.3297 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87813\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.3878 - acc: 0.8544 - val_loss: 0.2908 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.87813 to 0.90625, saving model to 191118_vgg16\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.4046 - acc: 0.8468 - val_loss: 0.3048 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90625\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.3799 - acc: 0.8566 - val_loss: 0.3501 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.90625\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.3624 - acc: 0.8634 - val_loss: 0.3298 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.90625\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.3587 - acc: 0.8649 - val_loss: 0.3181 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.90625\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.3587 - acc: 0.8619 - val_loss: 0.2768 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.90625 to 0.90938, saving model to 191118_vgg16\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.3361 - acc: 0.8722 - val_loss: 0.3762 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90938\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.3170 - acc: 0.8834 - val_loss: 0.3060 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90938\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.3365 - acc: 0.8734 - val_loss: 0.2370 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.90938 to 0.91250, saving model to 191118_vgg16\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.3390 - acc: 0.8734 - val_loss: 0.2616 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.91250\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.3275 - acc: 0.8781 - val_loss: 0.3072 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.91250\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.3044 - acc: 0.8825 - val_loss: 0.2434 - val_acc: 0.9031\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.91250\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.2986 - acc: 0.8865 - val_loss: 0.2493 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.91250 to 0.92038, saving model to 191118_vgg16\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.2736 - acc: 0.9056 - val_loss: 0.3687 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.92038\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.2908 - acc: 0.8937 - val_loss: 0.3049 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.92038\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.2548 - acc: 0.9021 - val_loss: 0.3049 - val_acc: 0.9045\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.92038\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.2649 - acc: 0.9028 - val_loss: 0.2720 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.92038\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.2715 - acc: 0.8994 - val_loss: 0.3134 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.92038\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.2488 - acc: 0.9065 - val_loss: 0.1906 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.92038 to 0.93949, saving model to 191118_vgg16\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.2494 - acc: 0.9100 - val_loss: 0.2909 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.93949\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.2522 - acc: 0.9097 - val_loss: 0.3320 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.93949\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.2306 - acc: 0.9105 - val_loss: 0.2372 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.93949\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.2122 - acc: 0.9237 - val_loss: 0.2598 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.93949\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.2192 - acc: 0.9203 - val_loss: 0.3067 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.93949\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1917 - acc: 0.9300 - val_loss: 0.2043 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.93949\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.1949 - acc: 0.9322 - val_loss: 0.2093 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.93949\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.2097 - acc: 0.9213 - val_loss: 0.1831 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.93949 to 0.94375, saving model to 191118_vgg16\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1874 - acc: 0.9322 - val_loss: 0.3951 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.94375\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.1663 - acc: 0.9391 - val_loss: 0.3009 - val_acc: 0.9013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_acc did not improve from 0.94375\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.1981 - acc: 0.9256 - val_loss: 0.2625 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.94375\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.1496 - acc: 0.9434 - val_loss: 0.2546 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.94375\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1350 - acc: 0.9513 - val_loss: 0.2865 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.94375\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1753 - acc: 0.9340 - val_loss: 0.2601 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.94375\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1156 - acc: 0.9591 - val_loss: 0.2435 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.94375\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1465 - acc: 0.9447 - val_loss: 0.3646 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.94375\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1350 - acc: 0.9471 - val_loss: 0.2080 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.94375\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1243 - acc: 0.9581 - val_loss: 0.3496 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.94375\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1142 - acc: 0.9550 - val_loss: 0.3978 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.94375\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.1163 - acc: 0.9562 - val_loss: 0.3387 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.94375\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0911 - acc: 0.9653 - val_loss: 0.3370 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.94375\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0960 - acc: 0.9600 - val_loss: 0.2593 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.94375\n",
      "Epoch 55/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0921 - acc: 0.9672 - val_loss: 0.2956 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.94375\n",
      "Epoch 56/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0723 - acc: 0.9756 - val_loss: 0.4267 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.94375\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0815 - acc: 0.9697 - val_loss: 0.3362 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.94375\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0809 - acc: 0.9694 - val_loss: 0.3183 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.94375\n",
      "Epoch 59/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0702 - acc: 0.9731 - val_loss: 0.3504 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.94375\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0767 - acc: 0.9738 - val_loss: 0.2309 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.94375\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0740 - acc: 0.9718 - val_loss: 0.3831 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.94375\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0556 - acc: 0.9794 - val_loss: 0.2714 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.94375\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0815 - acc: 0.9719 - val_loss: 0.5182 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.94375\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0505 - acc: 0.9819 - val_loss: 0.3163 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.94375\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0662 - acc: 0.9778 - val_loss: 0.4288 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.94375\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0627 - acc: 0.9778 - val_loss: 0.4003 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.94375\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0217 - acc: 0.9934 - val_loss: 0.3438 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.94375\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0302 - acc: 0.9897 - val_loss: 0.4049 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.94375\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0423 - acc: 0.9844 - val_loss: 0.4854 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.94375\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0469 - acc: 0.9831 - val_loss: 0.4503 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.94375\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.1016 - acc: 0.9616 - val_loss: 0.3420 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.94375\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0632 - acc: 0.9765 - val_loss: 0.2684 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.94375\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0301 - acc: 0.9912 - val_loss: 0.5072 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.94375\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0241 - acc: 0.9928 - val_loss: 0.3684 - val_acc: 0.9045\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.94375\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0487 - acc: 0.9834 - val_loss: 0.3973 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.94375\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0570 - acc: 0.9788 - val_loss: 0.4727 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.94375\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0547 - acc: 0.9806 - val_loss: 0.3104 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.94375\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0312 - acc: 0.9894 - val_loss: 0.3617 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.94375\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0109 - acc: 0.9984 - val_loss: 0.2950 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.94375\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.6186 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.94375\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0140 - acc: 0.9944 - val_loss: 0.4311 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.94375\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0150 - acc: 0.9963 - val_loss: 0.5961 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.94375\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0144 - acc: 0.9953 - val_loss: 0.3161 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.94375\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0254 - acc: 0.9915 - val_loss: 0.6098 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.94375\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0762 - acc: 0.9694 - val_loss: 0.4267 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.94375\n",
      "Epoch 86/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0830 - acc: 0.9712 - val_loss: 0.3194 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.94375\n",
      "Epoch 87/500\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0280 - acc: 0.9925 - val_loss: 0.2912 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.94375\n",
      "Epoch 88/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.6536 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.94375\n",
      "Epoch 89/500\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0602 - acc: 0.9784 - val_loss: 0.3613 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.94375\n",
      "Epoch 90/500\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.4526 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.94375\n",
      "Epoch 91/500\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0072 - acc: 0.9981 - val_loss: 0.4540 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.94375\n",
      "Epoch 92/500\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0078 - acc: 0.9997 - val_loss: 0.4256 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.94375\n",
      "Epoch 93/500\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0059 - acc: 0.9997 - val_loss: 0.4340 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.94375\n",
      "Epoch 94/500\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 6.8113e-04 - acc: 1.0000 - val_loss: 0.3432 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.94375\n",
      "Epoch 95/500\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5037 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.94375\n",
      "Epoch 96/500\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 4.6705e-04 - acc: 1.0000 - val_loss: 0.5757 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.94375\n",
      "Epoch 97/500\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 3.4430e-04 - acc: 1.0000 - val_loss: 0.5694 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.94375\n",
      "Epoch 98/500\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.5819 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.94375\n",
      "Epoch 99/500\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.1342 - acc: 0.9578 - val_loss: 0.2707 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.94375\n",
      "Epoch 100/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0446 - acc: 0.9866 - val_loss: 0.3810 - val_acc: 0.9076\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.94375\n",
      "Epoch 101/500\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.5626 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.94375\n",
      "Epoch 102/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0160 - acc: 0.9950 - val_loss: 0.2742 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.94375\n",
      "Epoch 103/500\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0168 - acc: 0.9928 - val_loss: 0.4165 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.94375\n",
      "Epoch 104/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0495 - acc: 0.9831 - val_loss: 0.4651 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.94375\n",
      "Epoch 105/500\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0433 - acc: 0.9872 - val_loss: 0.4168 - val_acc: 0.9031\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.94375\n",
      "Epoch 106/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.4427 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.94375\n",
      "Epoch 107/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.6567 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.94375\n",
      "Epoch 108/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.3540 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.94375\n",
      "Epoch 109/500\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.0619 - acc: 0.9762 - val_loss: 0.4725 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.94375\n",
      "Epoch 110/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0328 - acc: 0.9894 - val_loss: 0.3589 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.94375\n",
      "Epoch 111/500\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0203 - acc: 0.9922 - val_loss: 0.5720 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.94375\n",
      "Epoch 112/500\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0296 - acc: 0.9900 - val_loss: 0.4231 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.94375\n",
      "Epoch 113/500\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0394 - acc: 0.9866 - val_loss: 0.4194 - val_acc: 0.9031\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.94375\n",
      "Epoch 114/500\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0238 - acc: 0.9903 - val_loss: 0.4727 - val_acc: 0.9031\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.94375\n",
      "Epoch 00114: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"191118_vgg16\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='acc', patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=500,callbacks=[checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resize = 128\n",
    "image_width = image_resize\n",
    "image_height = image_resize\n",
    "home_dir = \"D:/Backup_data/jongkeun\"\n",
    "test_image_file = \"/dir_images/190309_new_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image as PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir = home_dir + test_image_file\n",
    "test_image_list = os.listdir(test_image_dir)\n",
    "test_image_list = sorted(test_image_list, key=lambda y: (int(re.sub('\\D','',y)),y))\n",
    "test_image_array = []\n",
    "\n",
    "for j in range(len(test_image_list)) :\n",
    "    test_image_list[j] = test_image_dir + test_image_list[j]\n",
    "    img = cv2.imread(test_image_list[j])\n",
    "    img2 = cv2.resize(img, (128,128))\n",
    "    test_image_array.append(np.array(img2))\n",
    "    \n",
    "test_image_array = np.reshape(test_image_array, [-1, image_width*image_height*3])\n",
    "test_image_array = np.array(np.reshape(test_image_array, [-1, image_width, image_height, 3]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018/1018 [==============================] - 4s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(test_image_array, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.999 0.000 0.001]\n",
      " [0.997 0.002 0.001]\n",
      " [0.846 0.077 0.078]\n",
      " ...\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 392ms/step\n",
      "[0 0 0 ... 2 2 2]\n",
      "[[0.995 0.005 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.020 0.011 0.970]\n",
      " [0.893 0.001 0.106]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.001 0.999]\n",
      " [0.992 0.000 0.008]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.002 0.000 0.998]\n",
      " [0.000 0.000 0.999]\n",
      " [0.393 0.088 0.520]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.006 0.000 0.994]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.732 0.211 0.057]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.010 0.000 0.990]\n",
      " [0.002 0.021 0.978]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.007 0.993]\n",
      " [0.000 0.000 1.000]\n",
      " [0.850 0.000 0.150]\n",
      " [0.096 0.426 0.478]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.714 0.000 0.286]\n",
      " [0.006 0.000 0.994]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.987 0.013 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.032 0.018 0.951]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.987 0.001 0.012]\n",
      " [0.000 1.000 0.000]\n",
      " [0.985 0.012 0.003]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.401 0.598]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.981 0.003 0.015]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.997 0.000 0.003]\n",
      " [0.012 0.005 0.983]\n",
      " [0.000 0.000 1.000]\n",
      " [0.998 0.002 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.991 0.001 0.008]\n",
      " [0.904 0.078 0.018]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.001 0.999]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.018 0.982]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.750 0.001 0.249]\n",
      " [0.000 1.000 0.000]\n",
      " [0.868 0.000 0.132]\n",
      " [0.853 0.003 0.144]\n",
      " [1.000 0.000 0.000]\n",
      " [0.001 0.008 0.991]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.006 0.994]\n",
      " [0.001 0.000 0.999]\n",
      " [0.038 0.000 0.962]\n",
      " [0.000 0.002 0.998]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.003 0.997]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.001 0.998]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.997 0.000 0.003]\n",
      " [0.086 0.914 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.999 0.001 0.000]\n",
      " [0.989 0.011 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.045 0.000 0.954]\n",
      " [0.001 0.874 0.125]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.924 0.069 0.007]\n",
      " [0.000 0.000 1.000]\n",
      " [0.993 0.002 0.005]\n",
      " [0.995 0.004 0.001]\n",
      " [0.001 0.892 0.107]\n",
      " [0.998 0.001 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.001 0.000 0.999]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.998 0.000 0.002]\n",
      " [0.989 0.011 0.000]\n",
      " [0.269 0.002 0.729]\n",
      " [0.004 0.000 0.995]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.003 0.986 0.011]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.962 0.037 0.001]\n",
      " [1.000 0.000 0.000]\n",
      " [0.108 0.016 0.876]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.936 0.046 0.018]\n",
      " [0.000 1.000 0.000]\n",
      " [0.999 0.001 0.000]\n",
      " [0.352 0.000 0.648]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.027 0.056 0.917]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.990 0.000 0.010]\n",
      " [0.000 1.000 0.000]\n",
      " [0.999 0.000 0.001]\n",
      " [0.109 0.891 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.998 0.001 0.001]\n",
      " [0.000 1.000 0.000]\n",
      " [0.003 0.039 0.958]\n",
      " [0.020 0.007 0.973]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.028 0.971 0.001]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.413 0.290 0.297]\n",
      " [0.084 0.905 0.011]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.010 0.138 0.851]\n",
      " [0.000 1.000 0.000]\n",
      " [0.432 0.005 0.562]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.509 0.491 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.897 0.102 0.000]\n",
      " [0.929 0.071 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.999 0.001 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.010 0.001 0.990]\n",
      " [0.000 0.235 0.765]\n",
      " [0.242 0.755 0.002]\n",
      " [0.933 0.065 0.002]\n",
      " [0.091 0.210 0.700]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.993 0.006 0.001]\n",
      " [0.915 0.085 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.315 0.020 0.665]\n",
      " [0.991 0.000 0.009]\n",
      " [0.000 1.000 0.000]\n",
      " [0.135 0.004 0.861]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.771 0.094 0.136]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.747 0.253 0.000]\n",
      " [0.507 0.001 0.492]\n",
      " [0.000 0.000 1.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.994 0.006 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.994 0.005 0.001]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.746 0.015 0.239]\n",
      " [0.968 0.000 0.032]\n",
      " [0.996 0.004 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.050 0.001 0.949]\n",
      " [0.000 1.000 0.000]\n",
      " [0.003 0.110 0.887]\n",
      " [0.000 0.013 0.987]\n",
      " [0.994 0.006 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.008 0.020 0.971]\n",
      " [0.000 1.000 0.000]\n",
      " [0.936 0.004 0.060]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.940 0.055 0.005]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.993 0.007 0.000]\n",
      " [0.429 0.255 0.316]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.002 0.998]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.004 0.996]\n",
      " [0.782 0.052 0.166]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.997 0.003 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.989 0.000 0.011]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 1.000 0.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.000 0.000 1.000]\n",
      " [0.951 0.002 0.047]\n",
      " [0.001 0.002 0.997]]\n"
     ]
    }
   ],
   "source": [
    "output = model.predict_generator(testdata, steps=10, verbose=1)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "noc = testdata.classes\n",
    "print(noc)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-179f6dbbe1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'noc' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(noc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018\n"
     ]
    }
   ],
   "source": [
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParam(arr, threshold) :\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    \n",
    "    predT = [0 for col in range(len(arr))]\n",
    "    \n",
    "    for i in range(len(arr)) :\n",
    "        if arr[i][1] > threshold or arr[i][2] >= threshold :\n",
    "            predT[i] = 1\n",
    "\n",
    "    for i in range(len(arr)) :\n",
    "        if i < 349 :\n",
    "            if predT[i] == 1 :\n",
    "                fp += 1\n",
    "            else :\n",
    "                tn += 1\n",
    "        else :\n",
    "            if predT[i] == 1 :\n",
    "                tp += 1\n",
    "            else :\n",
    "                fn += 1\n",
    "                \n",
    "    return [tp, fp, fn, tn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParam_(arr, threshold) :\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    \n",
    "    predF = [0 for col in range(len(arr))]\n",
    "    \n",
    "    for i in range(len(arr)) :\n",
    "        if arr[i][0] >= threshold :\n",
    "            predF[i] = 1\n",
    "            \n",
    "    for i in range(len(arr)) :\n",
    "        if i > 348 :\n",
    "            if predF[i] == 1 :\n",
    "                fp += 1\n",
    "            else :\n",
    "                tn += 1\n",
    "        else :\n",
    "            if predF[i] == 1 :\n",
    "                tp += 1\n",
    "            else :\n",
    "                fn += 1\n",
    "                \n",
    "    return [tp, fp, fn, tn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = [0]\n",
    "tpr_ = [0]\n",
    "\n",
    "fpr = [0]\n",
    "fpr_ = [0]\n",
    "\n",
    "result = 0\n",
    "result_ = 0\n",
    "\n",
    "for i in range(101) :\n",
    "    result = getParam(output, i*0.01)\n",
    "    result_ = getParam_(output, i*0.01)\n",
    "    \n",
    "    tpr.append(result[0]/(result[0]+result[2]))\n",
    "    tpr_.append(result_[0]/(result_[0]+result_[2]))\n",
    "    \n",
    "    fpr.append(result[1]/(result[1]+result[3]))\n",
    "    fpr_.append(result_[1]/(result_[1]+result_[3]))\n",
    "    \n",
    "fpr.sort()\n",
    "fpr_.sort()\n",
    "\n",
    "tpr.sort()\n",
    "tpr_.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[645, 63, 24, 286]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = getParam(output, 0.5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[282, 24, 67, 645]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = getParam_(output, 0.5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_abnormal:  0.9601166690223187\n",
      "auc_normal:  0.9623588214886865\n"
     ]
    }
   ],
   "source": [
    "print(\"auc_abnormal: \", metrics.auc(fpr, tpr))\n",
    "print(\"auc_normal: \", metrics.auc(fpr_, tpr_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # warning 출력 방지\n",
    "from keras.applications.resnet50 import ResNet50, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 64, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 32, 32, 64)   4160        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 32, 32, 256)  16640       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 32, 32, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 16, 16, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 16, 16, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 16, 16, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 8, 8, 256)    131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 8, 8, 1024)   525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 8, 8, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 1024)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 4, 4, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 4, 4, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 4, 4, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 2048)         0           activation_49[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(128,128,3))\n",
    "resnet = ResNet50(input_tensor=input, include_top=False, weights=None, pooling='max')\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 64, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 32, 32, 64)   4160        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 32, 32, 256)  16640       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 32, 32, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 32, 32, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 32, 32, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 32, 32, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 32, 32, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 32, 32, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 16, 16, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 16, 16, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 16, 16, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 16, 16, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 16, 16, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 16, 16, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 16, 16, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 8, 8, 256)    131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 8, 8, 1024)   525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 8, 8, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 8, 8, 256)    262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 8, 8, 256)    590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 8, 8, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 8, 8, 1024)   263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 1024)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 4, 4, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 4, 4, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 4, 4, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 2048)         0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         2098176     global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1024)         4096        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1024)         0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            3075        activation_50[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 25,693,059\n",
      "Trainable params: 25,637,891\n",
      "Non-trainable params: 55,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = resnet.output\n",
    "x = Dense(1024, init='uniform')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "resnet = Model(resnet.input, x)\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9182 images belonging to 3 classes.\n",
      "Found 1018 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory='D:/Backup_data/jongkeun/dir_images/traindata/', target_size=(128,128))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory='D:/Backup_data/jongkeun/dir_images/testdata/', target_size=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=1e-5)\n",
    "resnet.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 1.2420 - acc: 0.4062 - val_loss: 1.0033 - val_acc: 0.5094\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50938, saving model to 191118_resnet50\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 1.0268 - acc: 0.5037 - val_loss: 1.0777 - val_acc: 0.5031\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50938\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.9267 - acc: 0.5541 - val_loss: 0.9054 - val_acc: 0.5531\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.50938 to 0.55312, saving model to 191118_resnet50\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.8592 - acc: 0.6038 - val_loss: 0.9369 - val_acc: 0.5987\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.55312 to 0.59873, saving model to 191118_resnet50\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.8178 - acc: 0.6306 - val_loss: 0.8126 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.59873\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.7885 - acc: 0.6501 - val_loss: 0.7145 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59873 to 0.71875, saving model to 191118_resnet50\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.7536 - acc: 0.6778 - val_loss: 0.6918 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.71875\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 43s 429ms/step - loss: 0.7081 - acc: 0.6931 - val_loss: 0.6524 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.71875 to 0.74375, saving model to 191118_resnet50\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.7060 - acc: 0.6989 - val_loss: 0.6801 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.74375\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 43s 429ms/step - loss: 0.6904 - acc: 0.6969 - val_loss: 0.6181 - val_acc: 0.7452\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.74375 to 0.74522, saving model to 191118_resnet50\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 43s 430ms/step - loss: 0.6870 - acc: 0.7113 - val_loss: 0.6262 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.74522 to 0.75625, saving model to 191118_resnet50\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.6750 - acc: 0.7067 - val_loss: 0.6752 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.75625\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.6537 - acc: 0.7169 - val_loss: 0.6422 - val_acc: 0.7166\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.75625\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 43s 429ms/step - loss: 0.6764 - acc: 0.7053 - val_loss: 0.6497 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.75625\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.6312 - acc: 0.7378 - val_loss: 0.5822 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.75625 to 0.78438, saving model to 191118_resnet50\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 43s 429ms/step - loss: 0.6580 - acc: 0.7197 - val_loss: 0.6093 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.78438\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.6396 - acc: 0.7231 - val_loss: 0.5697 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.78438\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.6576 - acc: 0.7188 - val_loss: 0.5635 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.78438 to 0.78750, saving model to 191118_resnet50\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.6230 - acc: 0.7372 - val_loss: 0.6129 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.78750\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.6291 - acc: 0.7309 - val_loss: 0.5709 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.78750\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 43s 435ms/step - loss: 0.6104 - acc: 0.7330 - val_loss: 0.5655 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.78750\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.5964 - acc: 0.7497 - val_loss: 0.5871 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.78750\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"191118_resnet50\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', patience=20, verbose=1, mode='min')\n",
    "hist = resnet.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=500,callbacks=[checkpoint, early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018/1018 [==============================] - 5s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "output_res = resnet.predict(test_image_array, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[495, 46, 174, 303]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ = getParam(output_res, 0.5)\n",
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[222, 63, 127, 606]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ = getParam_(output_res, 0.5)\n",
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_res = [0]\n",
    "tpr_res_ = [0]\n",
    "\n",
    "fpr_res = [0]\n",
    "fpr_res_ = [0]\n",
    "\n",
    "result_res = 0\n",
    "result_res_ = 0\n",
    "\n",
    "for i in range(101) :\n",
    "    result_res = getParam(output_res, i*0.01)\n",
    "    result_res_ = getParam_(output_res, i*0.01)\n",
    "    \n",
    "    tpr_res.append(result_res[0]/(result_res[0]+result_res[2]))\n",
    "    tpr_res_.append(result_res_[0]/(result_res_[0]+result_res_[2]))\n",
    "    \n",
    "    fpr_res.append(result_res[1]/(result_res[1]+result_res[3]))\n",
    "    fpr_res_.append(result_res_[1]/(result_res_[1]+result_res_[3]))\n",
    "    \n",
    "fpr_res.sort()\n",
    "fpr_res_.sort()\n",
    "\n",
    "tpr_res.sort()\n",
    "tpr_res_.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_abnormal:  0.8988097532561536\n",
      "auc_normal:  0.9024353159357721\n"
     ]
    }
   ],
   "source": [
    "print(\"auc_abnormal: \", metrics.auc(fpr_res, tpr_res))\n",
    "print(\"auc_normal: \", metrics.auc(fpr_res_, tpr_res_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dogs & cats 16 conv.\n",
    "pred_list = [[0.8550839, 0.02433403, 0.12058208], [0.93155086, 0.023091994, 0.045357123], [0.6064808, 0.07595143, 0.31756774], [0.8049942, 0.15170033, 0.043305527], [0.9310309, 0.046784665, 0.022184378], [0.7114639, 0.057208415, 0.23132765], [0.737086, 0.019443775, 0.24347024], [0.91781443, 0.030932264, 0.051253244], [0.9071993, 0.00630453, 0.08649613], [0.8562902, 0.05192993, 0.09177984], [0.79184544, 0.02886922, 0.17928539], [0.7999655, 0.06740565, 0.13262878], [0.80253637, 0.030325722, 0.16713786], [0.89187473, 0.007361247, 0.100764036], [0.9071038, 0.007032554, 0.08586365], [0.94532216, 0.01082957, 0.043848287], [0.73131645, 0.050834734, 0.21784875], [0.9055954, 0.018310804, 0.076093785], [0.8698312, 0.042090446, 0.08807835], [0.690687, 0.053544004, 0.25576904], [0.94336754, 0.03678138, 0.019851163], [0.5205585, 0.33014688, 0.14929463], [0.8910589, 0.030676385, 0.078264646], [0.96063954, 0.01388984, 0.025470681], [0.9237419, 0.032253552, 0.04400464], [0.9245817, 0.011977259, 0.06344104], [0.77217364, 0.035726998, 0.19209935], [0.97629887, 0.0029478762, 0.0207532], [0.8767398, 0.010611086, 0.11264911], [0.7847712, 0.028492337, 0.18673643], [0.8891517, 0.008302562, 0.102545746], [0.6731362, 0.0809697, 0.2458941], [0.87394816, 0.005108474, 0.120943345], [0.87475324, 0.009100774, 0.1161459], [0.90425974, 0.041903, 0.05383729], [0.8960183, 0.023878047, 0.08010358], [0.8961511, 0.048474275, 0.055374693], [0.92277026, 0.015587238, 0.06164246], [0.7619312, 0.11440555, 0.123663194], [0.96278346, 0.004146918, 0.03306955], [0.6796627, 0.1256893, 0.19464798], [0.9325144, 0.028366722, 0.039118852], [0.6820466, 0.042865966, 0.2750875], [0.8553849, 0.10794128, 0.036673833], [0.5650531, 0.15685515, 0.2780918], [0.5534401, 0.3603783, 0.08618161], [0.9218122, 0.015577597, 0.06261019], [0.91963494, 0.00603002, 0.07433503], [0.93419856, 0.020789858, 0.045011528], [0.41228577, 0.32466516, 0.26304907], [0.8426703, 0.014999017, 0.14233065], [0.5090879, 0.060219847, 0.4306922], [0.88038266, 0.018721757, 0.100895606], [0.779388, 0.12779272, 0.09281928], [0.9418532, 0.01167193, 0.046474885], [0.70776224, 0.032881353, 0.2593564], [0.9206486, 0.0043694465, 0.07498197], [0.74712807, 0.13177413, 0.12109783], [0.94187444, 0.0113058025, 0.046819724], [0.9314497, 0.012896686, 0.05565369], [0.9118236, 0.04638382, 0.04179259], [0.78576756, 0.081500106, 0.13273223], [0.8632464, 0.06002238, 0.07673129], [0.9398992, 0.021789214, 0.03831151], [0.9402986, 0.0062572933, 0.053444006], [0.7074324, 0.059929397, 0.23263824], [0.53250456, 0.12011482, 0.34738067], [0.84609014, 0.075940594, 0.07796935], [0.8186754, 0.09098807, 0.090336554], [0.9365541, 0.01171567, 0.051730305], [0.8757697, 0.07594822, 0.048282124], [0.9287657, 0.003872225, 0.0673621], [0.5912136, 0.22180462, 0.18698177], [0.91346025, 0.00667326, 0.07986647], [0.9228085, 0.026866907, 0.050324664], [0.9756487, 0.009374054, 0.014977239], [0.9584162, 0.009622454, 0.031961385], [0.97374016, 0.0042117815, 0.02204798], [0.8882103, 0.015571414, 0.096218295], [0.9438582, 0.0022316463, 0.05391008], [0.959244, 0.011062911, 0.029693073], [0.8193981, 0.008224623, 0.17237726], [0.87757134, 0.077463806, 0.04496484], [0.88212913, 0.0046684276, 0.113202445], [0.9540535, 0.019806018, 0.026140554], [0.7766571, 0.045347303, 0.1779957], [0.11087181, 0.84221435, 0.046913885], [0.64447254, 0.11667849, 0.23884895], [0.89443874, 0.031299982, 0.07426126], [0.965217, 0.017807843, 0.016975187], [0.8763101, 0.094694346, 0.02899552], [0.92465544, 0.008221907, 0.0671227], [0.87544614, 0.057539556, 0.067014284], [0.2949444, 0.34863573, 0.35641983], [0.8569724, 0.029859273, 0.113168396], [0.7924172, 0.026132219, 0.18145062], [0.95578986, 0.009455265, 0.034754902], [0.96529526, 0.0051800855, 0.029524643], [0.6776746, 0.08641258, 0.23591277], [0.9646308, 0.0036689313, 0.03170025], [0.76932263, 0.14602, 0.08465731], [0.5809712, 0.1921938, 0.22683504], [0.828175, 0.040002815, 0.13182215], [0.8621058, 0.069973126, 0.06792111], [0.9177411, 0.003642916, 0.07861597], [0.8193415, 0.011630034, 0.1690285], [0.8587415, 0.010851364, 0.13040718], [0.72966117, 0.15967186, 0.11066699], [0.97135454, 0.007791286, 0.020854287], [0.6248991, 0.17728058, 0.19782035], [0.8639358, 0.046069723, 0.08999436], [0.83604634, 0.036476295, 0.1274774], [0.892569, 0.037209813, 0.070221215], [0.94596684, 0.009080146, 0.0449529], [0.8861669, 0.05763699, 0.056196097], [0.64728504, 0.04275483, 0.30996013], [0.8119864, 0.060850985, 0.12716258], [0.8384376, 0.09144614, 0.07011626], [0.8913398, 0.02887823, 0.07978203], [0.27797082, 0.6395679, 0.0824613], [0.9261075, 0.027474314, 0.046418175], [0.7302241, 0.15532993, 0.11444609], [0.9162032, 0.0061966926, 0.07760006], [0.7387194, 0.07402199, 0.18725853], [0.21958625, 0.6651766, 0.11523725], [0.9325251, 0.011611356, 0.05586349], [0.83622867, 0.08007941, 0.083691835], [0.8092244, 0.07187008, 0.118905455], [0.8614752, 0.0037103638, 0.13481437], [0.8863306, 0.012735463, 0.10093386], [0.61694777, 0.31924808, 0.06380415], [0.8569364, 0.036106706, 0.10695689], [0.9320043, 0.008057806, 0.059937946], [0.8176045, 0.01682414, 0.16557142], [0.9068351, 0.008623053, 0.084541894], [0.4633171, 0.2531726, 0.28351027], [0.27689478, 0.6742371, 0.04886822], [0.8351496, 0.051178873, 0.113671556], [0.68584055, 0.18718794, 0.1269715], [0.9332005, 0.0074072434, 0.059392318], [0.89550906, 0.032164022, 0.07232688], [0.80162203, 0.051058255, 0.14731978], [0.5448124, 0.32653356, 0.12865406], [0.5750799, 0.14220732, 0.28271276], [0.97431767, 0.0019898564, 0.023692518], [0.8473761, 0.05619978, 0.09642414], [0.96124816, 0.010428251, 0.028323624], [0.94481385, 0.0011898743, 0.053996295], [0.94552565, 0.009770248, 0.044704046], [0.89166844, 0.007797775, 0.10053378], [0.9140551, 0.038997672, 0.046947245], [0.21804154, 0.6177871, 0.16417138], [0.8449119, 0.08119029, 0.07389786], [0.91199136, 0.013131522, 0.07487717], [0.25218612, 0.4779656, 0.26984832], [0.8827657, 0.008374397, 0.108859956], [0.69961154, 0.058950413, 0.24143808], [0.8375787, 0.12326556, 0.039155755], [0.9031362, 0.051618468, 0.04524538], [0.49851063, 0.23836625, 0.26312315], [0.42225596, 0.33264723, 0.24509685], [0.84093726, 0.013363757, 0.145699], [0.6467565, 0.2194439, 0.13379964], [0.42979217, 0.35144076, 0.21876703], [0.89481443, 0.019529272, 0.085656315], [0.92105705, 0.022149017, 0.05679401], [0.870369, 0.047399573, 0.08223134], [0.82356197, 0.03760829, 0.13882975], [0.37360224, 0.5234882, 0.10290953], [0.9798448, 0.0013456474, 0.018809656], [0.9016271, 0.085267454, 0.013105423], [0.9502106, 0.0076262997, 0.042163175], [0.5779801, 0.32107112, 0.10094872], [0.53519017, 0.13548526, 0.3293245], [0.9553604, 0.019377984, 0.02526152], [0.7726644, 0.10046252, 0.12687309], [0.8222876, 0.018529864, 0.1591826], [0.96124727, 0.011715208, 0.027037488], [0.4962804, 0.3522831, 0.15143643], [0.9539705, 0.0081467405, 0.037882756], [0.867195, 0.014590041, 0.118214935], [0.8136041, 0.058808587, 0.1275873], [0.56707644, 0.38081196, 0.05211164], [0.89175534, 0.01913381, 0.08911078], [0.9441607, 0.014433457, 0.041405857], [0.92474055, 0.012131182, 0.06312824], [0.9477126, 0.0084059695, 0.043881375], [0.6240558, 0.1302934, 0.24565078], [0.97049683, 0.0050196825, 0.024483537], [0.91790026, 0.016863972, 0.065235734], [0.9842795, 0.0027640942, 0.012956421], [0.63358676, 0.1980106, 0.16840263], [0.6473075, 0.10108425, 0.25160822], [0.9048526, 0.007075184, 0.08807225], [0.6730875, 0.18619949, 0.14071296], [0.90368366, 0.0038600247, 0.092456326], [0.9149442, 0.015916662, 0.069139145], [0.5259844, 0.33558977, 0.13842583], [0.847486, 0.054377787, 0.09813621], [0.80156463, 0.07575834, 0.12267706], [0.5831798, 0.22559017, 0.1912301], [0.8816579, 0.030159382, 0.08818277], [0.8115764, 0.011725247, 0.17669837], [0.8911434, 0.008924153, 0.099932484], [0.4020891, 0.45558444, 0.14232643], [0.3020896, 0.64922845, 0.04868192], [0.9550285, 0.010028772, 0.03494273], [0.3315396, 0.62590426, 0.04255617], [0.92903537, 0.007038649, 0.06392588], [0.8983216, 0.019929735, 0.08174862], [0.75743705, 0.026214248, 0.21634862], [0.8031896, 0.134337, 0.062473435], [0.8982671, 0.00801199, 0.09372093], [0.96260846, 0.0032996247, 0.034092005], [0.88152575, 0.062246002, 0.056228183], [0.806365, 0.09761624, 0.09601876], [0.891196, 0.008704804, 0.100099236], [0.9142648, 0.0124510685, 0.07328416], [0.96856874, 0.010396339, 0.021034898], [0.67168325, 0.0933145, 0.23500219], [0.35951188, 0.18717358, 0.45331457], [0.88036233, 0.027759548, 0.09187819], [0.90236765, 0.008764925, 0.08886736], [0.05697166, 0.9298821, 0.0131461825], [0.850253, 0.09938484, 0.05036218], [0.8420855, 0.10558948, 0.052325103], [0.87086535, 0.039262094, 0.08987251], [0.94037944, 0.027494889, 0.03212561], [0.93504536, 0.015494069, 0.049460568], [0.88706094, 0.013859256, 0.09907979], [0.5381299, 0.28607202, 0.17579807], [0.7380842, 0.12850574, 0.13341005], [0.02923816, 0.9443605, 0.026401356], [0.587043, 0.27041313, 0.14254388], [0.64128226, 0.12737823, 0.23133945], [0.9567187, 0.0042750146, 0.039006237], [0.8812113, 0.0252047, 0.09358395], [0.94479394, 0.0057574427, 0.049448628], [0.31359944, 0.5941648, 0.09223579], [0.8985233, 0.017374333, 0.08410237], [0.8144561, 0.08944598, 0.096097894], [0.8840846, 0.004325094, 0.111590356], [0.77951276, 0.060118683, 0.16036853], [0.94991034, 0.006808504, 0.043281063], [0.38214153, 0.40914047, 0.20871805], [0.920654, 0.020529415, 0.058816567], [0.86280555, 0.009616107, 0.1275784], [0.55942017, 0.2434496, 0.19713022], [0.9460982, 0.0076028085, 0.046298906], [0.73689944, 0.028054586, 0.235046], [0.83435434, 0.036878772, 0.12876685], [0.6888158, 0.1420085, 0.16917568], [0.89389277, 0.036800615, 0.06930666], [0.20461579, 0.7461814, 0.049202763], [0.8882547, 0.083447985, 0.02829731], [0.9823422, 0.0022112196, 0.0154466685], [0.5689549, 0.34861833, 0.082426794], [0.7228479, 0.15376997, 0.1233821], [0.91684836, 0.05131902, 0.031832643], [0.89395326, 0.042921633, 0.06312503], [0.95172405, 0.010370464, 0.037905574], [0.72323686, 0.14658818, 0.13017498], [0.52158904, 0.1690038, 0.30940711], [0.8153629, 0.09929859, 0.08533852], [0.37515774, 0.4245659, 0.20027632], [0.6145142, 0.22741893, 0.15806681], [0.92825806, 0.013629111, 0.05811288], [0.8295306, 0.04561063, 0.12485881], [0.9129029, 0.020767746, 0.066329315], [0.1536337, 0.7357872, 0.110579096], [0.9667481, 0.0064118635, 0.02683998], [0.95636487, 0.008888412, 0.034746747], [0.8989171, 0.013779437, 0.08730349], [0.43118936, 0.4651907, 0.10361994], [0.7022157, 0.18864603, 0.109138325], [0.80044323, 0.02009396, 0.1794628], [0.8622668, 0.029787151, 0.10794605], [0.10283368, 0.86546665, 0.031699687], [0.8260328, 0.102133475, 0.07183366], [0.78078306, 0.07694761, 0.14226933], [0.96541715, 0.0031885863, 0.031394266], [0.90803045, 0.010920885, 0.08104858], [0.8082937, 0.10737818, 0.08432805], [0.1582122, 0.8055989, 0.036188893], [0.5888334, 0.19098775, 0.22017887], [0.9239765, 0.0042161155, 0.07180745], [0.33980465, 0.15071775, 0.5094776], [0.2220474, 0.6898366, 0.088116005], [0.22602154, 0.7040447, 0.06993371], [0.7175432, 0.09025333, 0.19220346], [0.7868403, 0.11462841, 0.09853131], [0.19732405, 0.696663, 0.106012926], [0.8850474, 0.004215607, 0.110737], [0.8783414, 0.037477165, 0.08418144], [0.94929934, 0.02420604, 0.026494624], [0.8374972, 0.026733972, 0.13576888], [0.9010958, 0.0074015344, 0.09150274], [0.006275229, 0.98331535, 0.010409444], [0.7778026, 0.07959485, 0.14260256], [0.9342231, 0.015869634, 0.049907215], [0.9151407, 0.005440772, 0.07941843], [0.73568714, 0.055335242, 0.20897762], [0.92100686, 0.049039304, 0.029953785], [0.5957754, 0.23848766, 0.16573694], [0.64743084, 0.06630124, 0.28626794], [0.8552301, 0.06136423, 0.08340572], [0.947307, 0.032451026, 0.020242022], [0.9557159, 0.011837789, 0.032446258], [0.95057505, 0.005564644, 0.043860283], [0.58744615, 0.35473844, 0.057815433], [0.70347434, 0.14445749, 0.15206815], [0.22464998, 0.69015086, 0.08519913], [0.9570026, 0.0042172675, 0.03878016], [0.9205495, 0.008267917, 0.071182564], [0.12827614, 0.7677617, 0.1039622], [0.93181753, 0.010505115, 0.05767741], [0.85078704, 0.07451107, 0.07470187], [0.84866893, 0.024618626, 0.12671241], [0.87270844, 0.037893876, 0.089397684], [0.8419557, 0.09193757, 0.06610673], [0.79083824, 0.085928716, 0.12323306], [0.6074717, 0.18347515, 0.20905313], [0.8195028, 0.07977626, 0.10072099], [0.9063159, 0.026423747, 0.067260385], [0.9221684, 0.016226495, 0.061605174], [0.3865344, 0.47057292, 0.1428927], [0.92820317, 0.023500413, 0.048296448], [0.31249884, 0.14705104, 0.5404501], [0.64311284, 0.19444136, 0.16244586], [0.9832097, 0.0033613506, 0.013428787], [0.9343301, 0.020995865, 0.044674058], [0.8661567, 0.027716482, 0.10612682], [0.88228, 0.022178473, 0.09554154], [0.49108577, 0.24897367, 0.2599406], [0.89693224, 0.007577059, 0.0954907], [0.937402, 0.012950588, 0.049647428], [0.6335402, 0.023674436, 0.3427853], [0.65373605, 0.15589471, 0.19036926], [0.7929535, 0.02685706, 0.18018952], [0.9502082, 0.03331055, 0.016481165], [0.88582164, 0.048941087, 0.06523733], [0.9094094, 0.007086912, 0.083503745], [0.50732416, 0.3241545, 0.16852133], [0.96485925, 0.013830859, 0.021309957], [0.5343961, 0.22196905, 0.24363483], [0.010920948, 0.9812413, 0.00783783], [0.92995965, 0.0064341878, 0.063606195], [0.89533263, 0.017616076, 0.087051235], [0.925411, 0.03409628, 0.040492736], [0.04112111, 0.9123265, 0.04655235], [0.28182775, 0.3960897, 0.32208255], [0.19135308, 0.27607766, 0.5325692], [0.010840785, 0.9760527, 0.0131065715], [0.23496956, 0.60151243, 0.16351803], [0.054721218, 0.8666758, 0.078602955], [0.015459117, 0.97206974, 0.012471056], [0.07199953, 0.901006, 0.026994511], [0.07490519, 0.89688766, 0.028207095], [0.08846601, 0.9011646, 0.0103694135], [0.09100029, 0.8776089, 0.031390868], [0.042254917, 0.9236669, 0.03407817], [0.081321456, 0.83892196, 0.079756655], [0.0051752133, 0.9925168, 0.0023080525], [0.0070708194, 0.9826804, 0.010248713], [0.031103127, 0.9408432, 0.028053692], [0.00043410197, 0.99879885, 0.0007670445], [0.11509697, 0.8242955, 0.0606076], [0.03735568, 0.9379589, 0.024685446], [0.27649504, 0.6204949, 0.10301008], [0.03728551, 0.93365157, 0.029063016], [0.06704008, 0.48247358, 0.45048627], [0.055157255, 0.8895531, 0.055289656], [0.5168659, 0.39036748, 0.0927666], [0.13490295, 0.80812913, 0.056967918], [0.15626822, 0.716707, 0.12702481], [0.16879341, 0.81048787, 0.02071868], [0.04388774, 0.85833794, 0.09777435], [0.08000748, 0.9024866, 0.017505853], [0.19393434, 0.7622596, 0.043806084], [0.33377317, 0.4950405, 0.17118637], [0.010522596, 0.9661338, 0.023343658], [0.002006339, 0.9937059, 0.00428775], [0.004434381, 0.9767304, 0.018835269], [0.3213916, 0.40372294, 0.27488545], [0.13330963, 0.779992, 0.086698346], [0.21629298, 0.45307812, 0.33062887], [0.032912813, 0.8902216, 0.07686564], [0.18416622, 0.73749423, 0.07833952], [0.04743525, 0.9158299, 0.036734913], [0.06429328, 0.8887491, 0.046957623], [0.13079679, 0.77777815, 0.09142507], [0.02878391, 0.94918203, 0.022034075], [0.014185579, 0.942847, 0.04296735], [0.23646484, 0.6802007, 0.08333447], [0.07816063, 0.79855865, 0.12328074], [0.010299175, 0.98144203, 0.008258771], [0.02619707, 0.9606817, 0.013121234], [0.06537422, 0.87914354, 0.055482354], [0.11121118, 0.6621355, 0.22665332], [0.032350387, 0.9444243, 0.023225367], [0.0041037365, 0.98902416, 0.0068721212], [0.069568396, 0.8496984, 0.08073308], [0.0045176945, 0.97038394, 0.02509833], [0.4571554, 0.48050043, 0.062344227], [0.058332045, 0.909058, 0.032609966], [0.13759622, 0.7456605, 0.116743326], [0.10188549, 0.81546533, 0.082649104], [0.10103668, 0.7798216, 0.119141795], [0.2150146, 0.65587914, 0.1291063], [0.029308084, 0.9366237, 0.03406813], [0.08100377, 0.8593388, 0.059657417], [0.34494674, 0.36164865, 0.29340464], [0.006779984, 0.97976893, 0.013451109], [0.0942467, 0.8394479, 0.066305414], [0.0072681443, 0.9788663, 0.013865525], [0.30800468, 0.41931108, 0.27268425], [0.23568091, 0.5911603, 0.17315882], [0.037817936, 0.93828213, 0.023899952], [0.054116268, 0.92334825, 0.022535374], [0.11618393, 0.79092693, 0.09288921], [0.4814046, 0.25928465, 0.25931072], [0.05745395, 0.90499926, 0.03754691], [0.0010005498, 0.9954314, 0.0035679517], [0.3435717, 0.60057414, 0.055854212], [0.030261572, 0.9278083, 0.041930236], [0.45255658, 0.46864542, 0.07879799], [0.30865818, 0.2174616, 0.4738803], [0.0069834967, 0.97648877, 0.016527778], [0.011211988, 0.92724353, 0.06154447], [0.0041205212, 0.97838914, 0.017490277], [0.0039904495, 0.9867491, 0.009260385], [0.13875613, 0.83588153, 0.025362313], [0.12565887, 0.7703629, 0.10397824], [0.05049338, 0.8583348, 0.09117182], [0.053062268, 0.92207015, 0.024867615], [0.002814733, 0.99074686, 0.006438473], [0.023442606, 0.9340752, 0.04248218], [0.0073987306, 0.9790469, 0.013554447], [0.16937336, 0.7429754, 0.08765117], [0.0041949414, 0.9870517, 0.00875344], [0.0444457, 0.9234611, 0.032093223], [0.043364547, 0.9002228, 0.05641264], [0.008097423, 0.9859409, 0.0059616864], [0.074514486, 0.8365362, 0.08894929], [0.069751956, 0.89213175, 0.038116302], [0.0854773, 0.8251424, 0.08938027], [0.12732048, 0.79551923, 0.07716023], [0.029355487, 0.9182982, 0.052346375], [0.7728559, 0.08266467, 0.14447936], [0.09403351, 0.83398944, 0.07197697], [0.16772032, 0.75385094, 0.078428805], [0.15856719, 0.62945366, 0.21197921], [0.0063054883, 0.9880051, 0.005689422], [0.2533234, 0.6338458, 0.112830825], [0.6046989, 0.17788821, 0.21741286], [0.13677067, 0.77075565, 0.0924737], [0.34496322, 0.61366284, 0.04137389], [0.10380002, 0.8139467, 0.08225326], [0.060442835, 0.9102439, 0.029313145], [0.021660741, 0.9331788, 0.045160476], [0.03821596, 0.9336976, 0.028086452], [0.13162361, 0.69146615, 0.17691025], [0.023696546, 0.9282056, 0.048097897], [0.10352665, 0.8468214, 0.049651958], [0.06096183, 0.92035985, 0.018678274], [0.010057357, 0.973361, 0.016581636], [0.21606545, 0.70590687, 0.0780277], [0.21078968, 0.5945615, 0.19464876], [0.013267834, 0.9829305, 0.0038016732], [0.113993764, 0.7931021, 0.092904165], [0.080279924, 0.80769104, 0.11202904], [0.51584715, 0.3789697, 0.10518314], [0.28280914, 0.65756524, 0.059625555], [0.6733967, 0.2709524, 0.055650946], [0.19240858, 0.75372344, 0.053867985], [0.238294, 0.5794182, 0.18228777], [0.083972596, 0.8460967, 0.06993076], [0.0154597, 0.96780694, 0.016733354], [0.15861927, 0.69236344, 0.14901726], [0.010538028, 0.97044027, 0.019021727], [0.19206521, 0.71608293, 0.09185186], [0.6557784, 0.108053386, 0.23616816], [0.052497305, 0.84755474, 0.099947914], [0.38498443, 0.13820058, 0.476815], [0.01079977, 0.97705144, 0.012148832], [0.916227, 0.0143943755, 0.06937867], [0.030331064, 0.94465125, 0.025017712], [0.015179789, 0.97856516, 0.0062549426], [0.040516417, 0.93688554, 0.022597995], [0.40902704, 0.42573676, 0.16523616], [0.2459283, 0.48841473, 0.2656569], [0.9298504, 0.008999974, 0.061149593], [0.4849614, 0.39602393, 0.11901471], [0.31592253, 0.60674614, 0.077331305], [0.005119975, 0.9904709, 0.0044091255], [0.21292564, 0.66575485, 0.12131955], [0.60562545, 0.099153124, 0.29522133], [0.3240643, 0.36382613, 0.3121096], [0.061079074, 0.8884439, 0.05047702], [0.025521114, 0.9402636, 0.034215197], [0.44845685, 0.26652372, 0.28501943], [0.053173654, 0.9209136, 0.025912812], [0.38740385, 0.26316842, 0.3494277], [0.43021882, 0.20873651, 0.3610447], [0.2065156, 0.6132608, 0.18022363], [0.004315153, 0.97558004, 0.020104803], [0.18877359, 0.7163313, 0.094895035], [0.30338988, 0.48126605, 0.2153441], [0.4592515, 0.33787277, 0.20287566], [0.04253959, 0.9116027, 0.04585777], [0.2219838, 0.56931907, 0.20869711], [0.238368, 0.6133968, 0.14823522], [0.009837015, 0.98456454, 0.0055984207], [0.12802851, 0.7471292, 0.12484233], [0.22410256, 0.7025316, 0.073365815], [0.38466895, 0.47737813, 0.13795295], [0.06709922, 0.8000632, 0.13283762], [0.38641205, 0.46527115, 0.1483168], [0.025043167, 0.9650186, 0.009938234], [0.002710709, 0.9926934, 0.0045959605], [0.11150919, 0.8237895, 0.06470132], [0.0883172, 0.839701, 0.071981795], [0.6162873, 0.1981128, 0.18559991], [0.11035469, 0.84631133, 0.043334033], [0.036381, 0.95075935, 0.012859597], [0.50305563, 0.2750202, 0.22192417], [0.18693052, 0.7776546, 0.03541491], [0.15222451, 0.7562721, 0.09150338], [0.05640715, 0.9210178, 0.02257501], [0.3776506, 0.48885196, 0.13349746], [0.013303187, 0.95438695, 0.032309797], [0.0033980135, 0.97329706, 0.02330487], [0.5293849, 0.3536139, 0.11700117], [0.004604667, 0.98792225, 0.0074730287], [0.15132803, 0.7534622, 0.09520976], [0.18602134, 0.6931364, 0.12084227], [0.070498206, 0.89429367, 0.035208035], [0.21867236, 0.68031275, 0.10101488], [0.15955007, 0.67359525, 0.16685477], [0.51287806, 0.30906764, 0.17805429], [0.34506378, 0.5860011, 0.06893509], [0.4074693, 0.40671626, 0.18581447], [0.052385043, 0.9329734, 0.014641585], [0.23742738, 0.60118926, 0.16138335], [0.33741522, 0.6057184, 0.05686636], [0.673042, 0.12617059, 0.2007874], [0.40233555, 0.4435322, 0.15413222], [0.61036617, 0.23266965, 0.1569642], [0.06759235, 0.8507194, 0.08168822], [0.47049987, 0.37039587, 0.1591043], [0.23303416, 0.69046587, 0.07649997], [0.50661737, 0.32744366, 0.16593899], [0.79814756, 0.14996272, 0.051889703], [0.2631078, 0.4706931, 0.2661991], [0.047247164, 0.9214859, 0.031267], [0.024714796, 0.91239876, 0.06288648], [0.07415459, 0.44837892, 0.47746655], [0.0052947067, 0.98736155, 0.0073437095], [0.008097427, 0.9379963, 0.053906284], [0.019868653, 0.90327, 0.076861344], [0.106363185, 0.8295856, 0.06405114], [0.7384703, 0.21450256, 0.04702711], [0.0052600605, 0.98061174, 0.014128223], [0.09731606, 0.79784095, 0.10484305], [0.6752815, 0.14562255, 0.17909586], [0.032451656, 0.93808085, 0.02946748], [0.023565648, 0.9686216, 0.00781275], [0.36324936, 0.5676186, 0.06913206], [1.4783467e-07, 1.5313333e-07, 0.99999964], [9.460279e-06, 6.462612e-06, 0.999984], [0.000292412, 0.0001114778, 0.9995962], [3.483261e-05, 1.4750111e-05, 0.9999504], [0.000620895, 0.00015861726, 0.9992205], [0.0019047289, 0.0010236959, 0.9970715], [1.0558539e-06, 6.1260613e-07, 0.99999833], [0.00013745832, 3.9724364e-05, 0.9998228], [0.00018149476, 0.00017151178, 0.999647], [0.0020582606, 0.0017591758, 0.9961825], [9.886254e-06, 2.3751752e-06, 0.9999877], [4.009228e-07, 3.853597e-07, 0.99999917], [1.8230598e-06, 2.6719795e-06, 0.99999547], [1.7051175e-06, 1.1827235e-06, 0.99999714], [0.00011643771, 7.0444315e-05, 0.99981314], [1.3398296e-06, 1.3223839e-06, 0.9999974], [2.0948595e-05, 4.295948e-06, 0.9999747], [4.7917424e-06, 3.0996853e-06, 0.99999213], [1.7661386e-06, 4.9592677e-07, 0.99999774], [0.031976815, 0.019499961, 0.9485233], [6.9928456e-07, 1.9474253e-07, 0.99999917], [4.583889e-06, 1.7346575e-06, 0.9999937], [0.0008739759, 0.00057467533, 0.9985513], [5.359596e-06, 8.942453e-07, 0.9999938], [0.009245381, 0.017602405, 0.9731522], [4.549017e-07, 2.6044574e-07, 0.9999993], [2.3193314e-05, 1.9913268e-06, 0.99997485], [3.9610242e-07, 2.1466694e-07, 0.9999994], [3.2500186e-06, 3.9391175e-06, 0.99999285], [1.4900995e-07, 3.503073e-08, 0.99999976], [2.1131749e-08, 4.4726165e-09, 1.0], [3.6817687e-06, 1.924507e-06, 0.9999944], [4.9949937e-05, 2.4160063e-05, 0.99992585], [2.393793e-05, 4.3880456e-05, 0.99993217], [2.6892308e-06, 1.6989111e-06, 0.9999956], [7.547478e-05, 5.6490328e-05, 0.99986804], [0.0011678054, 0.0007455541, 0.9980867], [2.0771658e-08, 3.6098002e-09, 1.0], [6.4234297e-07, 4.082906e-07, 0.9999989], [0.0011941064, 0.00096182805, 0.9978441], [2.2996371e-06, 3.2365754e-06, 0.9999945], [2.1740285e-05, 1.46684115e-05, 0.99996364], [4.5220645e-06, 4.5016986e-06, 0.99999094], [4.6673344e-07, 5.4444385e-08, 0.9999995], [0.0018885583, 0.0023570682, 0.99575436], [3.1885694e-08, 4.554398e-09, 1.0], [1.8474329e-06, 1.9068223e-06, 0.9999963], [0.0056432677, 0.024713958, 0.9696427], [1.4831827e-06, 4.589485e-06, 0.9999939], [5.8679772e-05, 5.822873e-05, 0.99988306], [7.3360656e-07, 4.655213e-07, 0.9999988], [6.3272246e-06, 9.1011725e-06, 0.9999846], [9.8650424e-05, 0.00016038127, 0.999741], [0.0001081893, 0.00032755447, 0.99956423], [0.00799405, 0.004073831, 0.98793215], [0.0069524786, 0.004025617, 0.9890219], [2.8620312e-07, 3.672155e-08, 0.99999964], [1.3568804e-06, 2.9748264e-07, 0.99999833], [8.286348e-06, 1.674471e-05, 0.99997497], [2.4331353e-06, 7.783474e-07, 0.9999968], [0.00014928688, 2.4191648e-05, 0.9998266], [0.00018745288, 7.0385206e-05, 0.9997422], [5.805312e-07, 9.704703e-08, 0.9999993], [5.7138193e-05, 0.00025856987, 0.99968433], [3.7743164e-05, 1.6086971e-05, 0.9999461], [1.5784442e-08, 1.8462569e-09, 1.0], [4.5978624e-05, 2.9006267e-05, 0.999925], [3.055337e-06, 1.5817536e-06, 0.99999535], [3.9139726e-05, 4.275856e-05, 0.9999181], [4.561148e-05, 1.3805622e-05, 0.99994063], [0.00020799552, 0.00011720889, 0.9996748], [5.385825e-06, 2.7685091e-06, 0.9999919], [2.2647633e-08, 5.431286e-09, 1.0], [0.0003200055, 0.00053582736, 0.9991442], [3.268729e-06, 1.5950077e-06, 0.9999951], [2.4714356e-05, 3.5017347e-05, 0.9999403], [0.02435547, 0.011779101, 0.96386546], [1.2512883e-05, 9.700554e-06, 0.9999778], [4.9702007e-06, 3.5108671e-06, 0.99999154], [1.2167322e-05, 4.5482175e-06, 0.9999833], [1.0933873e-06, 3.28362e-07, 0.99999857], [8.673565e-08, 3.6637957e-08, 0.9999999], [1.7059147e-06, 1.0858416e-06, 0.99999726], [6.269407e-05, 2.2790806e-05, 0.9999145], [0.09548964, 0.46570504, 0.43880528], [8.384222e-05, 0.00012020845, 0.999796], [2.7731335e-06, 5.058368e-07, 0.99999666], [1.2253387e-06, 7.1976405e-07, 0.9999981], [5.860907e-08, 1.3518051e-08, 0.9999999], [0.00033630105, 3.8899427e-05, 0.9996247], [9.136268e-05, 2.7828351e-05, 0.9998808], [8.7065935e-05, 2.9620092e-05, 0.9998833], [0.0011420936, 0.0005924498, 0.99826545], [7.05071e-05, 3.9973173e-05, 0.9998895], [1.0640864e-05, 9.764764e-06, 0.9999796], [2.1035123e-06, 2.4162493e-06, 0.99999547], [0.007268476, 0.3778858, 0.6148457], [3.172064e-07, 1.571544e-07, 0.9999995], [4.421301e-06, 1.0773537e-06, 0.9999945], [8.160043e-06, 3.5011482e-05, 0.99995685], [1.9039484e-05, 9.912457e-05, 0.99988186], [3.3940016e-06, 1.142781e-06, 0.99999547], [0.01748777, 0.007345002, 0.9751672], [0.00030754835, 0.0010984234, 0.998594], [3.9192328e-06, 8.147948e-07, 0.99999523], [2.0279579e-07, 7.399687e-08, 0.99999976], [8.354007e-08, 5.172864e-08, 0.9999999], [8.778605e-05, 0.0001318916, 0.99978036], [2.2065436e-08, 2.5835554e-09, 1.0], [4.6657405e-07, 1.5694049e-07, 0.9999994], [0.00017157198, 0.00019856932, 0.99962986], [6.395327e-06, 3.2458286e-06, 0.99999034], [1.0680313e-07, 1.2156996e-08, 0.9999999], [0.9012709, 0.005931555, 0.092797585], [0.00019852996, 5.237938e-05, 0.9997491], [9.451222e-05, 5.2797226e-05, 0.99985266], [3.0378016e-05, 1.998246e-05, 0.9999497], [1.7613625e-06, 7.628562e-06, 0.9999906], [0.0005393782, 0.0002046671, 0.99925596], [6.203737e-05, 1.9912983e-05, 0.999918], [5.5235094e-05, 9.144855e-05, 0.99985325], [0.00058166403, 0.0026138602, 0.9968045], [2.565554e-05, 1.7062363e-05, 0.9999573], [1.1658756e-05, 7.2800512e-06, 0.99998105], [0.18354668, 0.6480101, 0.16844325], [0.0032151325, 0.008021716, 0.9887631], [0.0002490902, 0.00023079453, 0.9995202], [2.4965727e-06, 2.2456835e-07, 0.99999726], [0.83066833, 0.019014962, 0.1503167], [0.00039922187, 0.0012080339, 0.99839276], [2.7301232e-06, 9.695409e-07, 0.9999963], [0.005545329, 0.15773518, 0.83671945], [2.3254327e-07, 1.8942343e-07, 0.9999995], [0.35897273, 0.35749507, 0.28353223], [0.007485362, 0.061098695, 0.931416], [0.0004666568, 0.0023825532, 0.99715084], [0.00016477428, 0.00025886766, 0.9995764], [3.5491914e-06, 9.943246e-07, 0.99999547], [4.24005e-07, 5.3075762e-08, 0.9999995], [1.7118339e-07, 6.1363e-08, 0.99999976], [7.89398e-05, 0.00014603851, 0.999775], [0.00020648721, 0.0010007721, 0.99879277], [0.37568852, 0.30695876, 0.31735268], [0.7733288, 0.058149304, 0.16852197], [7.3730746e-08, 1.7183332e-08, 0.9999999], [3.4696992e-08, 1.925381e-09, 1.0], [0.00025410717, 0.00029085533, 0.99945503], [4.378917e-06, 3.438789e-06, 0.99999213], [0.88168913, 0.07690689, 0.041404024], [0.0007412874, 0.0011249947, 0.9981337], [1.28307756e-08, 2.6329088e-09, 1.0], [4.3953396e-06, 1.3811214e-06, 0.9999943], [1.949661e-08, 6.756383e-08, 0.9999999], [2.5536568e-08, 4.1498494e-09, 1.0], [1.1599183e-05, 6.980099e-06, 0.9999814], [5.5888676e-09, 8.1190116e-10, 1.0], [5.4583954e-05, 4.792996e-05, 0.9998975], [7.808633e-06, 1.0243866e-05, 0.999982], [0.09240795, 0.8599591, 0.04763292], [0.048260916, 0.0155998925, 0.93613917], [0.8959772, 0.03166015, 0.07236267], [2.7203287e-06, 5.9262464e-07, 0.99999666], [2.6537136e-06, 7.026078e-07, 0.99999666], [1.5959886e-05, 2.8956116e-05, 0.99995506], [2.7013604e-05, 9.303424e-06, 0.99996364], [0.015236664, 0.031325817, 0.9534375], [1.8840841e-06, 2.7779524e-06, 0.99999535], [5.004692e-07, 1.8882152e-07, 0.9999993], [1.4393738e-07, 1.07343695e-07, 0.99999976], [0.6986354, 0.07287915, 0.22848545], [0.0019954904, 0.017044049, 0.9809605], [6.2435277e-07, 1.2605449e-07, 0.9999993], [0.45705345, 0.2283419, 0.31460464], [6.6981653e-07, 3.4405952e-07, 0.9999989], [1.143138e-05, 4.6078235e-06, 0.9999839], [7.8556457e-07, 5.5801314e-07, 0.9999987], [7.39286e-08, 1.24962645e-08, 0.9999999], [5.0341256e-05, 1.5506797e-05, 0.9999342], [1.3723694e-06, 3.6878956e-07, 0.9999982], [0.00021207226, 8.8957204e-05, 0.999699], [1.3691799e-07, 1.00129974e-07, 0.99999976], [6.1853356e-08, 8.231015e-09, 0.9999999], [4.9737697e-05, 1.766562e-05, 0.99993265], [4.8500613e-05, 7.436757e-05, 0.9998771], [4.886904e-07, 1.2708e-07, 0.9999994], [4.2636696e-08, 6.6307706e-09, 1.0], [2.1701708e-05, 4.5330103e-06, 0.9999738], [1.455566e-06, 1.1433305e-06, 0.9999974], [6.7730696e-05, 5.92968e-05, 0.9998729], [1.011383e-06, 1.1814302e-06, 0.99999785], [2.0925876e-07, 5.0626234e-08, 0.99999976], [1.04541205e-05, 7.516131e-06, 0.999982], [3.3560832e-06, 1.954409e-06, 0.99999464], [1.4107596e-06, 6.146746e-07, 0.999998], [1.2675344e-05, 2.1914452e-06, 0.9999851], [5.145674e-06, 1.9395573e-06, 0.99999297], [5.2894478e-08, 1.5724222e-08, 0.9999999], [8.7274316e-08, 2.7958412e-08, 0.9999999], [0.0035561642, 0.022365445, 0.97407836], [0.6980551, 0.056411672, 0.2455332], [1.0175321e-05, 2.069309e-06, 0.9999877], [3.2548915e-06, 1.0590248e-06, 0.9999957], [0.00017217154, 0.0019858417, 0.99784195], [0.0061662095, 0.04879052, 0.9450433], [6.811147e-05, 0.00010819871, 0.99982375], [1.8302137e-08, 4.588631e-09, 1.0], [4.1398012e-05, 1.4032837e-05, 0.99994457], [1.8526716e-05, 3.031121e-05, 0.9999511], [3.6983042e-08, 6.006041e-09, 1.0], [1.1082195e-07, 1.9036657e-08, 0.9999999], [0.0005540556, 0.00024879278, 0.9991972], [6.4417577e-07, 6.3473567e-07, 0.9999987], [1.7289861e-06, 6.568345e-07, 0.9999976], [4.7476387e-06, 1.074258e-06, 0.99999416], [0.00010818657, 0.00080450624, 0.99908733], [2.3720447e-07, 2.954388e-08, 0.99999976], [0.010746266, 0.98047596, 0.008777771], [1.8373046e-06, 4.444514e-07, 0.99999774], [2.665161e-07, 8.668485e-08, 0.99999964], [5.742641e-07, 3.840473e-07, 0.99999905], [0.06830806, 0.04156893, 0.890123], [7.178835e-07, 3.859098e-07, 0.9999989], [9.594453e-07, 8.31218e-07, 0.9999982], [0.3409201, 0.38921848, 0.26986143], [0.00026514227, 2.3103068e-05, 0.9997117], [6.4727365e-06, 2.4719725e-06, 0.99999106], [0.14212187, 0.55774903, 0.30012906], [4.5952712e-07, 2.5930643e-07, 0.9999993], [2.1065294e-07, 6.728912e-08, 0.99999976], [0.6399488, 0.2936279, 0.06642332], [0.0001318877, 0.00077310455, 0.99909496], [0.00014544278, 0.0004400704, 0.99941444], [1.8412331e-07, 1.0765658e-07, 0.99999976], [5.005471e-10, 1.7419259e-10, 1.0], [0.0018855971, 0.0016758874, 0.9964385], [2.8587603e-08, 8.131662e-09, 1.0], [4.7432837e-05, 4.0346735e-05, 0.99991226], [0.0037370618, 0.00013303298, 0.9961299], [2.565554e-05, 1.7062363e-05, 0.9999573], [7.564919e-06, 6.321535e-06, 0.9999862], [7.1288957e-09, 1.105219e-09, 1.0], [0.05160945, 0.18298493, 0.7654056], [1.9260764e-07, 1.2374225e-07, 0.99999964], [2.0916494e-05, 3.78583e-05, 0.99994123], [7.2935477e-06, 1.0459538e-05, 0.99998224], [0.0010934919, 0.006343825, 0.9925627], [0.07296739, 0.82825726, 0.09877531], [2.4539657e-05, 2.785756e-05, 0.99994755], [6.211082e-07, 4.0655883e-07, 0.9999989], [0.65917134, 0.04982472, 0.2910039], [4.418778e-06, 6.03625e-07, 0.999995], [7.284742e-05, 0.00010062334, 0.9998266], [1.6277658e-05, 3.115959e-06, 0.99998057], [1.911127e-06, 1.3545975e-06, 0.9999968], [1.9369702e-06, 9.006038e-07, 0.99999714], [4.0297607e-08, 1.4471473e-08, 1.0], [5.8170076e-06, 2.680731e-06, 0.99999154], [9.376042e-06, 5.2499263e-06, 0.99998534], [0.00014500832, 3.2414504e-05, 0.99982256], [0.0010439241, 0.0009747992, 0.99798125], [4.582501e-06, 2.3221435e-06, 0.9999931], [0.0011202713, 0.00017944556, 0.99870026], [9.331894e-06, 2.3451398e-06, 0.9999883], [2.5578984e-07, 5.146724e-08, 0.99999964], [2.5012501e-05, 2.6106849e-05, 0.99994886], [3.6190046e-05, 1.1493918e-05, 0.9999523], [1.9165415e-07, 3.4393103e-08, 0.99999976], [0.0012542239, 0.0012442748, 0.99750155], [5.3098543e-09, 3.727792e-09, 1.0], [2.9582575e-06, 1.645137e-06, 0.99999535], [0.00030893838, 0.0003124519, 0.99937856], [0.00012418994, 7.055921e-05, 0.9998053], [7.499222e-07, 1.8657305e-07, 0.99999905], [2.2769994e-08, 3.4583794e-09, 1.0], [0.6176706, 0.09400918, 0.28832024], [3.90162e-05, 1.0257468e-05, 0.99995077], [1.3640872e-08, 1.0684383e-08, 1.0], [2.173271e-08, 9.82474e-09, 1.0], [0.00019607847, 0.0005015489, 0.9993024], [3.5788733e-07, 5.912938e-08, 0.99999964], [0.48525164, 0.2006424, 0.31410596], [0.83996433, 0.031645227, 0.12839048], [0.01116251, 0.05050604, 0.9383314], [7.7299525e-08, 1.3732003e-08, 0.9999999], [9.3070554e-07, 5.143276e-07, 0.99999857], [3.0665124e-06, 1.3945092e-06, 0.9999956], [2.0517943e-07, 9.917048e-07, 0.9999988], [0.0059899776, 0.12128528, 0.8727248], [1.6279131e-07, 2.786524e-08, 0.99999976], [3.1791154e-05, 1.1397467e-05, 0.99995685], [0.33394486, 0.1952799, 0.47077528], [2.4574344e-06, 7.9970437e-07, 0.9999968], [0.615751, 0.076042555, 0.30820638], [0.017644346, 0.10001418, 0.88234144], [1.611353e-06, 5.138659e-07, 0.99999785], [4.3288155e-06, 1.1486825e-06, 0.9999945], [0.107839316, 0.14516774, 0.74699295], [1.323073e-05, 3.1140821e-06, 0.99998367], [0.48918352, 0.15721218, 0.35360432], [1.6052934e-06, 4.988959e-06, 0.99999344], [1.7877033e-05, 1.0066098e-05, 0.9999721], [9.557484e-05, 1.4918134e-05, 0.9998895], [1.2365016e-06, 1.2145067e-06, 0.9999975], [0.1753797, 0.21564032, 0.60898], [0.00014742777, 0.00017392071, 0.9996786], [2.6782923e-06, 4.2347656e-07, 0.9999969], [5.148716e-09, 8.005435e-10, 1.0], [0.3125136, 0.4792153, 0.20827118], [0.9430582, 0.013886335, 0.0430554], [0.65599144, 0.20286506, 0.14114349], [0.85225546, 0.012703983, 0.13504057], [4.099838e-07, 6.4907354e-08, 0.9999995], [0.020955924, 0.0029099362, 0.9761342], [5.126209e-05, 3.946647e-06, 0.9999448], [2.3156334e-08, 1.0669762e-08, 1.0], [1.2427115e-07, 3.3154606e-08, 0.9999999], [2.4210569e-05, 1.5855134e-05, 0.99995995], [0.00052674976, 0.00015652145, 0.9993167], [7.2568824e-08, 1.1948401e-08, 0.9999999], [4.1019184e-06, 8.920148e-07, 0.999995], [0.6989464, 0.05068383, 0.25036973], [1.331531e-05, 3.0883045e-06, 0.99998355], [1.1431986e-06, 4.7484394e-07, 0.99999833], [6.0458905e-07, 2.2622457e-07, 0.99999917], [9.8149234e-05, 4.806046e-05, 0.99985373], [1.0748888e-05, 2.7464703e-06, 0.9999865], [1.817108e-05, 4.784393e-05, 0.99993396], [0.0004531345, 0.00018538575, 0.99936146], [5.6993184e-08, 2.6302695e-08, 0.9999999], [0.45885012, 0.19695686, 0.34419295], [6.607467e-07, 8.472911e-08, 0.9999993], [0.00038055945, 0.0013608902, 0.99825853], [0.008654013, 0.009140176, 0.98220575], [9.100801e-06, 3.1279383e-06, 0.9999877], [9.680226e-07, 2.5251657e-07, 0.9999988], [4.3372772e-08, 1.3781386e-08, 1.0], [0.1162936, 0.71003586, 0.17367052], [6.182454e-06, 2.3366395e-06, 0.99999154], [0.51485574, 0.23680739, 0.2483369], [0.024164295, 0.018955858, 0.95687985], [3.6912922e-08, 8.602338e-09, 1.0], [1.115878e-05, 2.9946216e-06, 0.9999858], [0.00037586872, 0.00016956568, 0.99945456], [1.7480464e-06, 3.4496463e-07, 0.99999785], [0.11498357, 0.84470063, 0.040315803], [6.0105153e-06, 2.303311e-06, 0.99999166], [6.21509e-08, 8.825101e-09, 0.9999999], [1.8003264e-05, 5.1714283e-06, 0.9999769], [0.00014815583, 5.5339253e-05, 0.99979657], [3.4126783e-06, 2.8415047e-06, 0.9999938], [1.1651772e-06, 4.0568332e-07, 0.99999845], [5.046854e-05, 1.3556247e-05, 0.999936], [1.5053237e-06, 2.7953473e-07, 0.9999982], [0.9559979, 0.0038087976, 0.040193345], [1.0041203e-05, 8.955725e-06, 0.99998105], [0.91204613, 0.053654455, 0.034299374], [1.2119446e-08, 2.9357659e-09, 1.0], [5.9291118e-08, 1.3817496e-08, 0.9999999], [1.3755589e-07, 8.416107e-08, 0.99999976], [0.014944103, 0.0063995062, 0.9786564], [2.0005028e-07, 2.8514677e-08, 0.99999976], [0.49823073, 0.40764362, 0.09412564], [6.041941e-06, 2.0079517e-06, 0.9999919], [5.388638e-07, 7.306342e-07, 0.9999987], [5.370576e-05, 1.301905e-05, 0.99993324], [2.7010376e-06, 1.0136381e-06, 0.9999963], [5.4408684e-07, 1.4571349e-07, 0.9999993], [3.1818382e-07, 4.3475954e-08, 0.99999964], [1.3341605e-05, 6.161484e-06, 0.99998045], [1.6184056e-07, 2.0534864e-08, 0.99999976], [1.4368323e-06, 3.4509407e-07, 0.9999982], [9.3150976e-07, 7.2055127e-07, 0.99999833], [0.68851805, 0.1524539, 0.159028], [0.13659616, 0.20158905, 0.6618148], [1.0711508e-05, 5.3349377e-06, 0.9999839], [2.5206816e-06, 7.615694e-07, 0.99999666], [0.032697774, 0.010317934, 0.9569842], [1.0943071e-06, 4.1256658e-07, 0.99999845], [3.719598e-07, 9.65215e-08, 0.9999995], [3.7421025e-05, 1.45993245e-05, 0.999948], [2.811652e-06, 3.6315825e-05, 0.9999609], [2.4388885e-07, 1.5345421e-07, 0.99999964], [2.3408807e-05, 1.5619942e-05, 0.999961], [7.940859e-06, 6.082444e-06, 0.99998593], [3.5721714e-06, 7.536724e-07, 0.9999957], [1.4586705e-07, 7.691273e-08, 0.99999976], [1.0694171e-05, 9.752683e-06, 0.9999795], [2.1415037e-06, 1.183691e-06, 0.99999666], [8.529929e-08, 1.6190226e-08, 0.9999999], [2.6011643e-08, 4.0139714e-09, 1.0], [0.57360065, 0.16395463, 0.26244473], [0.7346884, 0.08413559, 0.18117605], [6.130608e-07, 7.581637e-08, 0.9999993], [5.9082145e-06, 9.7756465e-06, 0.99998426], [1.6035122e-08, 4.592098e-09, 1.0], [2.0409698e-05, 8.075399e-06, 0.9999715], [7.7419014e-07, 2.2382875e-07, 0.99999905], [5.31595e-06, 3.7243844e-06, 0.99999094], [5.0864804e-07, 4.1107205e-07, 0.99999905], [3.1156899e-06, 6.7630407e-07, 0.9999962], [4.322043e-05, 1.687334e-05, 0.9999399], [2.9388807e-06, 5.5831646e-07, 0.99999654], [6.869872e-05, 8.660801e-05, 0.9998447], [0.0006482734, 0.0029297462, 0.99642205], [3.6703204e-05, 3.9630784e-05, 0.9999237], [0.3681949, 0.280324, 0.35148105], [2.6408065e-08, 8.164843e-09, 1.0], [7.767051e-06, 7.239688e-06, 0.999985], [2.0659296e-07, 2.2380958e-08, 0.99999976], [6.585175e-05, 6.8935675e-05, 0.9998652], [1.306656e-07, 1.9244864e-07, 0.99999964], [7.739152e-05, 4.7034744e-05, 0.99987555], [0.056284063, 0.06824551, 0.87547046], [2.003579e-05, 9.649963e-06, 0.9999703], [3.8992276e-08, 4.821895e-09, 1.0], [2.4754183e-06, 1.0258518e-06, 0.99999654], [2.6735466e-07, 6.3144675e-08, 0.99999964], [1.4485757e-07, 4.8437155e-08, 0.99999976], [1.0651132e-05, 3.6431002e-06, 0.9999857], [9.71004e-06, 1.4377319e-05, 0.9999759], [2.4946216e-06, 1.9352958e-06, 0.9999956], [0.0010590412, 0.0008007629, 0.9981402], [7.752174e-08, 4.1894886e-08, 0.9999999], [0.00011882901, 0.00016849737, 0.99971265], [2.5585578e-05, 0.00018159708, 0.9997929], [2.703659e-05, 4.2830456e-05, 0.99993014], [7.8853344e-07, 1.803554e-07, 0.99999905], [1.7780716e-05, 1.36584285e-05, 0.9999685], [0.64918244, 0.06905313, 0.28176442], [1.2093691e-05, 1.5781181e-05, 0.9999721], [0.0017915481, 0.0047189323, 0.9934895], [5.9467297e-06, 5.487143e-06, 0.99998856], [0.00033382897, 5.5093726e-05, 0.9996111], [6.719581e-06, 4.2320335e-06, 0.99998903], [3.8504804e-05, 8.579546e-06, 0.9999529], [0.00036571934, 0.0029336968, 0.9967006], [0.14313404, 0.6678744, 0.18899155], [5.3164538e-08, 1.2321315e-08, 0.9999999], [0.00013630326, 7.849094e-05, 0.99978524], [1.5997651e-07, 6.1114787e-07, 0.9999993], [0.38407168, 0.43517643, 0.18075186], [0.0035605263, 0.0029157607, 0.9935237], [1.4144929e-05, 2.5845043e-06, 0.9999833], [5.639239e-07, 2.1347668e-07, 0.99999917], [7.5366194e-07, 1.5483892e-07, 0.99999905], [1.058106e-07, 6.348766e-08, 0.9999999], [5.8619908e-06, 1.1680346e-06, 0.99999297], [0.00019748813, 0.00020405206, 0.99959844], [2.7585558e-05, 2.2657096e-05, 0.9999498], [7.2948237e-06, 2.538556e-06, 0.9999902]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_dc = [0]\n",
    "tpr_dc_ = [0]\n",
    "\n",
    "fpr_dc = [0]\n",
    "fpr_dc_ = [0]\n",
    "\n",
    "result_dc = 0\n",
    "result_dc_ = 0\n",
    "\n",
    "for i in range(101) :\n",
    "    result_dc = getParam(pred_list, i*0.01)\n",
    "    result_dc_ = getParam_(pred_list, i*0.01)\n",
    "    \n",
    "    tpr_dc.append(result_dc[0]/(result_dc[0]+result_dc[2]))\n",
    "    tpr_dc_.append(result_dc_[0]/(result_dc_[0]+result_dc_[2]))\n",
    "    \n",
    "    fpr_dc.append(result_dc[1]/(result_dc[1]+result_dc[3]))\n",
    "    fpr_dc_.append(result_dc_[1]/(result_dc_[1]+result_dc_[3]))\n",
    "    \n",
    "fpr_dc.sort()\n",
    "fpr_dc_.sort()\n",
    "\n",
    "tpr_dc.sort()\n",
    "tpr_dc_.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_abnormal:  0.9685884504520711\n",
      "auc_normal:  0.9694429097014318\n"
     ]
    }
   ],
   "source": [
    "print(\"auc_abnormal: \", metrics.auc(fpr_dc, tpr_dc))\n",
    "print(\"auc_normal: \", metrics.auc(fpr_dc_, tpr_dc_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xt8zuX/wPHXtSMzZ5JDM+ewzRyGDrQ5JIT4FkISRVTfDg4lYUgO8YtUonImrQNJJcp8pZKRmfP5NOc5xTY7Xr8/rt3bve3e3GO3bfZ+Ph734971Od3Xpq73fZ2V1hohhBACwCmvMyCEECL/kKAghBAilQQFIYQQqSQoCCGESCVBQQghRCoJCkIIIVJJUBBCCJFKgoIQQohUEhSEEEKkcsnrDORUuXLltLe3d15nQwghCpRt27ZFaa3L3+y6AhcUvL292bp1a15nQwghChSl1HF7rpPmIyGEEKkkKAghhEglQUEIIUQqCQpCCCFSSVAQQgiRymFBQSk1Tyl1Xim1K4vzSin1oVLqkFIqQinVyFF5EUKIWxIRnDf3p9w3dSo0awa9h2/De4Y3Gz5X3PvS0zRsGUmHDreXtaw4sqawAHgsm/PtgVopr4HAbAfmRQiRX+VVwWuPXeMIDU1/KDQUOnTA5vGpU80r9dyucenO2ZLueqvP7dQJAgIgPCKBZdMacnz2DAI94NxnnxH+eyVK199227+eLQ6bp6C13qiU8s7mki7AIm32A92slCqllKqotT7jqDwJUaBEBINfcP5+dm48Z9e423uG9f1aQ3IcJMeDswc4uUD8VYg9DcVrgpMrXD8G1w9BUjwkx7FqZRw1q8dTr04cJJl7Dx+KY9W+V3i9Msx8O5RqI0Px7jSO0A2KL8avYszTW9k0H6pdA29vOHYMtn0D3Z802fhmPqDHEQRsW/0NGxcfxufZFzhx9TpuJ76mfNIlnJQzUdf+pUWlWH6fVw599CzuReKJvaFoUwR+/BGKlD9F7zc64x7Wj0/XvWwenuABjw7lj8orgGO3/nfLgnLkHs0pQWG11trHxrnVwGSt9aaU9G/Am1rrTDPTlFIDMbUJvLy8Gh8/btccDHG3sKfguZXCKS+fa49lCno56P/P3Hh2chIsd4Fu5yEpNu2VGGPei9eGYvfBjSg4+S1UfBQ8q8HVvXD4i7Trj8wHrx6gEyA5EZITQCeCbzCUfxDOb4Jtr8IDi6BUfTgeAuFvmmuSE+DGObSLJyTFoXRCWv7abSG6eD0u7JiI98FJzOFnvAI01SIXcf+Z5Tn+dWft6s7r/7eApRNa0b3y3wDoZEABGlAaUCg0SmW+P/hoJcZN38jKsQF0Ln/F6n7z76DA5n0x8W54uMVnOr4hBgKft//fUCm1TWvd5GbX5eWMZhu/PjZ/Q631XGAuQJMmTRwXxcSdY6vgzKowteeb5M2uSYozBZBbKZOOPWvuqdYn63uUU9pzY8+ab6DFqppzMZGQdCPzPVqbe7yeBOUCJe83xy+HAwpKNzDps79CwrWUgs2qILSkdWL65x6eD0UrQqWUFtn9s0AnZf83KXF/+utLN4R7HjaFNsDOceZ3SIpNec/ws9dTUPN5iLsEa5uTXG8UN7yeIvHSdoqvD0RZ8vjdPbY/v/EsdO2XWBU+my5HxvBjhefYW7Qel35IYoLPDJyVVf5PfAXAFdy5QFH0dS+i4xO4r2UUb/wwkAEukYx9bxQR+5oT2PZ95lYsQrn402n/VInXAdhwqSS/JF+l8/3/4QGPKiz5PpKpX14goHZdfphzPzFdnqNqrQ28Xen/+PbjB2nX8wyfXRxM/JWqxK2aQ5znGeLP12PkjMP0Dgrkvk1OlFnxFlyrzOVfzDf1t/c8Tq0+H1MyphH1fZKIu+FM/Se/o9YjW/l1/FCCXg7ho1c7MuktL2a3UDRaFEHf4N/QbuVp1lyzp8oigh5vyZVzJZiz6BJ+rfZSpQrMfq8GS+fcy4CXL/L5A+U420rTqnUye/c4AYmAM3qpE6p3Ejw6lKrtVzignpC3NYU5wAat9Zcp6f1A4M2aj5o0aaJlmYt8IqtCPONxW9fZ+qa6TMETkRB7xrxunIGqveDr4vDIj3B8OTT/wjQB7J9l0pZC7N+94FHFNBskJ6S8x8OTV8GlKGx9FY4ugqcum8/6/Sk4+U32v59rSUi4avK5qTtc2QWP7zHn1rWAC5uyv79kPei4O+X6h8HJHVr/ZtKrasD1I9nfbyXJpQT/lmvBoTpjiUuK48E/W+OUnPnbYzrefeDBxczcPJPBR4ZywqkcNZPPZbosHieuaWfitOKGhhsa3NxKUrPxOKg1GN9ZNXjb7QgL/oW1MTD1RhOG+2b+f/CbY7VY4naQWA2tanTgzfafQ9GKFBnvTBmnZC4mQ7wGjgZSZMUP/LTSkwda3KBISFGcB0ZxT79XKFV3O27ObgTpcSwd8wTzllxndlR3/t3fmK0zhtNy2MdU9T/Ck/WepF3NdlyKvUSZFWV5776JnAyvyeIxnRg0+Vee71aTs7vq0qWLpucru3mq/xl2/X0Pw/r60ajpdY4eLMZXy5149FHNf4dGM3Z8IpPGF2PqJFcefRR++QWSksD5K8X8OM2IETBoEMyeDcuWQbt2ph+ge3cYPNgcDwmBoCDzt7CcuzBTUf5Vne6cLRmfdWGm4v/OaoYOBaWS0VpB7e/RY7ui+l2DBA96DdvO0vcb2/3fkL01hbwMCh2Bl4EOQDPgQ61105s9U4LCrVu6cymjfhvFiasn8CrpxcTWE+nt2zvrGyyFeVaFf1ZNEBmPL1Pw5BX4dx+U8gGXYuZYnVch5iREnzDvNzIXWJnUGwn+78GBj2HvdIg+mvmaMk3NN2InN9ME4ewO5zbA1d1w40Jq55+1mArt+PeeNiQmJ+J5bi2lojL2/MGxkg8QWukF4pPiqXAtnA73BeDm7MbO8ztRJ1fgE38wc158xvJJ3D3s2DOf+OQEdiW6Ep8UTw0Vy3fdvwblwvjf3+Pbfd8TnRhPTFICiUBx91IcrnwFemn+E/If1u3/jiQNMSl/Vp9SVdg5eCcA3UK6sf5o+vzWLXc/f72wFVyK0WZRGw6d3QpO7miXIrg7u3Gg7CF4OgmUEwHPfg2Vw6jouw83ZzfcXdwpfqoz1WN7MGIETN40mevx13F3dsfdxZ3j4dVZMrozK791I+iM4t2ETbz/ehNGzgyn6cPRuDu7U7F4RaqXrg7A4YvHKOrmhruzOyePuZMQ6861qy706KFo0wa+7KQIragJCoLPP4f4eBgyxBSUjz9uOlt37zaF7ldfQYkSaZ22XbrA9z3S/nurUwdOnIDhw03h6uQEjz4Kixeb65s0gW3bYPRoGD8eVq+G2rXh1CnbBfzRVcE0HRCcmrYU3iNHwqRJZDoeEmI+x/JzUNlgQi8Gp6VtBAbrey3P+nNOMKNDgunQAS5cgASPE2zfUIXQ6V70PNyCijvfp6JnFX76KfPzspLnQUEp9SUQCJQDzgFjAVcArfWnSikFfIQZoRQDPGerPyEjCQqZ2VPYL925lIE/DCQmISb1mIerB381bYdfm+/SP9ASBCyF+zIFT12Fs79B2QDzjfzaIfihFtR5DRKvm6aQxOuQeA3ObzTNLHGX4MGlsLEzV5svpeTmbAJQBvtKPkwR//fwrvggLHdhXKVg4pPiiUuKM++JcbzU9CX8Kvjxd+TfNNvYnEeT2qY7/3nnz2lUsRHf7vmWIT8NST0elxRHsk5G1wJ6aT4J+4SXfnopUx4OvnKQmmtqMbXqFN789c1M588MPcO9nvcyNnQsU/6YklqgXqgcRfUL1dg1ZBcerh7M3DyTb/Z+g7uze+o1RVyKsKzbMpRSLNu5jK2nt6Y77+nmyX8Pvwq9NBuPb+TUv6dSz7k7u1PcvTjNqzQH4MTVE8QnxacW2m7OpgAu6lrU5t926lQYUSWtIA0Nha5doUcPU1iuWGEKzJAQuOceOHDAnAf44w/Yswdq1kz7JlzsBc3q1aZAmzQJtmwxzwDo0we2boV9+0y6Wzc4eBB27oQxY2DCBHi/bzDDFgYDZlRPbGzaaBxfX9i1K60QHzzYBIUpU8z5t96CztWCeXCQuX/KFPj9d9NJO3o0PPcclC8Pnp5Zf7O3VShb0mFhJihZF+ahofD++ybwZDweFmZ+tnVPWBiMGGH73yPj9c2amb/9Dz+kHfu//4NffyVHgcBangcFR7lbg8LSnUt59edXuRh7MfWYk3IiWSendF2Zf6eyRcsys/3M1EI/Y2E/tgy8f82DuZ3mpgWGiGCq/Po5p66dyvS5uhbop5OZ/td04hLjSE68zuizk1ld9CEej/0Dmn0Ofz/P6y7t+CDxFzbpCjysMn+jj9VQ1FYvUQaTL8HIi+ZzP609mxebvMi209to8lkTdC1QVl+2l3RdQm+/3rBMoQ6Ci5NLaoHn7uLOgi4LaFezHRuPb6TlH4/wYOwDqQWnm7Mb77V6D98Kvmw5tYV52+elu9fN2Y0xp8ZCL83eC3v58+Sf6QpUdxd3Wni1oNg3npzrfJZz0ecyFbqli5bGSdkY1Z1bHcS30WFtq6CxFEwBARA2L5iA/sGUKgXvvQdr15pOTj8/U6j+9hu0amW+EU+fbr69AwwdCnPnwrVrplB32h3MB78Gc/WqOT99uvkmvmyZSX/zjfkW/uqrJv3PP3DjBsTFmYK3Xz+YPx++/jrzt+jsmmeykpsFf1aFeEElQaGAaLOoDb8d/S1H97g6uTLioRE8cf8TJK59mAeOx6WesxSsRZyL8N9m/2VK63dhuRvqIHQsBpeS4K8bZoLK0nuhZ3GgdEOOnt9OaSco5Zz9Zy+Iv5cvdE3cnV351SUUemn+jfuXl396OfWb7sdXP2HkPW/RvlZ7WlZtCcsUs2p8mPot183Zjad39uJo+yNUK12N6Phodp3fRbONzdnTdnfqNeU8ylHUtSg6YizJPmNwdsomc3fj6COyL9wh+4LfViH4wQewYwc0bWqaaFq2hO++gwUL4PBh8829Vy/T3OLkBKdPw8WL5hs7wL//mgCxc2fOC2zrPGZVQGdsk8/umpw8t7AU/NmRoJCP3UogsDa2DIy7BC28WrDR/XeqHoX6buDjDlPLwY/RcJ8L1CpShKIVWsLZtZSPLMffFaL4MxYOJUBwWTs/zGesaYO3p+8gq+O2rsvJ6KNCLLuCDmyfmzABOneG/fvhqaegWjU4dMgU/qVKQfPmsGqVaQqaMMHUBtq2tb+Qv5UC21p2gc5SQNtzza08tzCToJAP2GoSulWhlSEopfVH1wKng4pzrXpT/uSSHD1n6mV4M8r87OHqQbR3jO1RQJa+BOv3jG5n9JGw2/ffQ//+8NJLpsBu0cI0u3TubEbJdOhgCvVt22DePHN8+nR44w0YNsz83LmzeU5ysvmm/9dfaUFg5kzTdLRihX2FvBS+BZO9QUEWxMtFHu96oMap1Fef7/rcdkAYW8a8B3rA8NIQ4WXSybV0poCwKda8FzvmwVLfJaYgtxTmvTRLfZfwSUJVFIqqJasyt9Pc7D/cZ2z694yyKugzHpeAkCVbSxyEhppv8AAJCdC3L1Svbo4NHgx//22ab8CMrPH1NcFh8GAzWuf77+E//zHPWbgQ3nkH/vzTpJ2c0gJCSIjpvO3Z00yvsAgKSmtysWXEiMzBIihIAsLdQoJCLli6cylqnCI2KfaWn2Ep/DOmg8tC+ZSm9KnlwNfdxuf7LsH7fFVaRpqe3nSdzFZ6+/bm2GvHSB6bzLHXjplrbBX4lmOWwlwK9RzLqrDPuP6Npe1//fq0ax57LG30jqsrvPyyaf4ZPdrUFJYsgVGjzPkNG0xHruXchg2mVnDkSFrBP2GCee/ePe0bvXUtYM4cWLkyfRCQQr7wkuajHFq6cymDfhhEdEL0Ld1v6Q/I+HPGkTe6FpxNhHuzmnNuqQFkbNqRtvp8Ibt292rVYPv2tGGezz1nOnZHjoRPPzVNRTVqwMCBt9anIB2rwhbpU3CApTuX0ue7bJZFyIKtwl8BybWg2lF4pgSMLwuL/4XSzvCQZzFKaxtBx2ds+vkDAOsCoe2GW/yNRG7Iqo19+XLTudu2rRmauWqVqQVMmgRvv21G8hQvbq6ZMsWM77eMx7/Zs29lPLwo3CQoOIDLeBeSbrbeTAZjy5gmIEstQNeCEwkwIgqWV7TzIRlHAMk3/zxxs8Lf8o194UKzJML8+bB3r2m+qVjRFPpVqpjmnosXoV49cHG5tfH4QuSUdDTnkiE/DsFpnBNqnLrlgAApk8RqmZ+9XLMICJaOYavOYcAEgPKPpF0nAcGhsuoPOHw4rV1ea9O52727WWqhdWvTHDRmjBn1U6QInDxpCvnRo02H8cGULwaVK5tJYtYBwdLpa932L0RekKCQhTaL2qDGKWZvnZ06mzgnrANCJhkL/uxmv1o6faWJ6JbY2+FrzdL5a7kvNNQsz9C4cVqhXaSIORYSYkYA/fijuW/CBDN09JtvzFIINyvsM3b63mzkjxCOJkHBhsrTK9/W5LLQylkEhKyGdto6l3EEUCFkT4F+s2tsFfDdu5vj1nbvNkswgCmYn3jCjOIZM8Zc7+pqRvYEBZlmnvh4UzMICjLf+FeuhPDwtFFAX39tX2EvwztFfiNBIYMhPw7h9PXTN7/QhuAysNqnCYEe2VxkXfhb/3yXj+3PqvDObltDewr0rK6pUsUsxGYpjLt0Md/oLU01ixZBx45pz3npJXjttbT0qVPmGZa5AatXm2/+oaFpTUK//GLSoaFmrL91reC7DGsMghT2omCQoJDBnG1zcnxPUeei6K5jGVsWOtZoY/siy8gh68L+Liv4s5NV4d2mje3jjRpB/fppzS4vvGC+uVu+fS9ebIZuWgr9jh2hXLm0Qn/NGngyZWvEoCCoWhXWrTMFfFAQNGhgVqK0mDYNZs1KSw8fDlFRad/8o6PNaqG22v+XL5cmIHH3yMud1/KdpTuXkqyTc3RPUeeixLwTY4aJAuyZnPmi8o8UqgAAmUfqBAWZgrZTJ9MRO3Mm3H8/DBgADRua5ppSpSAmxhSoGzaYb/bx8aYgnzDBLMUQGGied+qUGetveXb79ubb+ejRJl2mjAkkYALN2bNpBXxQUPpaAZh19i0yzg0ICjJpSx+CrcLfVhOQjCASBZHUFKy8+vOrN73GxcmFJd2WoMdq9FhNTLOmaQHBFo+qhbKTOGPNYO5cs/b9Qw+ZAr5t25Q91pNN4dm6tdkc5bnnTLpjR/PNff16U5APHQqlS6c976230oJCaChs3JhW6IeGmppAixa3Nronq87fGjWk/V8UAlrrAvVq3LixdoQlEUs0wWT7ar2wdfqbdozVeilZv3aMdUhe85MpU7Revz79sfXrzfHPPtO6VCmtR4/Wulw5rf38tC5dOi1tuW/9epPO6nhWaXuuyS5/QhQmwFZtRxmb54V8Tl+OCApLIpZoj4ke2QaEJRFL0t+0oqrWax8xhX/4O2mBQGvzvvaRXM9nXsmuYLUuhLdv13rcuLT0Aw9oXbmy+a/smWdsF97Tp2ddqNtToEuhL4R97A0KMqMZ8J7hzfGrx7M8X7ZoWaJGRKUdiAi2uc8v5R8xTUV32dITWa2/M2oUXLkCjzxi0pUrQ0SE2TKwVSvTZDRypBnZM326aTZ64430z81uW0NplhEi99g7o1k6mjF73GbF1cmVme1npj94eXv6dC+dfumJuyAgWHcUW9rUO3Uyk7aUMunNm01hP2xYWmfw66+bgBAaaoLGN9+k76xt2DB9W72tzljppBUi70hHM+BV0svmcWflzPwn5qctQ73S23Qqn1qV/sJlytQc1gU6NJ93UsaOYoDERLNmT69eptB+9VVTUwgLSxu7v3ix7eWZZZimEAWDBAVgYuuJeLimn3Hm4erBwq4L0+9LUMzb9gPKP2JqCwW8hmA9wSwoCL76ykwuq1PHDBktUsR8+1+2zFzn4WFG/dga3ZNx4TjLM6VJSIj8TYICZvOZuZ3m4qTMn8OyK1m6gBARDBf+lzcZvEMy1g4sWzceOGBqCStWwLvvZr9hi9QIhCjYpKPZSsnJJXnO/zlmPDYj88mIYNBJsPvdtGOWlUsLeA3BWmio2ex9wACz4YtSZiG4rVvN+j6Wwl86g4UoWKSjOYe01lyPv46nm2fmk+sCbdcSLvwv+0Xu8rGs9gbYsgWSksx5Dw+z5o+tzdylM1iIu5M0H6WIS4ojWSdTzLVY+hM7xmTdbGRZz6gAsm4q2rMHnn8+bTXQpCSz7IOL1VcGaRYSonCQmkIKVydXtr6wlXs97007mFUNAQpcQLC1FtFbb5lhpg89BGvXmtFDkybB999L7UCIwkpqCimcnZxpXKkxlUtUNv0HBTQg2LNr2KlTZuev8ePNIm9r15q1hTw9pdNYiMJOagopLsZc5Nu939I9aQ+lDs3M+sJ8GBCsawGWZqGRI82IIUv65ZfNq3t3GDjQ7B3cu7fZMcyykJytvYGldiBE4SJBIcWRy0c4/ecgSmW1hSbk2yWwLQW/pVAfOdLMMvb3T9sSMjjYXGuZefzMMyYgZFweWjaNF6Jwk6CQotzhj7LeU9myplE+Y11DCAmB//wHHn/c7CtQs6ZZWtqyv0DdurBrFzz9tDlmWYvInr0BhBCFh0P7FJRSjyml9iulDiml3rJx3kspFaqU2q6UilBKdXBkfrIUEUy1yEVZn8+HAQHSjyAqXhyuXjXLTDz4IFy6BO+8k7a/wN69JiBYZh6vXm06la37H2TGsRDCYUFBKeUMfAy0B+oBTyul6mW47B0gRGvdEOgJfOKo/GQpqxVPwWyQkw/mIWTVeRwWZpai6N7dbCeZnGxWLP31V3j7bVMTkC0jhRA54ciaQlPgkNb6iNY6HlgOdMlwjQZKpPxcEjjtwPzYZquPwGesWcvoiWP5og8hq/2Nr1+Hvn3NrmVffWX6CTp0MAHCUguQXcOEEDnhyD6FysBJq3Qk0CzDNcHAWqXUK0AxIItd7x0ku1pCHrO1dHXnzuDsDE5O8O23Zsbxn3/CqlUmIPz8c9p2lg0bpvUPyAgiIYS9HFlTsLVxccaFlp4GFmitqwAdgMVKqUx5UkoNVEptVUptvXDhQu7kLh8HBEhfO0hKMseSkky/weOPm0I+JgZ27DCjiBYtSr9QndQChBC3wpFBIRK4zypdhczNQwOAEACt9V9AEaBcxgdpredqrZtorZuUL18+d3LnF2yaiHqlxamZ1WeYdB41GWVcujokBB57DO691yxd7eZmRg79/LOsUCqEcAxHBoUwoJZSqppSyg3TkZxhdxpOAK0BlFJ1MUEhl6oCN7Eu0GyOsyytQvPqkdfu6EY5GTuQAwKga1cYNCjtmNYQFQUJCWbpatmzQAjhSA4LClrrROBl4BdgL2aU0W6l1HilVOeUy4YCLyildgBfAv30nVrLu+0GUyuoPwqAdS71qXSm4h0dfmqriUhrmDcP+vQxtQMPD9OR7Oqadp/UCIQQjlJ491PIqk/hDi1jYelIjokxhX+TJmYuQffupo/g9Onsl64WQoicsHc/hcK7IN65DTk7nssstQRnZ2jTBjZvNruc3X+/eW/dWpauFkLceYW3pgAQexYiV0LYYFontiIuMY5N/TflzrOtZLWhzaefwpo1ZuE6rc1QUxcX03cgtQMhRG6SmsLNRATDiooQNhiA31zWs6nIH+Z4LrM1+axLF1PYx8ebJqRhw8yqpdYxWmoHQog7rXDXFM5vgl9bmJ97OebvYKklgNn7uEcPWLrUjDLaswf27YNXX01buhpk72MhRO6TmsLNRASnBQRIG56ayzUFSy1Ba7No3SefmNqBry8cOQIrV6YfZgoSEIQQeafwBoV6w0E5pybLnyrH4BIv5trII8scBEsTUNeuZsczZ2czCW3/fpl4JoTIfwpnUIgIhhBP0Emphy5UjqJrwo5c+wjrfoRz5yAuzkxA69XLdCR/913me2TimRAirxXOoJAFN2e327rf1jIVjz9u9jGIi0tbtA6kViCEyJ8KX1DIYtJa8EUIK9vxth6dcZQRmOGmYGoI1ovWgdQKhBD5T+ELCtnwdPO85XunTjXvISHw5JOmI7lTJ7PxzTPPwNq16fsYpJYghMiP7AoKSik3pVRNR2cmL73TYhQDGg245fsttQSAli3NfsjR0TBliixrLYQoOG4aFJRSHYGdwLqUtL9SaoWjM3anuTi55LhPwVYfQqdOZu0iFxcoVsxsdmN9XmoIQoj8zJ6awnjMjmlXALTW4UDBrTVY9lFom7KcRS/N4ccO8UrkRQ5cPJCjR2XsQ5g509QOEhJg5Ej44Yf056WGIITI7+wJCgla6ysZjhWsadDWIoLNJLV1D5v0MkWNNTUpe/gjzkeft+sRGecgdO9u9kpetcqsX/TOO2aGMkjtQAhRsNgTFPYqpboDTikb5swANjs4X45jqSm0+s2ke2lWNlrBuEtQzLVYlrdZNxVZagiTJ8OYMdCiBSxebCalrVsHEybIKCMhRMHkcvNLeBkYAyQD32E2zRnpyEw5VMYhqcsUTwBjy0Axt/RBwXp1U0sgeOMNs0/yyJFmETswS1g0agSHDoFK2cjNug9BVjgVQhQYWutsX0A3e47dqVfjxo31bbvwl9ZLMS+t9ZytczTB6MirkekuGzhQ65IltV6/3qSnTdMatC5RQuty5bRu29akn3nGnF+/3hy3XC+EEPkFsFXbUcba03z0jo1jo3IxLt15hz9Pl4xPisdJOaWrKUydCnXqmFpA166mmWjsWNNn8O+/0L49/PFH2ixlmYMghLgbZLl0tlKqHfAY0AtYanWqBNBAax3g+OxldltLZ2ezBaf2HQuASmn/sWxwM3IkDB9uJqGBGWbarRssWQLTppnmJNkMRwiR39m7dHZ2fQrngV3ADWC31fFrwFu3l7084hdsXqvrwr/70u2hoKwus/QlhISY/ZOV1cnx483SFdOmwaRJZh6C9B8IIe4WWQYFrfV2YLtSaqnW+sYdzNMdN3/7fMLPhjOz/UwADh+G994zQ0vj4iApyQQGJycTFCzbZTZsmBYILC8hhCjI7OlTqKyUWq6UilBKHbC8HJ6zO2jjiY2s2GcmaVv3JQx/ksydAAAgAElEQVQfboKCs7NJDxxo3pcvN/fJZDQhxN3GnqCwAJiPaWFpD4QAyx2YJ8fSGmJOpTsUHR9NMbdiTJ1qlqeYNMksameRlASDB8PXX5vO5ho17nCehRDiDrEnKHhorX8B0Fof1lq/AxTMhpKIYPjSCRKvmXTKFpxd4rdTzLUYAQEmILz1ltlH2Snlr+PqavZXDgkx/QlSOxBC3K3sCQpxygzJOayUelEp1Qm4x8H5cgy/YKg1JC3dS0Mvzejvh5Jw+GGCguDTT+HNN836RcnJZgnsokXNsFSQgCCEuLvZExReBzyB/wIPAS8A/R2ZKYewrHl08JO0Y8sURARTtuYR9s0eT2golCljWpiSk6FaNThzxjQZWfclCCHE3eqmy1xorf9O+fEa8AyAUqqKIzN1J/21GcLem0poWzPXoEwZExAaN4bjx808hUmTTGCw7KImhBB3q2yDglIqAKgMbNJaRyml6gNvAq2AghUYLHMUAJYppkZqjh0zzUVT/4Vz58ypAwegdm3YujX9BDbpSxBCFAZZNh8ppSZhZjL3BtYopUYBocAOoPadyZ4D7DNzEQICzGiiF1+E4SOSmflRAlFRJiBcupR+2QoJCEKIwiK7mkIXzHIWsUqpMsDplPT+O5O1XJZhiYugM4oLM2Hqz2PBYzAJ0RW4914YMCBtRVTLshUyKU0IUVhk19F8Q2sdC6C1vgTsK7ABAdL2UUhZ2iK0oqb8q5pF28dAdAXci8Vy9iwcOyYL2wkhCq/sagrVlVLfpfysAG+rNFrrbjd7uFLqMWAm4Ax8rrWebOOa7kAwZje3HVrrXvZn/9a9/z5UqAC7d5uFjeJiilC/vtkxrWZNs9Cd1BCEEIVNdkHhPxnSH+XkwUopZ+BjoC0QCYQppVZprfdYXVMLs2HPQ1rry0opx89/iAgG4PJl2L0b7q2UxNnTLtxXJ4rdu8vj5QW//mqCghBCFDbZLYj3220+uylwSGt9BEAptRzTT7HH6poXgI+11pdTPtO+TZJvx8UtAJw6ZWYsnz3tjHOpU5zcVwknJzMf4aefHJ4LIYTIl+yZvHarKgMnrdKRKces1QZqK6X+UEptTmluykQpNVAptVUptfXChQu3lhvL5LUzPwNwYrIiabFibLdxJF2pjFKK5GR4/PFbe7wQQtwN7Nmj+VYpG8cy7ujjAtQCAjHzHn5XSvlora+ku0nrucBcMJvs3FJu/IKh5vOw8j4ABv1P89dfsHOn5TPAywu8vW/p6UIIcVewOygopdy11nE5eHYkcJ9VugpmWGvGazZrrROAo0qp/Zgg4ZhxPydT+8lZsgRiYkCpZLR2AjQnTihcHBkmhRAin7tp85FSqqlSaidwMCXdQCk1y45nhwG1lFLVlFJuQE9gVYZrVpKy4qpSqhymOelIDvKfM3unE3ndl8k/jqVECXNIa4WlAuPqCl995bBPF0KIfM+ePoUPgceBiwBa6x3YsXS21joReBn4BdgLhGitdyulxiulOqdc9gtwUSm1BzNberjW+mLOfw07xJ6BmBP8E/Uk734fzNmzlhMKnBJo0OQGCQlQvrxDPl0IIQoEexpLnLTWx5VK10WQZM/DtdY/AT9lODbG6mcNvJHycqg1X/zIY2XgnwsZR9pqSHalfNlkpk83w1GFEKKwsqemcFIp1RTQSilnpdRrQMHZjjNl1NFjZV4AIDjAh+tzFWO7Badd43yDLZvdadhQhqMKIQo3e4LCYMw3eS/gHNA85VjBkLK8xddRprQv/8oNnPpoxn0XnHqJk7NmzGgty1oIIQo9e5qPErXWPR2eEwcr59sezkDpsu5EXUo77uqqcHP14MABmDMn7/InhBD5gT01hTCl1E9KqWeVUsUdniMHCQuDH0+M5eDBtL2XGzcGZ+e8zZcQQuQnNw0KWusawLtAY2CnUmqlUqrg1Rz2TqNd5XepWD46dWe1bdvAtfgl4pKv53XuhBAiX7BrmQut9Z9a6/8CjYB/MZvvFCgRp5ox+Ye3iLrkSrFiJiAAaNdoqgz8LzVq5G3+hBAiP7hpn4JSyhOzkF1PoC7wPfCgg/OV6666t2D01y0ASIhOO16k/Ckq+OxhxPN5lDEhhMhH7Olo3gX8AEzVWv/u4Pw4zNVLMVQsE82ZS+WwLMvk6go3/vXE080zbzMnhBD5hD1BobrWOtnhOXGwd5/5mJYlRlCs/3Vi4ooBmBnMjX6mmFuxPM6dEELkD1kGBaXUdK31UOBbpVSmlUnt2XktPwkNhZZdMh+/uG4gDV+OznxCCCEKoexqCpal4XK041p+5epq3p1TutadnSEpCSqUKklwYHCe5UsIIfKTLEcfaa23pPxYV2v9m/UL0+FcoBRPmWGRlAz33WcCgosL3Htv3uZLCCHyE3uGpPa3cWxAbmfE0S6lzGIuXQpOnoRq1SAxUbN1dxQf/v1h3mZOCCHyiez6FHpghqFWU0p9Z3WqOHDF9l35131e5v3yFShaFI4eNenkSn8RHS99CkIIAdn3KWzB7KFQBfjY6vg1YLsjM5Xbpk6FmqXS0rGxVierrcfTrfodz5MQQuRHWQYFrfVR4ChQ4HcYOHwYLp6BbtXAxWqto5q1Ezh0pA3F3M7lXeaEECIfybJPQSn1v5T3y0qpS1avy0qpS1ndlx/17JkWDBKT0hbEO3ncBR6aRjFXmacghBCQffORZcvNcnciI44UFgYnYprz7spRJCS64u4ObdvCqlWKChFTqFqqwM/NE0KIXJFd85GlpLwPOK21jldKPQz4AUswC+MVCAEB8PbbLYAWJCWBUzJs2ACDByuOHWtK8yp5nUMhhMgf7BmSuhKzFWcNYBFmjsIyh+bKAUqXiKG0xwVatNDExZl5Ck89JdtvCiGENXuCQrLWOgHoBszQWr8CVHZstnLP1KkwfTq82OpjLnx6D9u2xNC2rQkK780+wj3v38PBiwfzOptCCJEv2BMUEpVSTwHPAKtTjrk6Lku5KyDArHtUtFob3lj2EX7+bvz6Kzz3HJSsGMWFmAu4u7jndTaFECJfsHdGcxBm6ewjSqlqwJeOzVbuCQqCCRPgnekNiSrzEocOuzJtGnz9NVSsEwkgo4+EECKFPdtx7gL+C2xVSt0PnNRaT3R4znJRYiIMevY8O/63g8EvJvHGGxASAod2mhltsp+CEEIY9uy81gJYDJzC7E5zr1LqGa31H47OXG6YOtUsfFfm6kJ2TBqB17DrlCpdjMREaPTkb6z7wxk3Z7e8zqYQQuQL9jQffQB00Fo/pLV+EOgIzHRstnKPiwsMGwZNm5r066+btIsLNLi3Af38+6GUyttMCiFEPmFPUHDTWu+xJLTWe4EC89U6MRGmTYPtKas1zZhh0omJ0L1+dz7v/HneZlAIIfIRe7bj/EcpNQfThATQmwK2IF7DhnAj5ecBA0w6LAy01lJLEEIIK/bUFF4EDgMjgDeBI8AgR2YqNwUEwBNPwO8bTXr2bJMOCICuX3XlwS8ezNsMCiFEPpJtTUEp5QvUAFZorafemSzlPqVAOaVPA1yLv4aTsicuCiFE4ZDdKqlvY5a46A2sU0rZ2oEtW0qpx5RS+5VSh5RSb2Vz3ZNKKa2UapLTz7iZsDBYsQJatjTpIUNMOiwMouOjKeYmcxSEEMIiu6/JvQE/rfVTQAAwOCcPVko5YzbnaQ/UA55WStWzcV1xzDyIv3Py/JzYvh3++sv8/PnnaZ3O1+OvyxwFIYSwkl1QiNNaRwNorS/c5FpbmgKHtNZHtNbxwHKgi43rJgBTSesLzlWWIaleKdtxvvJK2pDU6IRomc0shBBWsutTqG61N7MCaljv1ay17naTZ1cGTlqlI4Fm1hcopRoC92mtVyulhtmfbftZhqQumN+G2PiPmL7cLXVI6jMNn6FO2TqO+FghhCiQsgsK/8mQ/iiHz7Y11lOnnlTKCTMxrt9NH6TUQGAggJflK7+dRoww71euNOTlCQ0ZPRreeMNydnyOniWEEHe77DbZ+e02nx2J2aDHogpw2ipdHPABNqTMFbgXWKWU6qy13pohL3OBuQBNmjTR5FBoKIQsOs+Ut84w/VMfgoKcCQzUxCTE4OHqIXMVhBAihSPHY4YBtZRS1ZRSbkBPYJXlpNb6qta6nNbaW2vtDWwGMgWE2zVoEHTtChMHLGSErz+L5t+ga1fo/0ICnpM8mf7X9Nz8OCGEKNDsmdF8S7TWiUqpl4FfAGdgntZ6t1JqPLBVa70q+yfkZl4gqVIX1sXVxNnVHa0hMTkRkGWzhRDCmt1BQSnlrrWOy8nDtdY/AT9lODYmi2sDc/Jse82ZAz17QvfutRk8uDazR8DKleDV4AxLZiHzFIQQwspNm4+UUk2VUjuBgynpBkqpWQ7PWS4KCoKhg44QtupnhryYSFCQGY4KUlMQQghr9vQpfAg8DlwE0FrvwOzEVmCEhsK1Pd/y84gOzP8ijtBQM3ENpKYghBDW7Gk+ctJaH88wQifJQfnJdYMGwVdfwboPTXr+AtPx3P4JH0b1HUWtMrXyNH9CCJGf2BMUTiqlmgI6ZemKV4ADjs1W7tIaSpcGroGLs0mXcC/Bu63ezeusCSFEvmJP89Fg4A3ACzgHNCeH6yDlpTlzTMfykpTdIPr2Nen/mxXN+ejzJOvkvM2gEELkIzcNClrr81rrnilzCsql/Bx1JzKXW4KCoFIl83Pfvia9JGIJFaZV4My1M3mbOSGEyEdu2nyklPoMq+UpLLTWAx2So1w2dapZ/O7qKZNetAhKl4f1h+vCPcgqqUIIYcWe5qNfgd9SXn8A9wA5mq+QlyyrpLZubdJDh5l0IrGAjD4SQghr9jQffWX1Wgh0w+yPUCBYVkldv96kp08z6RvxSbg7u+Pi5LBJ3UIIUeDcSolYDaia2xlxFMsqqXNfM+99+5pVUo/89BM7d0ktQQghrNnTp3CZtD4FJ+ASkOXWmvlRaCh8tqYLV3VNPlvuTqtHoUudLtxf7v68zpoQQuQr2QYFZWasNQBSumlJ1lrneOnqvBQaCt27Q0hIbYKCatPkCUu6LS8Htc3r7AkhRL6SbZ9CSgBYobVOSnkVqIAAEBYGISFQv+oRDmz4mZYPJxISAms3Xub0tdM3f4AQQhQi9ow+2qKUauTwnDjIiBFmXsKeX76l9ukO3IiJIygI/vDqQq9ve+V19oQQIl/JMigopSxNSw9jAsN+pdQ/SqntSql/7kz2co93YG/+KLYZ96JFALNKqsxREEKI9LLrU9gCNAKeuEN5cSjvupXwrlspNR0dHy1zFIQQIoPsgoIC0FofvkN5caiz+3dy7WgY1Vs/g7OrK9EJ0bKXghBCZJBdUCivlHojq5Na6/9zQH5y3dSpEBAAat8aAkuOIDqmB1v+ceXiuv54vnI5r7MnhBD5SnYdzc6AJ1A8i1eBEBBghqCWLGnSf28x6f92fZCePj3zNnNCCJHPZFdTOKO1Hn/HcuIgQUFmSOr3n0LDLvBcP5MOCmqX11kTQoh8J7uagsrmXIESFAQNG5qf+/eHh1smsOXUFqJiCtQK4EII4XDZBYXWdywXDhYaClu2mJ+/+AJW/HyFZp83Y8XeFXmbMSGEyGeyDApa60t3MiOOYlnm4qmnTHrBQnixX2k4GihDUoUQIgN7ZjQXaJZlLvwbmHSrIJj06RE4FSBDUoUQIoO7fjMBy9LZFzdBWcz+CvUCzsHu9ynm9mie5k0IIfKbu76mYLFzp3mPi4Pr8dcBpKYghBAZFJqgcP9jvdlVeTNFPIrQ4N4GfPmfL6ldtnZeZ0sIIfIVVdBWw27SpIneunVrXmdDCCEKFKXUNq11k5tdV2hqCsd37GT7N/NIjE/g2JVjhB4NJSEpIa+zJYQQ+UqhCQpH/1xDw/gBxMXGE7I7hFaLWhGXFJfX2RJCiHyl0ASFep0HsavmcYoUK0p0fDQAHq4eeZwrIYTIXxw6JFUp9RgwE7O43uda68kZzr8BPA8kAheA/lrr47mZB8sqqUFBJbincgkA9oRVwPWvUTipQhMThchTCQkJREZGcuPGjbzOyl2vSJEiVKlSBVdX11u632FBQSnlDHwMtAUigTCl1Cqt9R6ry7YDTbTWMUqpwcBUoEdu5sOySurC6X/h5fYb58qOYPXEZyj29LO5+TFCiGxERkZSvHhxvL29UequWVYt39Fac/HiRSIjI6lWrdotPcORX5WbAoe01ke01vHAcqCL9QVa61CtdUxKcjNQJbczYVkl9Y+Vm/BJHs1zzybw8NBZlK4bntsfJYTIwo0bNyhbtqwEBAdTSlG2bNnbqpE5svmoMnDSKh0JNMvm+gHAz7ZOKKUGAgMBvLy8cpyRoCCIDjM/P/88dB3UiYuxD+b4OUKIWycB4c643b+zI2sKtnJmc1KEUqoP0AR439Z5rfVcrXUTrXWT8uXL5zgjoaGwdZv5+YsvIGqPL4HegTl+jhCiYAoMDOSXX35Jd2zGjBkMGTKEgwcP8vjjj1OjRg0aN25MUFAQGzduTL1uzZo1NG3alPvvvx9/f3969OjBiRMnAOjXrx+VK1cmLs6MZIyKisLb2zvbvFy5coVPPvkky/MbN26kUaNGuLi48M0336Q7d+LECR599FHq1q1LvXr1OHbsWA7+CvZxZFCIBO6zSlcBTme8SCnVBhgFdNZa5/oYUcsqqR07mvT8BdD1yXg+DtmT7X1CiLyzdOdSvGd44zTOCe8Z3izdufS2nvf000+zfPnydMeWL1/O008/TceOHRk4cCCHDx9m27ZtzJo1iyNHjgCwa9cuXnnlFRYuXMi+ffsIDw+nd+/e6QpjZ2dn5s2bZ3debhYUvLy8WLBgAb169cp0rm/fvgwfPpy9e/eyZcsW7rnnHrs/116ODAphQC2lVDWllBvQE1hlfYFSqiEwBxMQzjskEymrpEabUag0awrl+g7h45VbHPFxQojbtHTnUgb+MJDjV4+j0Ry/epyBPwy8rcDw5JNPsnr16tRv9MeOHeP06dMcOHCABx54gM6dO6de6+PjQ79+/QCYMmUKb7/9NnXr1k0937lzZ1q2bJmafu211/jggw9ITEzM9Lnvv/8+AQEB+Pn5MXbsWADeeustDh8+jL+/P8OHD890j7e3N35+fjg5pS+e9+zZQ2JiIm3btgXA09MTD4/cH1bvsD4FrXWiUupl4BfMkNR5WuvdSqnxwFat9SpMc5En8HVKO9gJrXXnLB96CyyrpMaUAPZD0aJAtQ34PxwD9MvNjxJC2ClwQWCmY93rd2dIwBBG/jqSmISYdOdiEmJ49edX6e3bm6iYKJ4MeTLd+Q39NmT7eWXLlqVp06asWbOGLl26sHz5cnr06MHu3btp1KhRlvft3r2bYcOGZftsLy8vHn74YRYvXkynTp1Sj69du5aDBw+yZcsWtNZ07tyZjRs3MnnyZHbt2kV4eM4Guxw4cIBSpUrRrVs3jh49Sps2bZg8eTLOzs45es7NOHSgvtb6J611ba11Da31xJRjY1ICAlrrNlrrClpr/5RXrgYEax5FzbuTE0QnROPp5umojxJC3IbIfyNtHr8Ye/G2nmvdhGRpOsqoa9eu+Pj40K1bt8yff/Ei/v7+1K5dm2nTpqU79/bbb/P++++TnJycemzt2rWsXbuWhg0b0qhRI/bt28fBgwdvOf+JiYn8/vvvTJs2jbCwMI4cOcKCBQtu+XlZuev3U7A4cxYqAgkJEB0fLctmC5GHsvtm71XSi+NXM89hrVqyKgDlPMrdtGZgyxNPPMEbb7zBP//8Q2xsLI0aNWL79u3pOpVXrFjB1q1bU2sH9evX559//qFBgwaULVuW8PBwpk2bxvXr19M9u2bNmvj7+xMSEpJ6TGvNyJEjGTRoULprM3YOjxo1ih9//BEg29pDlSpVaNiwIdWrV0/9fTZv3syAAQNy/LfITqGZ0rt/v3mPi9NSUxAiH5vYemKmJWg8XD2Y2HribT3X09OTwMBA+vfvn1pL6NWrF3/88QerVqV1d8bEpDVdjRgxgokTJ7J3716b562NGjUqXQ2iXbt2zJs3LzWAnDp1ivPnz1O8eHGuXbuWet3EiRMJDw+/aXNSQEAAly9f5sKFCwCsX7+eevXq2fvr209rXaBejRs31rfiatRVHbn/uE5MSNR/nfxLH7189JaeI4TIuT179uTo+iURS3TVD6pqFax01Q+q6iURS3IlH999950G9N69e1OP7d27V7dv315Xq1ZNN2/eXLdt21avW7cu9fzq1at1kyZNdJ06dfSDDz6oe/bsqffv36+11vrZZ5/VX3/9deq1Xbt21VWrVk1Nz5gxQ/v4+GgfHx/dvHlzfejQIa211k8//bSuX7++HjZsWKY8btmyRVeuXFl7eHjoMmXK6Hr16qWeW7t2rfb19dU+Pj762Wef1XFxcTZ/T1t/b0xf7k3LWNlPQQjhcHv37k03gkc4lq2/t+ynkMGBP//i73nvEnX1Al/88wVHLh/J6ywJIUS+U2iCwukdm2hWZDTHLxzn+R+eZ+tpqW0IIURGhSYoBDwzlHNBCSS6JwHI6CMhhLCh0ASFYp5OVKjoQkyiGTlQzE2CghBCZFRo5ikc2LiO2IMr+TegDYAMSRVCCBsKTU3h9O5wGhT9hGsxZsywNB8JIURmhSYoPPSQee/k04aIFyOoXrp63mZICHFHOTs74+/vj4+PD506deLKlSu39JzAwECaNEkb2bl161YCAwOzvefYsWMsW7Ysy/PBwcFUrlwZf39//P39+emnn1LPTZo0iZo1a1KnTp1My387QqEJCq4pDWUli5TAt4Iv7i7ueZshIYRNU6eaJe+thYaa47ejaNGihIeHs2vXLsqUKcPHH398y886f/48P/9sc08wm24WFABef/311JnNHTp0AMzKqMuXL2f37t2sWbOGIUOGkJSUdMv5tkehCQonUvaA23RkC7P+nkWyTs7+BiFEnrDsq24JDJY9UQICcu8zHnjgAU6dOpWatrXEdXR0NB07dqRBgwb4+Pjw1VdfpV4/fPhw3n333UzPTUpKYvjw4anPmjNnDmCWy/7999/x9/fngw8+sDuf33//PT179sTd3Z1q1apRs2ZNtmxx7LL/hSYopOyZwU/71/H6L6+jbG4MJ4S4EwIDwbLAZ0KCSS9ZYtLNmkGlStClC4wZA089ZdKXL5vzUVHm+h9+MOmzZ3P22UlJSfz222+peyhYL3EdHh7Otm3b2LhxI2vWrKFSpUrs2LGDXbt28dhjj6U+44EHHsDd3Z3QDFWaL774gpIlSxIWFkZYWBifffYZR48eZfLkybRo0YLw8HBef/11m/n66KOP8PPzo3///lxO+WVPnTrFffel7VVWpUqVdMHMEQpNUHj4YfOe4HwVTzdP2S9WiHysdGlo2xYmTID+/U36dsXGxuLv70/ZsmW5dOlS6mY1WS1x7evry6+//sqbb77J77//TsmSJdM975133slUW1i7di2LFi3C39+fZs2acfHiRbuWyx48eDCHDx8mPDycihUrMnToUMCsTZeRo8uuQhMUXFL2oVi4YwFX467myhZ/Qohbs2EDpGxuhqurSffpY9IeHjB2LGzcCKNHw/z5Jm3Z4qBcOXO9ZT+be++17zMtfQrHjx8nPj4+tU9BpyxxbWnPP3ToEAMGDKB27dps27YNX19fRo4cyfjx49M9r1WrVty4cYPNmzenHtNaM2vWrNRnHT16lEcffTRTXp577jn8/f1T+w4qVKiAs7MzTk5OvPDCC6lNRFWqVOHkyZOp90VGRlKpUiX7fuFbVGiCwtp/zNrZ0bHxALmyxZ8QIvdZ+hBCQmD8ePNu3cdwu0qWLMmHH37ItGnTSEhIyHKJ69OnT+Ph4UGfPn0YNmwY//zzT6ZnjRo1iqlWPeDt2rVj9uzZJCQkAGa3tOjo6EzLZc+fP5/w8PDUUUZnzpxJPbdixQp8fHwAs/Xn8uXLiYuL4+jRoxw8eJCmTZvmzh8iC4Vm8tqmiFM86g8ku4Kz2Us1JiGGUb+Nordv77zNnBAilWVf9aAgkw4KMumwsLRjt6thw4Y0aNCA5cuX88wzz7B3714eeOABwOy7sGTJEg4dOsTw4cNxcnLC1dWV2bNnZ3pOhw4dKF++fGr6+eef59ixYzRq1AitNeXLl2flypX4+fnh4uJCgwYN6NevX6Z+hREjRhAeHo5SCm9v79QO6vr169O9e3fq1auHi4sLH3/8ca5vv5lRoVk622msE05akeSUjHUfs0KRPFZGIgnhSLJ09p0lS2fbwau0F0nO6QMCmK3/hBBCGIUmKIwrM4hPEprhlpTWYpYbW/wJIcTdpNAEhcrXi/NklcO4JLsCZhPwuZ3mSn+CEEJYKTQdzW2GvEzkpnBizn3B6qdX07F2x7zOkhBC5DuFpqYAUOXEF+a9RJU8zokQQuRPhSYo7FqzEgD3JBcJCkIIkYW7PyhEBMMyhc+lrgDcuD+RsivLmeNCiELDeunsp556ipiYmLzOUpYCAwO5laH3ueHuDwp+wdBLg7+ZdRj9xDmT9gvO02wJIeyQi1/erJfOdnNz49NPP013XmtNcrLMWbr7g0IGsjezEAXIrnEOeWyLFi04dOgQx44do27dugwZMoRGjRpx8uRJvvzyS3x9ffHx8eHNN99MvcfT05OhQ4fSqFEjWrduzYULFwAIDw+nefPm+Pn50bVr19QVTj/88EPq1auHn58fPXv2BMxy3P379ycgIICGDRvy/fffA2axvp49e+Ln50ePHj2IjY11yO9tj0ITFA4dNu9fhn+btxkRQsCvgTd/7Z2W/vojC8zPN6IyX5sDiYmJ/Pzzz/j6+gKwf/9++vbty/bt23F1deXNN99k/fr1hIeHExYWxsqVpj8yOjqaRo0a8c8///DII48wbpwJWH379mXKlClERETg6+ubenzy5Mls376diIiI1FrJxIkTadWqFWFhYYSGhjJ8+HCio5aAVlIAAAwVSURBVKOZPXs2Hh4eREREMGrUKLZt25aj3yk3FZqgcOaMWc4j4vSePM6JECJb14/B+f/B9uEmvUyZ9MmVt/VYy9LZTZo0wcvLiwEDBgBQtWpVmjdvDkBYWBiBgYGUL18eFxcXevfuzcaNGwFwcnKiR48eAPTp04dNmzZx9epVrly5wiOPPALAs88+m3q9n58fvXv3ZsmSJbi4mNH/a9euZfLkyfj7+xMYGMiNGzc4ceIEGzdupE/KMrF+fn74+fnd1u96Oxw6T0Ep9RgwE3AGPtdaT85w3h1YBDQGLgI9tNbHHJEX/6bXYQ9433OPIx4vhMiJNhvsu26ZMn2A1oqUs/9+K5Y+hYyKFUtrUs7JWnA329fgxx9/ZOPGjaxatYoJEyawe/dutNZ8++231KlTJ8fPu1McVlNQSjkDHwPtgXrA00qpehkuGwBc1lrXBD4ApuR2Pjp0gN7Dt/HB5hkATNo0id7Dt5GyjLkQQqRq1qwZ//vf/4iKiiIpKYkvv/wytRaQnJzMN998A8CyZct4+OGHKVmyJKVLl+b3338HYPHixTzyyCMkJydz8uRJgoKCmDp1KleuXOH69eu0a9eOWbNmpQaf7du3A9CyZUuWLjXL+O/atYuIiIg7/auncmRNoSlwSGt9BEAptRzoAli333QBglN+/gb4SCmldC4u3Vq6/jaWTWvIsEFtoOUKLvzWi2VrGtJr2DZMBUUIkW/5jL2jH1exYkUmTZpEUFAQWms6dOhAly5dAFOj2L17N40bN6ZkyZKpezYvXLiQF198kZiYGKpXr878+fNJSkqiT58+XL16Fa01r7/+OqVKlWL06NG89tpr+Pn5obXG29ub1atXM3jwYJ577jn8/Pzw9/d3+J4J2XHY0tlKqSeBx7TWz6eknwGaaa1ftrpmV8o1kSnpwynXRGX13Jwune09w5vjP3flJV2dj/r9l2L9/yXmkTFUbb+CY68du7VfTgiRI3fD0tmenp6pG/Hkd7ezdLYjawq2GsgyRiB7rkEpNRAYCODllbOlrp9zPs7YZ2ekpqPnlQBg3MUcPUYIIQoFR44+igTus0pXAU5ndY1SygUoCVzK+CCt9VytdROtdRPrXY7sMT+pKmrha6jeSQCo3kmoha8xP6lqjp4jhCjcCkot4XY5MiiEAbWUUtWUUm5AT2BVhmtWAc+m/PwksD43+xMAHjr1LaydDo8ONQceHQprp5vjQggh0nFY85HWOlEp9TLwC2ZI6jyt9W6l1Hhgq9Z6FfAFsFgpdQhTQ+iZ2/m4vLsxvYZt44/KKxh3Eaq2X8FDfn24vFs6mYW4k7TW+WbY5d3sdr9XF5o9moUQeef/27v/WKvrOo7jz1cJAomUMJt2TXCihYRE5Ci3jDBHtqAcAxyoNK1JWVOjrUZb9uMPp7kWqSEZAzc1gmndmY6coTjGRVgKCNMkILubC8aIMcVSfPXH58PxeDlwvvfHOfeec96P7W7nfL/fc77v9zn3nvf5fr7f+/7s2bOH4cOHM3LkyCgMNWSbAwcOcPjwYcaMGfOedQPhRHMIIQDQ1tZGZ2dnqV9QqJ0hQ4bQ1tbz6QGiKIQQam7QoEHHfXMNA1PL9D4KIYRQXRSFEEIIJVEUQgghlDTc1UeS9gP/7OHDRwEnbKHRpCLn1hA5t4be5Hyu7ar//dtwRaE3JG0pcklWM4mcW0Pk3BrqkXMMH4UQQiiJohBCCKGk1YrCsv4OoB9Ezq0hcm4NNc+5pc4phBBCOLlWO1IIIYRwEk1ZFCRNl/SypF2SflBh/amSVuX1mySNrn+UfatAzrdK2ilpm6SnJDX8hBLVci7bbpYkS2r4K1WK5Cxpdn6vd0h6qN4x9rUCv9sflbRO0vP597uhZ2CXtFzSvjwzZaX1krQkvx7bJE3q0wBsN9UPqU33P4DzgMHAVmBcl22+BSzNt+cCq/o77jrkPBUYlm8vbIWc83bDgfVABzC5v+Ouw/s8Fnge+FC+f2Z/x12HnJcBC/PtccDe/o67lzl/DpgEvHiC9VcCT5BmrpwCbOrL/TfjkcIlwC7bu23/D/g9MLPLNjOBlfn2GmCaGrufb9Wcba+z/Ua+20GaCa+RFXmfAX4G3AG8Wc/gaqRIzt8A7rF9EMD2vjrH2NeK5Gzg9Hx7BMfP8NhQbK+nwgyUZWYCDzjpAD4o6ay+2n8zFoWPAP8qu9+Zl1XcxvbbwCFgZF2iq40iOZe7nvRNo5FVzVnSJ4FzbD9Wz8BqqMj7fAFwgaQNkjokTa9bdLVRJOfbgPmSOoHHge/UJ7R+092/925pxtbZlb7xd73Eqsg2jaRwPpLmA5OBy2oaUe2dNGdJ7wN+CSyoV0B1UOR9PoU0hPR50tHgs5LG2/5PjWOrlSI5Xw2ssH2XpM+QZnMcb/ud2ofXL2r6+dWMRwqdwDll99s4/nCytI2kU0iHnCc7XBvoiuSMpMuBxcAM2/+tU2y1Ui3n4cB44GlJe0ljr+0NfrK56O/2n2y/ZXsP8DKpSDSqIjlfD/wBwPZGYAipR1CzKvT33lPNWBQ2A2MljZE0mHQiub3LNu3Adfn2LOCvzmdwGlTVnPNQyn2kgtDo48xQJWfbh2yPsj3a9mjSeZQZtht5Ltciv9t/JF1UgKRRpOGk3XWNsm8VyflVYBqApI+TikIzT/HWDlybr0KaAhyy/VpfPXnTDR/ZflvSTcBa0pULy23vkPRTYIvtduB3pEPMXaQjhLn9F3HvFcz5TuA0YHU+p/6q7Rn9FnQvFcy5qRTMeS1whaSdwFHg+7YP9F/UvVMw5+8Bv5V0C2kYZUEjf8mT9DBp+G9UPk/yY2AQgO2lpPMmVwK7gDeAr/fp/hv4tQshhNDHmnH4KIQQQg9FUQghhFASRSGEEEJJFIUQQgglURRCCCGURFEIA46ko5JeKPsZfZJtR5+om2Q39/l07sS5NbeIuLAHz3GjpGvz7QWSzi5bd7+kcX0c52ZJEws85mZJw3q779AaoiiEgeiI7YllP3vrtN95ti8mNUu8s7sPtr3U9gP57gLg7LJ1N9je2SdRvhvnvRSL82YgikIoJIpCaAj5iOBZSX/LP5+tsM1Fkp7LRxfbJI3Ny+eXLb9P0vur7G49cH5+7LTcp3977nN/al5+u96dn+IXedltkhZJmkXqL/Vg3ufQ/A1/sqSFku4oi3mBpF/3MM6NlDVCk/QbSVuU5lH4SV72XVJxWidpXV52haSN+XVcLem0KvsJLSSKQhiIhpYNHT2al+0Dvmh7EjAHWFLhcTcCv7I9kfSh3JnbHswBLs3LjwLzquz/K8B2SUOAFcAc258gdQBYKOkM4GvARbYnAD8vf7DtNcAW0jf6ibaPlK1eA1xVdn8OsKqHcU4ntbU4ZrHtycAE4DJJE2wvIfXFmWp7am598SPg8vxabgFurbKf0EKars1FaApH8gdjuUHA3XkM/Sipp09XG4HFktqAR2y/Imka8Clgc27vMZRUYCp5UNIRYC+p/fKFwB7bf8/rVwLfBu4mzc9wv6Q/A4Vbc9veL2l37lnzSt7Hhvy83YnzA6S2D+Wzbs2W9E3S3/VZpAlntnV57JS8fEPez2DS6xYCEEUhNI5bgH8DF5OOcI+bNMf2Q5I2AV8G1kq6gdRmeKXtHxbYx7zyhnmSKs6xkfvxXEJqwjYXuAn4QjdyWQXMBl4CHrVtpU/ownGSZiC7HbgHuErSGGAR8GnbByWtIDWG60rAk7av7ka8oYXE8FFoFCOA13KP/GtI35LfQ9J5wO48ZNJOGkZ5Cpgl6cy8zRkqPj/1S8BoSefn+9cAz+Qx+BG2HyedxK10BdBhUvvuSh4BvkqaB2BVXtatOG2/RRoGmpKHnk4HXgcOSfow8KUTxNIBXHosJ0nDJFU66gotKopCaBT3AtdJ6iANHb1eYZs5wIuSXgA+RpqycCfpw/MvkrYBT5KGVqqy/SapA+VqSduBd4ClpA/Yx/LzPUM6iulqBbD02InmLs97ENgJnGv7ubys23HmcxV3AYtsbyXNzbwDWE4akjpmGfCEpHW295OujHo476eD9FqFAESX1BBCCGXiSCGEEEJJFIUQQgglURRCCCGURFEIIYRQEkUhhBBCSRSFEEIIJVEUQgghlERRCCGEUPJ/sTIJkKRRLYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr, tpr, color=\"green\", label='VGGNet-16', marker='o', linestyle='--')\n",
    "plt.plot(fpr_res, tpr_res, color=\"blue\", label='ResNet-50', marker='x', linestyle=':')\n",
    "plt.plot(fpr_dc, tpr_dc, color=\"orange\", label='Proposed', marker='+', linestyle='-.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "fig = plt.gcf()\n",
    "plt.legend()\n",
    "fig.savefig(\"D:/newFolder/2019/graph/compare_network_abnormal.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr_, tpr_, color=\"green\", label='VGGNet-16', marker='o', linestyle='--')\n",
    "plt.plot(fpr_res_, tpr_res_, color=\"blue\", label='ResNet-50', marker='x', linestyle=':')\n",
    "plt.plot(fpr_dc_, tpr_dc_, color=\"orange\", label='Proposed', marker='+', linestyle='-.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "fig = plt.gcf()\n",
    "plt.legend()\n",
    "fig.savefig(\"D:/newFolder/2019/graph/compare_network_normal.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
