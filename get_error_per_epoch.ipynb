{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras, os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9182 images belonging to 3 classes.\n",
      "Found 1018 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory='D:/Backup_data/jongkeun/dir_images/traindata/', target_size=(128,128))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory='D:/Backup_data/jongkeun/dir_images/testdata/', target_size=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 15:44:10.802671 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 15:44:11.547652 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0106 15:44:11.549646 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0106 15:44:11.571613 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0106 15:44:11.574580 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0106 15:44:11.580565 11136 deprecation.py:506] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0106 15:44:11.580565 11136 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0106 15:44:11.614499 11136 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0106 15:44:11.654392 11136 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0106 15:44:11.694285 11136 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0106 15:44:11.777064 11136 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(input_shape=(128,128,3),filters=8,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(rate=0.7))\n",
    "\n",
    "model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(rate=0.7))\n",
    "\n",
    "model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(rate=0.7))\n",
    "\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(rate=0.7))\n",
    "\n",
    "model.add(Conv2D(filters=1024,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(rate=0.7))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(units=512,activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 15:44:13.742804 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0106 15:44:13.748790 11136 deprecation_wrapper.py:119] From C:\\Users\\whdrm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=1e-5)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 8)       224       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 128)     9344      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 64)          294976    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 18,694,691\n",
      "Trainable params: 18,694,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.6266 - acc: 0.7079 - val_loss: 0.6824 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76115, saving model to 200106_total16\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 0.6074 - acc: 0.7200 - val_loss: 0.7173 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76115\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 0.6211 - acc: 0.7137 - val_loss: 0.7235 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76115\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 75s 752ms/step - loss: 0.6209 - acc: 0.7133 - val_loss: 0.7366 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76115\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.6317 - acc: 0.6988 - val_loss: 0.7348 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76115\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 0.5978 - acc: 0.7262 - val_loss: 0.7236 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76115\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 67s 670ms/step - loss: 0.6141 - acc: 0.7192 - val_loss: 0.6258 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76115\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.6108 - acc: 0.7153 - val_loss: 0.6868 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76115\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.6202 - acc: 0.7191 - val_loss: 0.6705 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76115\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 64s 639ms/step - loss: 0.6240 - acc: 0.7067 - val_loss: 0.6995 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76115\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.6110 - acc: 0.7172 - val_loss: 0.6549 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76115\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 0.6070 - acc: 0.7259 - val_loss: 0.6517 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76115\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 62s 617ms/step - loss: 0.6117 - acc: 0.7264 - val_loss: 0.6491 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76115\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.6212 - acc: 0.7006 - val_loss: 0.6191 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76115\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.6038 - acc: 0.7306 - val_loss: 0.6900 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76115\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.6265 - acc: 0.7120 - val_loss: 0.6242 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.76115 to 0.76562, saving model to 200106_total16\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 51s 513ms/step - loss: 0.5998 - acc: 0.7281 - val_loss: 0.6579 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76562\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 65s 648ms/step - loss: 0.5965 - acc: 0.7266 - val_loss: 0.6232 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76562\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 0.6037 - acc: 0.7299 - val_loss: 0.6312 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76562\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 49s 493ms/step - loss: 0.6072 - acc: 0.7175 - val_loss: 0.6954 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76562\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 0.5991 - acc: 0.7309 - val_loss: 0.6579 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76562\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 0.6109 - acc: 0.7270 - val_loss: 0.6316 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76562\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.5761 - acc: 0.7500 - val_loss: 0.5962 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76562\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 69s 693ms/step - loss: 0.6248 - acc: 0.7187 - val_loss: 0.6430 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76562\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.6001 - acc: 0.7272 - val_loss: 0.6535 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76562\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 50s 501ms/step - loss: 0.6047 - acc: 0.7319 - val_loss: 0.5991 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76562\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 66s 661ms/step - loss: 0.6069 - acc: 0.7261 - val_loss: 0.5918 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.76562 to 0.76752, saving model to 200106_total16\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.6071 - acc: 0.7256 - val_loss: 0.6068 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76752\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 0.5765 - acc: 0.7412 - val_loss: 0.5980 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76752\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 68s 679ms/step - loss: 0.6078 - acc: 0.7330 - val_loss: 0.6433 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76752\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5971 - acc: 0.7384 - val_loss: 0.6601 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76752\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 51s 507ms/step - loss: 0.5978 - acc: 0.7306 - val_loss: 0.5962 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76752\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 63s 635ms/step - loss: 0.5953 - acc: 0.7414 - val_loss: 0.5811 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.76752 to 0.77389, saving model to 200106_total16\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 50s 504ms/step - loss: 0.6069 - acc: 0.7331 - val_loss: 0.5917 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.77389\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 62s 616ms/step - loss: 0.5701 - acc: 0.7484 - val_loss: 0.5770 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.77389\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 64s 641ms/step - loss: 0.6045 - acc: 0.7423 - val_loss: 0.5867 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.77389\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 49s 491ms/step - loss: 0.5974 - acc: 0.7425 - val_loss: 0.5814 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.77389\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.5661 - acc: 0.7534 - val_loss: 0.5751 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.77389\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.5893 - acc: 0.7420 - val_loss: 0.5904 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.77389\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5937 - acc: 0.7447 - val_loss: 0.5959 - val_acc: 0.7452\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.77389\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 72s 720ms/step - loss: 0.5730 - acc: 0.7578 - val_loss: 0.5738 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.77389\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 0.5789 - acc: 0.7558 - val_loss: 0.5638 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.77389\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 50s 503ms/step - loss: 0.5836 - acc: 0.7547 - val_loss: 0.5414 - val_acc: 0.7548\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.77389\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 66s 656ms/step - loss: 0.5833 - acc: 0.7472 - val_loss: 0.5397 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.77389\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5760 - acc: 0.7524 - val_loss: 0.5662 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.77389\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.5892 - acc: 0.7447 - val_loss: 0.5530 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.77389\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 67s 674ms/step - loss: 0.5642 - acc: 0.7567 - val_loss: 0.5727 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.77389\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5818 - acc: 0.7553 - val_loss: 0.5262 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.77389 to 0.79688, saving model to 200106_total16\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.5836 - acc: 0.7425 - val_loss: 0.5916 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.79688\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 70s 698ms/step - loss: 0.5746 - acc: 0.7539 - val_loss: 0.5810 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.79688\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5864 - acc: 0.7500 - val_loss: 0.5433 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.79688\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 56s 563ms/step - loss: 0.5611 - acc: 0.7669 - val_loss: 0.5642 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.79688\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 63s 628ms/step - loss: 0.5709 - acc: 0.7545 - val_loss: 0.5775 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.79688\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5735 - acc: 0.7547 - val_loss: 0.5664 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.79688\n",
      "Epoch 55/500\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 0.5618 - acc: 0.7638 - val_loss: 0.5351 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.79688\n",
      "Epoch 56/500\n",
      "100/100 [==============================] - 63s 631ms/step - loss: 0.5534 - acc: 0.7736 - val_loss: 0.5093 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.79688\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5696 - acc: 0.7581 - val_loss: 0.5292 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.79688\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 0.5573 - acc: 0.7694 - val_loss: 0.5352 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.79688\n",
      "Epoch 59/500\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.5567 - acc: 0.7742 - val_loss: 0.5597 - val_acc: 0.7261\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.79688\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5643 - acc: 0.7581 - val_loss: 0.5677 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.79688\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 57s 567ms/step - loss: 0.5669 - acc: 0.7647 - val_loss: 0.5171 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.79688 to 0.80937, saving model to 200106_total16\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.5493 - acc: 0.7739 - val_loss: 0.5645 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.80937\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 50s 495ms/step - loss: 0.5552 - acc: 0.7672 - val_loss: 0.5855 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.80937\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 68s 676ms/step - loss: 0.5756 - acc: 0.7603 - val_loss: 0.5256 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.80937\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 54s 536ms/step - loss: 0.5416 - acc: 0.7849 - val_loss: 0.5825 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.80937\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5636 - acc: 0.7716 - val_loss: 0.5751 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.80937\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 63s 626ms/step - loss: 0.5480 - acc: 0.7736 - val_loss: 0.5703 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.80937\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 49s 492ms/step - loss: 0.5440 - acc: 0.7741 - val_loss: 0.5781 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.80937\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.5543 - acc: 0.7753 - val_loss: 0.6012 - val_acc: 0.6879\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.80937\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 67s 671ms/step - loss: 0.5442 - acc: 0.7798 - val_loss: 0.5729 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.80937\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5403 - acc: 0.7759 - val_loss: 0.5332 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.80937\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 50s 495ms/step - loss: 0.5587 - acc: 0.7756 - val_loss: 0.5446 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.80937\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 65s 652ms/step - loss: 0.5426 - acc: 0.7748 - val_loss: 0.6922 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.80937\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5523 - acc: 0.7794 - val_loss: 0.6824 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.80937\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 53s 533ms/step - loss: 0.5395 - acc: 0.7781 - val_loss: 0.6595 - val_acc: 0.6274\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.80937\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 64s 640ms/step - loss: 0.5483 - acc: 0.7717 - val_loss: 0.5430 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.80937\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5326 - acc: 0.7894 - val_loss: 0.6401 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.80937\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 50s 496ms/step - loss: 0.5462 - acc: 0.7803 - val_loss: 0.7243 - val_acc: 0.5828\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.80937\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 62s 623ms/step - loss: 0.5619 - acc: 0.7707 - val_loss: 0.5556 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.80937\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5281 - acc: 0.7847 - val_loss: 0.6842 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.80937\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 57s 572ms/step - loss: 0.5440 - acc: 0.7809 - val_loss: 0.5650 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.80937\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 0.5396 - acc: 0.7793 - val_loss: 0.6422 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.80937\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5443 - acc: 0.7844 - val_loss: 0.6088 - val_acc: 0.6750\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.80937\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 59s 592ms/step - loss: 0.5341 - acc: 0.7834 - val_loss: 0.6696 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.80937\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 56s 557ms/step - loss: 0.5396 - acc: 0.7842 - val_loss: 0.7224 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.80937\n",
      "Epoch 86/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5264 - acc: 0.7897 - val_loss: 0.7390 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.80937\n",
      "Epoch 87/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 65s 654ms/step - loss: 0.5324 - acc: 0.7781 - val_loss: 0.5717 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.80937\n",
      "Epoch 88/500\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 0.5301 - acc: 0.7846 - val_loss: 0.6868 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.80937\n",
      "Epoch 89/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5332 - acc: 0.7825 - val_loss: 0.6837 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.80937\n",
      "Epoch 90/500\n",
      "100/100 [==============================] - 65s 654ms/step - loss: 0.5358 - acc: 0.7826 - val_loss: 0.6538 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.80937\n",
      "Epoch 91/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5520 - acc: 0.7694 - val_loss: 0.6379 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.80937\n",
      "Epoch 92/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.5030 - acc: 0.8047 - val_loss: 0.6853 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.80937\n",
      "Epoch 93/500\n",
      "100/100 [==============================] - 64s 640ms/step - loss: 0.5304 - acc: 0.7892 - val_loss: 0.7752 - val_acc: 0.5656\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.80937\n",
      "Epoch 94/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5198 - acc: 0.7903 - val_loss: 0.8202 - val_acc: 0.5924\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.80937\n",
      "Epoch 95/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5278 - acc: 0.7847 - val_loss: 0.7967 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.80937\n",
      "Epoch 96/500\n",
      "100/100 [==============================] - 62s 622ms/step - loss: 0.5149 - acc: 0.7902 - val_loss: 0.6351 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.80937\n",
      "Epoch 97/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5248 - acc: 0.7903 - val_loss: 0.5990 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.80937\n",
      "Epoch 98/500\n",
      "100/100 [==============================] - 51s 509ms/step - loss: 0.5297 - acc: 0.7925 - val_loss: 0.6415 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.80937\n",
      "Epoch 99/500\n",
      "100/100 [==============================] - 63s 627ms/step - loss: 0.5030 - acc: 0.8083 - val_loss: 0.8484 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.80937\n",
      "Epoch 100/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5310 - acc: 0.7869 - val_loss: 0.6976 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.80937\n",
      "Epoch 101/500\n",
      "100/100 [==============================] - 52s 517ms/step - loss: 0.5069 - acc: 0.8034 - val_loss: 0.9131 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.80937\n",
      "Epoch 102/500\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 0.5354 - acc: 0.7864 - val_loss: 0.6381 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.80937\n",
      "Epoch 103/500\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.5104 - acc: 0.7909 - val_loss: 0.7581 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.80937\n",
      "Epoch 104/500\n",
      "100/100 [==============================] - 57s 574ms/step - loss: 0.5203 - acc: 0.7916 - val_loss: 0.8095 - val_acc: 0.5701\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.80937\n",
      "Epoch 105/500\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 0.4962 - acc: 0.8046 - val_loss: 0.8869 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.80937\n",
      "Epoch 106/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4991 - acc: 0.8075 - val_loss: 0.8445 - val_acc: 0.5906\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.80937\n",
      "Epoch 107/500\n",
      "100/100 [==============================] - 61s 614ms/step - loss: 0.5259 - acc: 0.7956 - val_loss: 0.8120 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.80937\n",
      "Epoch 108/500\n",
      "100/100 [==============================] - 55s 551ms/step - loss: 0.5163 - acc: 0.7964 - val_loss: 0.7918 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.80937\n",
      "Epoch 109/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5262 - acc: 0.7856 - val_loss: 0.7662 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.80937\n",
      "Epoch 110/500\n",
      "100/100 [==============================] - 65s 648ms/step - loss: 0.5069 - acc: 0.7934 - val_loss: 0.7222 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.80937\n",
      "Epoch 111/500\n",
      "100/100 [==============================] - 49s 492ms/step - loss: 0.5182 - acc: 0.7952 - val_loss: 0.8845 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.80937\n",
      "Epoch 112/500\n",
      "100/100 [==============================] - 49s 491ms/step - loss: 0.5030 - acc: 0.8028 - val_loss: 0.8576 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.80937\n",
      "Epoch 113/500\n",
      "100/100 [==============================] - 67s 672ms/step - loss: 0.4982 - acc: 0.8030 - val_loss: 0.7826 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.80937\n",
      "Epoch 114/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.5225 - acc: 0.8012 - val_loss: 0.8300 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.80937\n",
      "Epoch 115/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5023 - acc: 0.8037 - val_loss: 0.8464 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.80937\n",
      "Epoch 116/500\n",
      "100/100 [==============================] - 66s 659ms/step - loss: 0.4953 - acc: 0.8093 - val_loss: 0.8425 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.80937\n",
      "Epoch 117/500\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.5242 - acc: 0.7897 - val_loss: 0.7853 - val_acc: 0.5669\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.80937\n",
      "Epoch 118/500\n",
      "100/100 [==============================] - 50s 505ms/step - loss: 0.4925 - acc: 0.8153 - val_loss: 0.7504 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.80937\n",
      "Epoch 119/500\n",
      "100/100 [==============================] - 68s 675ms/step - loss: 0.4998 - acc: 0.8090 - val_loss: 0.7727 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.80937\n",
      "Epoch 120/500\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.5020 - acc: 0.8084 - val_loss: 0.9076 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.80937\n",
      "Epoch 121/500\n",
      "100/100 [==============================] - 51s 513ms/step - loss: 0.4906 - acc: 0.8047 - val_loss: 0.8703 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.80937\n",
      "Epoch 122/500\n",
      "100/100 [==============================] - 64s 639ms/step - loss: 0.4930 - acc: 0.8134 - val_loss: 0.6754 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.80937\n",
      "Epoch 123/500\n",
      "100/100 [==============================] - 51s 505ms/step - loss: 0.5050 - acc: 0.7991 - val_loss: 0.9139 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.80937\n",
      "Epoch 124/500\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 0.4910 - acc: 0.8059 - val_loss: 0.9127 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.80937\n",
      "Epoch 125/500\n",
      "100/100 [==============================] - 61s 612ms/step - loss: 0.5188 - acc: 0.7984 - val_loss: 0.8787 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.80937\n",
      "Epoch 126/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4855 - acc: 0.8122 - val_loss: 0.9522 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.80937\n",
      "Epoch 127/500\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 0.4772 - acc: 0.8162 - val_loss: 0.9814 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.80937\n",
      "Epoch 128/500\n",
      "100/100 [==============================] - 57s 566ms/step - loss: 0.5060 - acc: 0.8021 - val_loss: 0.7782 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.80937\n",
      "Epoch 129/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4841 - acc: 0.8081 - val_loss: 0.9447 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.80937\n",
      "Epoch 130/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 71s 711ms/step - loss: 0.4931 - acc: 0.8081 - val_loss: 0.8010 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.80937\n",
      "Epoch 131/500\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 0.4965 - acc: 0.8020 - val_loss: 0.9048 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.80937\n",
      "Epoch 132/500\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 0.4736 - acc: 0.8122 - val_loss: 0.7015 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.80937\n",
      "Epoch 133/500\n",
      "100/100 [==============================] - 72s 723ms/step - loss: 0.4997 - acc: 0.8102 - val_loss: 1.1261 - val_acc: 0.5669\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.80937\n",
      "Epoch 134/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.5045 - acc: 0.8044 - val_loss: 0.8267 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.80937\n",
      "Epoch 135/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4746 - acc: 0.8228 - val_loss: 0.8209 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.80937\n",
      "Epoch 136/500\n",
      "100/100 [==============================] - 63s 633ms/step - loss: 0.4806 - acc: 0.8183 - val_loss: 0.9326 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.80937\n",
      "Epoch 137/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4802 - acc: 0.8113 - val_loss: 0.8714 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.80937\n",
      "Epoch 138/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4961 - acc: 0.8047 - val_loss: 1.1126 - val_acc: 0.5594\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.80937\n",
      "Epoch 139/500\n",
      "100/100 [==============================] - 63s 631ms/step - loss: 0.4828 - acc: 0.8152 - val_loss: 1.0708 - val_acc: 0.5637\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.80937\n",
      "Epoch 140/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4913 - acc: 0.8066 - val_loss: 0.7949 - val_acc: 0.5656\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.80937\n",
      "Epoch 141/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4709 - acc: 0.8166 - val_loss: 0.9419 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.80937\n",
      "Epoch 142/500\n",
      "100/100 [==============================] - 62s 625ms/step - loss: 0.4774 - acc: 0.8168 - val_loss: 0.7947 - val_acc: 0.6178\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.80937\n",
      "Epoch 143/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4889 - acc: 0.8097 - val_loss: 0.8574 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.80937\n",
      "Epoch 144/500\n",
      "100/100 [==============================] - 51s 511ms/step - loss: 0.4809 - acc: 0.8125 - val_loss: 1.0008 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.80937\n",
      "Epoch 145/500\n",
      "100/100 [==============================] - 64s 636ms/step - loss: 0.4734 - acc: 0.8214 - val_loss: 1.1459 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.80937\n",
      "Epoch 146/500\n",
      "100/100 [==============================] - 50s 500ms/step - loss: 0.4512 - acc: 0.8284 - val_loss: 0.9583 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.80937\n",
      "Epoch 147/500\n",
      "100/100 [==============================] - 57s 574ms/step - loss: 0.4866 - acc: 0.8106 - val_loss: 0.8695 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.80937\n",
      "Epoch 148/500\n",
      "100/100 [==============================] - 61s 610ms/step - loss: 0.4555 - acc: 0.8246 - val_loss: 1.0297 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.80937\n",
      "Epoch 149/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4904 - acc: 0.8122 - val_loss: 0.8364 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.80937\n",
      "Epoch 150/500\n",
      "100/100 [==============================] - 53s 530ms/step - loss: 0.4631 - acc: 0.8203 - val_loss: 0.9142 - val_acc: 0.5531\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.80937\n",
      "Epoch 151/500\n",
      "100/100 [==============================] - 57s 572ms/step - loss: 0.4766 - acc: 0.8111 - val_loss: 0.7908 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.80937\n",
      "Epoch 152/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4465 - acc: 0.8319 - val_loss: 0.6638 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.80937\n",
      "Epoch 153/500\n",
      "100/100 [==============================] - 56s 556ms/step - loss: 0.4813 - acc: 0.8128 - val_loss: 1.0652 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.80937\n",
      "Epoch 154/500\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 0.4798 - acc: 0.8140 - val_loss: 0.8624 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.80937\n",
      "Epoch 155/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4636 - acc: 0.8253 - val_loss: 1.0663 - val_acc: 0.5637\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.80937\n",
      "Epoch 156/500\n",
      "100/100 [==============================] - 58s 580ms/step - loss: 0.4706 - acc: 0.8199 - val_loss: 0.8175 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.80937\n",
      "Epoch 157/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4643 - acc: 0.8216 - val_loss: 1.0542 - val_acc: 0.5250\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.80937\n",
      "Epoch 158/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4446 - acc: 0.8303 - val_loss: 0.8478 - val_acc: 0.6083\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.80937\n",
      "Epoch 159/500\n",
      "100/100 [==============================] - 57s 570ms/step - loss: 0.4714 - acc: 0.8180 - val_loss: 1.1187 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.80937\n",
      "Epoch 160/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4530 - acc: 0.8287 - val_loss: 0.8186 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.80937\n",
      "Epoch 161/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4826 - acc: 0.8116 - val_loss: 0.9069 - val_acc: 0.5637\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.80937\n",
      "Epoch 162/500\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 0.4663 - acc: 0.8203 - val_loss: 1.1411 - val_acc: 0.5656\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.80937\n",
      "Epoch 163/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4632 - acc: 0.8228 - val_loss: 0.9201 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.80937\n",
      "Epoch 164/500\n",
      "100/100 [==============================] - 50s 499ms/step - loss: 0.4561 - acc: 0.8303 - val_loss: 0.9031 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.80937\n",
      "Epoch 165/500\n",
      "100/100 [==============================] - 65s 648ms/step - loss: 0.4483 - acc: 0.8287 - val_loss: 0.9142 - val_acc: 0.5541\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.80937\n",
      "Epoch 166/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4734 - acc: 0.8184 - val_loss: 0.7893 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.80937\n",
      "Epoch 167/500\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.4565 - acc: 0.8303 - val_loss: 0.9297 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.80937\n",
      "Epoch 168/500\n",
      "100/100 [==============================] - 59s 594ms/step - loss: 0.4509 - acc: 0.8252 - val_loss: 0.8296 - val_acc: 0.5541\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.80937\n",
      "Epoch 169/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4406 - acc: 0.8334 - val_loss: 0.8110 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.80937\n",
      "Epoch 170/500\n",
      "100/100 [==============================] - 52s 521ms/step - loss: 0.4700 - acc: 0.8234 - val_loss: 0.9828 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.80937\n",
      "Epoch 171/500\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 0.4420 - acc: 0.8290 - val_loss: 0.8989 - val_acc: 0.5892\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.80937\n",
      "Epoch 172/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4495 - acc: 0.8344 - val_loss: 0.7852 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.80937\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 56s 556ms/step - loss: 0.4632 - acc: 0.8172 - val_loss: 0.9609 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.80937\n",
      "Epoch 174/500\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 0.4555 - acc: 0.8330 - val_loss: 1.1122 - val_acc: 0.5414\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.80937\n",
      "Epoch 175/500\n",
      "100/100 [==============================] - 49s 495ms/step - loss: 0.4630 - acc: 0.8187 - val_loss: 1.0087 - val_acc: 0.5375\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.80937\n",
      "Epoch 176/500\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 0.4415 - acc: 0.8334 - val_loss: 0.7386 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.80937\n",
      "Epoch 177/500\n",
      "100/100 [==============================] - 50s 501ms/step - loss: 0.4621 - acc: 0.8258 - val_loss: 0.5645 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.80937\n",
      "Epoch 178/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4469 - acc: 0.8306 - val_loss: 0.7417 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.80937\n",
      "Epoch 179/500\n",
      "100/100 [==============================] - 55s 548ms/step - loss: 0.4612 - acc: 0.8265 - val_loss: 0.7396 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.80937\n",
      "Epoch 180/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4437 - acc: 0.8300 - val_loss: 0.8415 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.80937\n",
      "Epoch 181/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4464 - acc: 0.8266 - val_loss: 0.8180 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.80937\n",
      "Epoch 182/500\n",
      "100/100 [==============================] - 55s 546ms/step - loss: 0.4450 - acc: 0.8361 - val_loss: 0.8663 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.80937\n",
      "Epoch 183/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4562 - acc: 0.8231 - val_loss: 0.8986 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.80937\n",
      "Epoch 184/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4475 - acc: 0.8344 - val_loss: 0.6045 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.80937\n",
      "Epoch 185/500\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 0.4416 - acc: 0.8295 - val_loss: 0.6658 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.80937\n",
      "Epoch 186/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4560 - acc: 0.8300 - val_loss: 0.8096 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.80937\n",
      "Epoch 187/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4619 - acc: 0.8203 - val_loss: 0.7899 - val_acc: 0.5924\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.80937\n",
      "Epoch 188/500\n",
      "100/100 [==============================] - 53s 530ms/step - loss: 0.4282 - acc: 0.8374 - val_loss: 0.8492 - val_acc: 0.5719\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.80937\n",
      "Epoch 189/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4458 - acc: 0.8316 - val_loss: 0.9771 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.80937\n",
      "Epoch 190/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4283 - acc: 0.8369 - val_loss: 0.7177 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.80937\n",
      "Epoch 191/500\n",
      "100/100 [==============================] - 52s 517ms/step - loss: 0.4537 - acc: 0.8286 - val_loss: 0.7869 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.80937\n",
      "Epoch 192/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4353 - acc: 0.8337 - val_loss: 0.7940 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.80937\n",
      "Epoch 193/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4309 - acc: 0.8363 - val_loss: 0.7812 - val_acc: 0.6497\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.80937\n",
      "Epoch 194/500\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 0.4493 - acc: 0.8330 - val_loss: 0.7168 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.80937\n",
      "Epoch 195/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4491 - acc: 0.8213 - val_loss: 0.8517 - val_acc: 0.5656\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.80937\n",
      "Epoch 196/500\n",
      "100/100 [==============================] - 51s 505ms/step - loss: 0.4269 - acc: 0.8381 - val_loss: 0.7125 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.80937\n",
      "Epoch 197/500\n",
      "100/100 [==============================] - 52s 521ms/step - loss: 0.4361 - acc: 0.8327 - val_loss: 0.8689 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.80937\n",
      "Epoch 198/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4413 - acc: 0.8353 - val_loss: 0.7959 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.80937\n",
      "Epoch 199/500\n",
      "100/100 [==============================] - 55s 554ms/step - loss: 0.4139 - acc: 0.8512 - val_loss: 0.9060 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.80937\n",
      "Epoch 200/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4304 - acc: 0.8365 - val_loss: 0.8615 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.80937\n",
      "Epoch 201/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4376 - acc: 0.8341 - val_loss: 0.7400 - val_acc: 0.6188\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.80937\n",
      "Epoch 202/500\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 0.4157 - acc: 0.8433 - val_loss: 0.6393 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.80937\n",
      "Epoch 203/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4432 - acc: 0.8309 - val_loss: 0.6904 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.80937\n",
      "Epoch 204/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4220 - acc: 0.8450 - val_loss: 0.8107 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.80937\n",
      "Epoch 205/500\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 0.4320 - acc: 0.8400 - val_loss: 0.7373 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.80937\n",
      "Epoch 206/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4354 - acc: 0.8344 - val_loss: 0.8038 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.80937\n",
      "Epoch 207/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4277 - acc: 0.8353 - val_loss: 0.8073 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.80937\n",
      "Epoch 208/500\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 0.4323 - acc: 0.8343 - val_loss: 0.8196 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.80937\n",
      "Epoch 209/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4358 - acc: 0.8347 - val_loss: 0.6102 - val_acc: 0.6529\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.80937\n",
      "Epoch 210/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4461 - acc: 0.8316 - val_loss: 0.7096 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.80937\n",
      "Epoch 211/500\n",
      "100/100 [==============================] - 51s 511ms/step - loss: 0.4178 - acc: 0.8449 - val_loss: 0.6635 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.80937\n",
      "Epoch 212/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4244 - acc: 0.8441 - val_loss: 0.9080 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.80937\n",
      "Epoch 213/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4303 - acc: 0.8363 - val_loss: 0.7356 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.80937\n",
      "Epoch 214/500\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 0.4241 - acc: 0.8408 - val_loss: 0.7249 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.80937\n",
      "Epoch 215/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4355 - acc: 0.8353 - val_loss: 0.7121 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.80937\n",
      "Epoch 216/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4340 - acc: 0.8400 - val_loss: 0.7455 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.80937\n",
      "Epoch 217/500\n",
      "100/100 [==============================] - 52s 523ms/step - loss: 0.4270 - acc: 0.8380 - val_loss: 0.6680 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.80937\n",
      "Epoch 218/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4231 - acc: 0.8450 - val_loss: 0.7445 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.80937\n",
      "Epoch 219/500\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 0.4121 - acc: 0.8428 - val_loss: 0.7819 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.80937\n",
      "Epoch 220/500\n",
      "100/100 [==============================] - 51s 513ms/step - loss: 0.4178 - acc: 0.8468 - val_loss: 0.7487 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.80937\n",
      "Epoch 221/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4275 - acc: 0.8438 - val_loss: 0.6571 - val_acc: 0.6750\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.80937\n",
      "Epoch 222/500\n",
      "100/100 [==============================] - 54s 542ms/step - loss: 0.4477 - acc: 0.8308 - val_loss: 0.7243 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.80937\n",
      "Epoch 223/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4098 - acc: 0.8497 - val_loss: 0.7563 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.80937\n",
      "Epoch 224/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4180 - acc: 0.8447 - val_loss: 0.6534 - val_acc: 0.6781\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.80937\n",
      "Epoch 225/500\n",
      "100/100 [==============================] - 55s 545ms/step - loss: 0.4273 - acc: 0.8418 - val_loss: 0.7845 - val_acc: 0.6242\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.80937\n",
      "Epoch 226/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4254 - acc: 0.8438 - val_loss: 0.7315 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.80937\n",
      "Epoch 227/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4143 - acc: 0.8397 - val_loss: 0.7542 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.80937\n",
      "Epoch 228/500\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 0.4184 - acc: 0.8424 - val_loss: 0.5370 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.80937\n",
      "Epoch 229/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4207 - acc: 0.8431 - val_loss: 0.6031 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.80937\n",
      "Epoch 230/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4167 - acc: 0.8462 - val_loss: 0.7666 - val_acc: 0.6188\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.80937\n",
      "Epoch 231/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.4022 - acc: 0.8493 - val_loss: 0.8551 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.80937\n",
      "Epoch 232/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4177 - acc: 0.8472 - val_loss: 0.8388 - val_acc: 0.6210\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.80937\n",
      "Epoch 233/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4189 - acc: 0.8419 - val_loss: 0.6850 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.80937\n",
      "Epoch 234/500\n",
      "100/100 [==============================] - 54s 535ms/step - loss: 0.4194 - acc: 0.8365 - val_loss: 0.8787 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.80937\n",
      "Epoch 235/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4221 - acc: 0.8384 - val_loss: 0.6689 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.80937\n",
      "Epoch 236/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4243 - acc: 0.8394 - val_loss: 0.6811 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.80937\n",
      "Epoch 237/500\n",
      "100/100 [==============================] - 53s 525ms/step - loss: 0.4073 - acc: 0.8512 - val_loss: 0.6517 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.80937\n",
      "Epoch 238/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4233 - acc: 0.8391 - val_loss: 0.6805 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.80937\n",
      "Epoch 239/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4075 - acc: 0.8497 - val_loss: 0.7829 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.80937\n",
      "Epoch 240/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.3928 - acc: 0.8573 - val_loss: 0.9578 - val_acc: 0.5813\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.80937\n",
      "Epoch 241/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4160 - acc: 0.8413 - val_loss: 0.8119 - val_acc: 0.6178\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.80937\n",
      "Epoch 242/500\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 0.4131 - acc: 0.8438 - val_loss: 0.6935 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.80937\n",
      "Epoch 243/500\n",
      "100/100 [==============================] - 50s 503ms/step - loss: 0.4130 - acc: 0.8414 - val_loss: 0.6527 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.80937\n",
      "Epoch 244/500\n",
      "100/100 [==============================] - 55s 555ms/step - loss: 0.4055 - acc: 0.8478 - val_loss: 0.6472 - val_acc: 0.6781\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.80937\n",
      "Epoch 245/500\n",
      "100/100 [==============================] - 61s 609ms/step - loss: 0.4079 - acc: 0.8481 - val_loss: 0.8860 - val_acc: 0.5669\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.80937\n",
      "Epoch 246/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4050 - acc: 0.8503 - val_loss: 0.7199 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.80937\n",
      "Epoch 247/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4080 - acc: 0.8497 - val_loss: 0.5923 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.80937\n",
      "Epoch 248/500\n",
      "100/100 [==============================] - 57s 566ms/step - loss: 0.3962 - acc: 0.8536 - val_loss: 0.7353 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.80937\n",
      "Epoch 249/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3979 - acc: 0.8481 - val_loss: 0.6762 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.80937\n",
      "Epoch 250/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4274 - acc: 0.8419 - val_loss: 0.6340 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.80937\n",
      "Epoch 251/500\n",
      "100/100 [==============================] - 57s 566ms/step - loss: 0.4096 - acc: 0.8461 - val_loss: 0.8818 - val_acc: 0.6019\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.80937\n",
      "Epoch 252/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3849 - acc: 0.8541 - val_loss: 0.7731 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.80937\n",
      "Epoch 253/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4102 - acc: 0.8528 - val_loss: 0.9795 - val_acc: 0.6031\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.80937\n",
      "Epoch 254/500\n",
      "100/100 [==============================] - 60s 605ms/step - loss: 0.4177 - acc: 0.8418 - val_loss: 0.4690 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.80937\n",
      "Epoch 255/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4023 - acc: 0.8434 - val_loss: 0.6016 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.80937\n",
      "Epoch 256/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4058 - acc: 0.8497 - val_loss: 0.5099 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.80937\n",
      "Epoch 257/500\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 0.3931 - acc: 0.8562 - val_loss: 0.7607 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.80937\n",
      "Epoch 258/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4004 - acc: 0.8484 - val_loss: 0.6607 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.80937\n",
      "Epoch 259/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4102 - acc: 0.8503 - val_loss: 0.4648 - val_acc: 0.7906\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.80937\n",
      "Epoch 260/500\n",
      "100/100 [==============================] - 55s 553ms/step - loss: 0.3963 - acc: 0.8511 - val_loss: 0.6271 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.80937\n",
      "Epoch 261/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3969 - acc: 0.8547 - val_loss: 0.6020 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.80937\n",
      "Epoch 262/500\n",
      "100/100 [==============================] - 49s 491ms/step - loss: 0.4050 - acc: 0.8516 - val_loss: 0.7747 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.80937\n",
      "Epoch 263/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.3802 - acc: 0.8568 - val_loss: 0.8412 - val_acc: 0.6031\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.80937\n",
      "Epoch 264/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4033 - acc: 0.8541 - val_loss: 0.7471 - val_acc: 0.6529\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.80937\n",
      "Epoch 265/500\n",
      "100/100 [==============================] - 54s 542ms/step - loss: 0.4147 - acc: 0.8413 - val_loss: 0.6546 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.80937\n",
      "Epoch 266/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3926 - acc: 0.8580 - val_loss: 0.6233 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.80937\n",
      "Epoch 267/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3978 - acc: 0.8600 - val_loss: 0.7687 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.80937\n",
      "Epoch 268/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.3954 - acc: 0.8502 - val_loss: 0.8382 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.80937\n",
      "Epoch 269/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.4086 - acc: 0.8522 - val_loss: 0.6900 - val_acc: 0.6719\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.80937\n",
      "Epoch 270/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3818 - acc: 0.8575 - val_loss: 0.5304 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.80937\n",
      "Epoch 271/500\n",
      "100/100 [==============================] - 54s 541ms/step - loss: 0.3889 - acc: 0.8580 - val_loss: 0.7860 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.80937\n",
      "Epoch 272/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3769 - acc: 0.8575 - val_loss: 0.7678 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.80937\n",
      "Epoch 273/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4230 - acc: 0.8391 - val_loss: 0.8340 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.80937\n",
      "Epoch 274/500\n",
      "100/100 [==============================] - 56s 555ms/step - loss: 0.3897 - acc: 0.8530 - val_loss: 0.5426 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.80937\n",
      "Epoch 275/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3975 - acc: 0.8569 - val_loss: 0.6652 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.80937\n",
      "Epoch 276/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4035 - acc: 0.8484 - val_loss: 0.8193 - val_acc: 0.6312\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.80937\n",
      "Epoch 277/500\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.3917 - acc: 0.8534 - val_loss: 0.6836 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.80937\n",
      "Epoch 278/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.4171 - acc: 0.8434 - val_loss: 0.6191 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.80937\n",
      "Epoch 279/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3551 - acc: 0.8725 - val_loss: 0.6524 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.80937\n",
      "Epoch 280/500\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 0.3958 - acc: 0.8565 - val_loss: 0.7316 - val_acc: 0.6592\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.80937\n",
      "Epoch 281/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3910 - acc: 0.8581 - val_loss: 0.8248 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.80937\n",
      "Epoch 282/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3960 - acc: 0.8509 - val_loss: 0.8737 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.80937\n",
      "Epoch 283/500\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 0.3830 - acc: 0.8608 - val_loss: 0.5959 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.80937\n",
      "Epoch 284/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3759 - acc: 0.8606 - val_loss: 0.6317 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.80937\n",
      "Epoch 285/500\n",
      "100/100 [==============================] - 49s 492ms/step - loss: 0.3973 - acc: 0.8538 - val_loss: 0.5892 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.80937\n",
      "Epoch 286/500\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.3902 - acc: 0.8577 - val_loss: 0.7128 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.80937\n",
      "Epoch 287/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3766 - acc: 0.8594 - val_loss: 0.6727 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.80937\n",
      "Epoch 288/500\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 0.3976 - acc: 0.8571 - val_loss: 0.7753 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.80937\n",
      "Epoch 289/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3882 - acc: 0.8522 - val_loss: 0.6502 - val_acc: 0.7166\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.80937\n",
      "Epoch 290/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.4012 - acc: 0.8519 - val_loss: 0.5443 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.80937\n",
      "Epoch 291/500\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.3888 - acc: 0.8543 - val_loss: 0.4945 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.80937\n",
      "Epoch 292/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3882 - acc: 0.8519 - val_loss: 0.7738 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.80937\n",
      "Epoch 293/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3782 - acc: 0.8616 - val_loss: 0.7747 - val_acc: 0.6146\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.80937\n",
      "Epoch 294/500\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 0.3902 - acc: 0.8565 - val_loss: 0.5730 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.80937\n",
      "Epoch 295/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3988 - acc: 0.8491 - val_loss: 0.5448 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.80937\n",
      "Epoch 296/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3781 - acc: 0.8588 - val_loss: 0.6663 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.80937\n",
      "Epoch 297/500\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 0.3849 - acc: 0.8546 - val_loss: 0.7820 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.80937\n",
      "Epoch 298/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3783 - acc: 0.8628 - val_loss: 0.6988 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.80937\n",
      "Epoch 299/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3770 - acc: 0.8600 - val_loss: 0.8704 - val_acc: 0.5955\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.80937\n",
      "Epoch 300/500\n",
      "100/100 [==============================] - 55s 551ms/step - loss: 0.3714 - acc: 0.8693 - val_loss: 0.7352 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.80937\n",
      "Epoch 301/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3928 - acc: 0.8584 - val_loss: 0.8496 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.80937\n",
      "Epoch 302/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 493ms/step - loss: 0.3743 - acc: 0.8609 - val_loss: 0.7690 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.80937\n",
      "Epoch 303/500\n",
      "100/100 [==============================] - 56s 558ms/step - loss: 0.3758 - acc: 0.8612 - val_loss: 0.7486 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.80937\n",
      "Epoch 304/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3737 - acc: 0.8681 - val_loss: 0.6719 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.80937\n",
      "Epoch 305/500\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 0.3881 - acc: 0.8578 - val_loss: 0.4934 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.80937\n",
      "Epoch 306/500\n",
      "100/100 [==============================] - 54s 540ms/step - loss: 0.3630 - acc: 0.8640 - val_loss: 0.8227 - val_acc: 0.6219\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.80937\n",
      "Epoch 307/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3741 - acc: 0.8628 - val_loss: 0.5649 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.80937\n",
      "Epoch 308/500\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 0.3991 - acc: 0.8519 - val_loss: 0.6601 - val_acc: 0.6781\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.80937\n",
      "Epoch 309/500\n",
      "100/100 [==============================] - 50s 504ms/step - loss: 0.3951 - acc: 0.8555 - val_loss: 0.5564 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.80937\n",
      "Epoch 310/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3799 - acc: 0.8631 - val_loss: 0.5808 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.80937\n",
      "Epoch 311/500\n",
      "100/100 [==============================] - 58s 580ms/step - loss: 0.3830 - acc: 0.8634 - val_loss: 0.8767 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.80937\n",
      "Epoch 312/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3634 - acc: 0.8694 - val_loss: 0.5395 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.80937\n",
      "Epoch 313/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3812 - acc: 0.8637 - val_loss: 0.7012 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.80937\n",
      "Epoch 314/500\n",
      "100/100 [==============================] - 58s 580ms/step - loss: 0.3782 - acc: 0.8577 - val_loss: 0.6542 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.80937\n",
      "Epoch 315/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3709 - acc: 0.8616 - val_loss: 0.7081 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.80937\n",
      "Epoch 316/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3763 - acc: 0.8647 - val_loss: 0.7090 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.80937\n",
      "Epoch 317/500\n",
      "100/100 [==============================] - 57s 570ms/step - loss: 0.3799 - acc: 0.8590 - val_loss: 0.7070 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.80937\n",
      "Epoch 318/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3832 - acc: 0.8538 - val_loss: 0.7624 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.80937\n",
      "Epoch 319/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3798 - acc: 0.8591 - val_loss: 0.7841 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.80937\n",
      "Epoch 320/500\n",
      "100/100 [==============================] - 57s 572ms/step - loss: 0.3650 - acc: 0.8662 - val_loss: 0.6003 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.80937\n",
      "Epoch 321/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3694 - acc: 0.8713 - val_loss: 0.5921 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.80937\n",
      "Epoch 322/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3712 - acc: 0.8622 - val_loss: 0.7329 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.80937\n",
      "Epoch 323/500\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 0.3749 - acc: 0.8603 - val_loss: 0.6721 - val_acc: 0.6781\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.80937\n",
      "Epoch 324/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3721 - acc: 0.8647 - val_loss: 0.5937 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.80937\n",
      "Epoch 325/500\n",
      "100/100 [==============================] - 50s 503ms/step - loss: 0.3639 - acc: 0.8691 - val_loss: 0.6692 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.80937\n",
      "Epoch 326/500\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 0.3783 - acc: 0.8574 - val_loss: 0.7933 - val_acc: 0.6750\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.80937\n",
      "Epoch 327/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3635 - acc: 0.8616 - val_loss: 0.7611 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.80937\n",
      "Epoch 328/500\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 0.3701 - acc: 0.8612 - val_loss: 0.7243 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.80937\n",
      "Epoch 329/500\n",
      "100/100 [==============================] - 54s 539ms/step - loss: 0.3783 - acc: 0.8612 - val_loss: 0.8788 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.80937\n",
      "Epoch 330/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3823 - acc: 0.8575 - val_loss: 0.7124 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.80937\n",
      "Epoch 331/500\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 0.3521 - acc: 0.8719 - val_loss: 0.7809 - val_acc: 0.6369\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.80937\n",
      "Epoch 332/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3717 - acc: 0.8652 - val_loss: 0.7577 - val_acc: 0.6781\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.80937\n",
      "Epoch 333/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3711 - acc: 0.8622 - val_loss: 0.6591 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.80937\n",
      "Epoch 334/500\n",
      "100/100 [==============================] - 58s 579ms/step - loss: 0.3497 - acc: 0.8703 - val_loss: 0.6583 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.80937\n",
      "Epoch 335/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3826 - acc: 0.8591 - val_loss: 0.6738 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.80937\n",
      "Epoch 336/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3606 - acc: 0.8678 - val_loss: 0.7228 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.80937\n",
      "Epoch 337/500\n",
      "100/100 [==============================] - 59s 593ms/step - loss: 0.3549 - acc: 0.8677 - val_loss: 0.7112 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.80937\n",
      "Epoch 338/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3762 - acc: 0.8578 - val_loss: 0.5136 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.80937\n",
      "Epoch 339/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3471 - acc: 0.8675 - val_loss: 0.8219 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.80937\n",
      "Epoch 340/500\n",
      "100/100 [==============================] - 57s 572ms/step - loss: 0.3584 - acc: 0.8684 - val_loss: 0.6104 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.80937\n",
      "Epoch 341/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3560 - acc: 0.8672 - val_loss: 0.6723 - val_acc: 0.7166\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.80937\n",
      "Epoch 342/500\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.3731 - acc: 0.8647 - val_loss: 0.6051 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.80937\n",
      "Epoch 343/500\n",
      "100/100 [==============================] - 58s 576ms/step - loss: 0.3587 - acc: 0.8671 - val_loss: 0.6158 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.80937\n",
      "Epoch 344/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3570 - acc: 0.8703 - val_loss: 0.6975 - val_acc: 0.6720\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.80937\n",
      "Epoch 345/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3590 - acc: 0.8628 - val_loss: 0.7942 - val_acc: 0.6750\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.80937\n",
      "Epoch 346/500\n",
      "100/100 [==============================] - 56s 565ms/step - loss: 0.3718 - acc: 0.8643 - val_loss: 0.7841 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.80937\n",
      "Epoch 347/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3549 - acc: 0.8722 - val_loss: 0.5904 - val_acc: 0.7420\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.80937\n",
      "Epoch 348/500\n",
      "100/100 [==============================] - 49s 492ms/step - loss: 0.3714 - acc: 0.8628 - val_loss: 0.4200 - val_acc: 0.8313\n",
      "\n",
      "Epoch 00348: val_acc improved from 0.80937 to 0.83125, saving model to 200106_total16\n",
      "Epoch 349/500\n",
      "100/100 [==============================] - 60s 600ms/step - loss: 0.3677 - acc: 0.8686 - val_loss: 0.7437 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.83125\n",
      "Epoch 350/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3495 - acc: 0.8741 - val_loss: 0.7971 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.83125\n",
      "Epoch 351/500\n",
      "100/100 [==============================] - 62s 622ms/step - loss: 0.3494 - acc: 0.8678 - val_loss: 0.7226 - val_acc: 0.6687\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.83125\n",
      "Epoch 352/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.3543 - acc: 0.8664 - val_loss: 0.5375 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.83125\n",
      "Epoch 353/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3614 - acc: 0.8681 - val_loss: 0.6301 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.83125\n",
      "Epoch 354/500\n",
      "100/100 [==============================] - 56s 565ms/step - loss: 0.3701 - acc: 0.8621 - val_loss: 0.6441 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.83125\n",
      "Epoch 355/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3519 - acc: 0.8759 - val_loss: 0.8299 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.83125\n",
      "Epoch 356/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3798 - acc: 0.8581 - val_loss: 0.4462 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.83125\n",
      "Epoch 357/500\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 0.3371 - acc: 0.8743 - val_loss: 0.6346 - val_acc: 0.7229\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.83125\n",
      "Epoch 358/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3534 - acc: 0.8725 - val_loss: 0.9229 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.83125\n",
      "Epoch 359/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3650 - acc: 0.8672 - val_loss: 0.6351 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.83125\n",
      "Epoch 360/500\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 0.3619 - acc: 0.8668 - val_loss: 0.6726 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.83125\n",
      "Epoch 361/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3578 - acc: 0.8688 - val_loss: 0.7065 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.83125\n",
      "Epoch 362/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3658 - acc: 0.8697 - val_loss: 0.7940 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.83125\n",
      "Epoch 363/500\n",
      "100/100 [==============================] - 58s 580ms/step - loss: 0.3556 - acc: 0.8746 - val_loss: 0.8737 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.83125\n",
      "Epoch 364/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3729 - acc: 0.8634 - val_loss: 0.9967 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.83125\n",
      "Epoch 365/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3555 - acc: 0.8744 - val_loss: 0.5598 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.83125\n",
      "Epoch 366/500\n",
      "100/100 [==============================] - 58s 578ms/step - loss: 0.3436 - acc: 0.8740 - val_loss: 0.9441 - val_acc: 0.6178\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.83125\n",
      "Epoch 367/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3587 - acc: 0.8659 - val_loss: 0.6904 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.83125\n",
      "Epoch 368/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3487 - acc: 0.8766 - val_loss: 0.7966 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.83125\n",
      "Epoch 369/500\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 0.3550 - acc: 0.8684 - val_loss: 0.6609 - val_acc: 0.7038\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.83125\n",
      "Epoch 370/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3544 - acc: 0.8753 - val_loss: 0.5649 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.83125\n",
      "Epoch 371/500\n",
      "100/100 [==============================] - 50s 503ms/step - loss: 0.3509 - acc: 0.8697 - val_loss: 0.6617 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.83125\n",
      "Epoch 372/500\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 0.3347 - acc: 0.8793 - val_loss: 0.8083 - val_acc: 0.6500\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.83125\n",
      "Epoch 373/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3736 - acc: 0.8591 - val_loss: 0.7836 - val_acc: 0.6497\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.83125\n",
      "Epoch 374/500\n",
      "100/100 [==============================] - 53s 527ms/step - loss: 0.3347 - acc: 0.8806 - val_loss: 0.6734 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.83125\n",
      "Epoch 375/500\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.3581 - acc: 0.8725 - val_loss: 0.7370 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.83125\n",
      "Epoch 376/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3532 - acc: 0.8675 - val_loss: 0.6642 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.83125\n",
      "Epoch 377/500\n",
      "100/100 [==============================] - 57s 567ms/step - loss: 0.3336 - acc: 0.8749 - val_loss: 0.9289 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.83125\n",
      "Epoch 378/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3335 - acc: 0.8866 - val_loss: 0.6790 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.83125\n",
      "Epoch 379/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3725 - acc: 0.8625 - val_loss: 0.4895 - val_acc: 0.7834\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.83125\n",
      "Epoch 380/500\n",
      "100/100 [==============================] - 56s 557ms/step - loss: 0.3472 - acc: 0.8659 - val_loss: 0.8256 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.83125\n",
      "Epoch 381/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3432 - acc: 0.8778 - val_loss: 0.7433 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.83125\n",
      "Epoch 382/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3664 - acc: 0.8619 - val_loss: 0.6097 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.83125\n",
      "Epoch 383/500\n",
      "100/100 [==============================] - 58s 581ms/step - loss: 0.3663 - acc: 0.8665 - val_loss: 0.6568 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.83125\n",
      "Epoch 384/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3446 - acc: 0.8769 - val_loss: 0.5671 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.83125\n",
      "Epoch 385/500\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.3324 - acc: 0.8781 - val_loss: 0.8257 - val_acc: 0.6465\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.83125\n",
      "Epoch 386/500\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 0.3558 - acc: 0.8677 - val_loss: 0.5927 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.83125\n",
      "Epoch 387/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3529 - acc: 0.8759 - val_loss: 0.5913 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.83125\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 490ms/step - loss: 0.3382 - acc: 0.8769 - val_loss: 0.8132 - val_acc: 0.6438\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.83125\n",
      "Epoch 389/500\n",
      "100/100 [==============================] - 58s 578ms/step - loss: 0.3300 - acc: 0.8808 - val_loss: 0.5409 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.83125\n",
      "Epoch 390/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3441 - acc: 0.8703 - val_loss: 0.6499 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.83125\n",
      "Epoch 391/500\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 0.3444 - acc: 0.8703 - val_loss: 0.7937 - val_acc: 0.6344\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.83125\n",
      "Epoch 392/500\n",
      "100/100 [==============================] - 57s 566ms/step - loss: 0.3563 - acc: 0.8655 - val_loss: 0.6013 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.83125\n",
      "Epoch 393/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3668 - acc: 0.8634 - val_loss: 0.5776 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.83125\n",
      "Epoch 394/500\n",
      "100/100 [==============================] - 55s 548ms/step - loss: 0.3273 - acc: 0.8831 - val_loss: 0.8185 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.83125\n",
      "Epoch 395/500\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 0.3427 - acc: 0.8755 - val_loss: 0.7186 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.83125\n",
      "Epoch 396/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3456 - acc: 0.8762 - val_loss: 0.8491 - val_acc: 0.6156\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.83125\n",
      "Epoch 397/500\n",
      "100/100 [==============================] - 57s 565ms/step - loss: 0.3399 - acc: 0.8747 - val_loss: 0.7646 - val_acc: 0.6656\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.83125\n",
      "Epoch 398/500\n",
      "100/100 [==============================] - 49s 494ms/step - loss: 0.3378 - acc: 0.8724 - val_loss: 0.6529 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.83125\n",
      "Epoch 399/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3394 - acc: 0.8762 - val_loss: 0.6595 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.83125\n",
      "Epoch 400/500\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 0.3505 - acc: 0.8721 - val_loss: 0.6699 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.83125\n",
      "Epoch 401/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3343 - acc: 0.8778 - val_loss: 0.6033 - val_acc: 0.7389\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.83125\n",
      "Epoch 402/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3380 - acc: 0.8797 - val_loss: 0.4653 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.83125\n",
      "Epoch 403/500\n",
      "100/100 [==============================] - 59s 590ms/step - loss: 0.3681 - acc: 0.8631 - val_loss: 0.6686 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.83125\n",
      "Epoch 404/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3409 - acc: 0.8691 - val_loss: 0.5348 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.83125\n",
      "Epoch 405/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3317 - acc: 0.8803 - val_loss: 0.6834 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.83125\n",
      "Epoch 406/500\n",
      "100/100 [==============================] - 56s 560ms/step - loss: 0.3605 - acc: 0.8696 - val_loss: 0.6269 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.83125\n",
      "Epoch 407/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3220 - acc: 0.8822 - val_loss: 0.6364 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.83125\n",
      "Epoch 408/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3382 - acc: 0.8747 - val_loss: 0.6397 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.83125\n",
      "Epoch 409/500\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 0.3672 - acc: 0.8656 - val_loss: 0.5465 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.83125\n",
      "Epoch 410/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3408 - acc: 0.8775 - val_loss: 0.6067 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.83125\n",
      "Epoch 411/500\n",
      "100/100 [==============================] - 49s 494ms/step - loss: 0.3407 - acc: 0.8791 - val_loss: 0.4764 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.83125\n",
      "Epoch 412/500\n",
      "100/100 [==============================] - 57s 575ms/step - loss: 0.3419 - acc: 0.8734 - val_loss: 0.5168 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.83125\n",
      "Epoch 413/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3488 - acc: 0.8797 - val_loss: 0.5617 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.83125\n",
      "Epoch 414/500\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 0.3267 - acc: 0.8766 - val_loss: 0.7120 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.83125\n",
      "Epoch 415/500\n",
      "100/100 [==============================] - 56s 556ms/step - loss: 0.3136 - acc: 0.8877 - val_loss: 0.6745 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.83125\n",
      "Epoch 416/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3398 - acc: 0.8806 - val_loss: 0.7743 - val_acc: 0.6375\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.83125\n",
      "Epoch 417/500\n",
      "100/100 [==============================] - 55s 548ms/step - loss: 0.3492 - acc: 0.8684 - val_loss: 0.5207 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.83125\n",
      "Epoch 418/500\n",
      "100/100 [==============================] - 52s 523ms/step - loss: 0.3454 - acc: 0.8771 - val_loss: 0.5694 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.83125\n",
      "Epoch 419/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3354 - acc: 0.8766 - val_loss: 0.7370 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.83125\n",
      "Epoch 420/500\n",
      "100/100 [==============================] - 60s 595ms/step - loss: 0.3462 - acc: 0.8711 - val_loss: 0.4852 - val_acc: 0.8031\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.83125\n",
      "Epoch 421/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3431 - acc: 0.8719 - val_loss: 0.6677 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.83125\n",
      "Epoch 422/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3322 - acc: 0.8828 - val_loss: 0.4339 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.83125\n",
      "Epoch 423/500\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 0.3324 - acc: 0.8790 - val_loss: 0.6176 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.83125\n",
      "Epoch 424/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3444 - acc: 0.8781 - val_loss: 0.5848 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.83125\n",
      "Epoch 425/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3493 - acc: 0.8713 - val_loss: 0.7876 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.83125\n",
      "Epoch 426/500\n",
      "100/100 [==============================] - 57s 574ms/step - loss: 0.3401 - acc: 0.8737 - val_loss: 0.4554 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.83125\n",
      "Epoch 427/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3271 - acc: 0.8831 - val_loss: 0.6962 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.83125\n",
      "Epoch 428/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3355 - acc: 0.8759 - val_loss: 0.7096 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.83125\n",
      "Epoch 429/500\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 0.3334 - acc: 0.8771 - val_loss: 0.6752 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.83125\n",
      "Epoch 430/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3383 - acc: 0.8819 - val_loss: 0.4524 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.83125\n",
      "Epoch 431/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3130 - acc: 0.8838 - val_loss: 0.6242 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.83125\n",
      "Epoch 432/500\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 0.3500 - acc: 0.8721 - val_loss: 0.4173 - val_acc: 0.8438\n",
      "\n",
      "Epoch 00432: val_acc improved from 0.83125 to 0.84375, saving model to 200106_total16\n",
      "Epoch 433/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3087 - acc: 0.8903 - val_loss: 0.5919 - val_acc: 0.7452\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.84375\n",
      "Epoch 434/500\n",
      "100/100 [==============================] - 55s 549ms/step - loss: 0.3325 - acc: 0.8741 - val_loss: 0.5676 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.84375\n",
      "Epoch 435/500\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 0.3273 - acc: 0.8880 - val_loss: 0.5320 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.84375\n",
      "Epoch 436/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3379 - acc: 0.8738 - val_loss: 0.7634 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.84375\n",
      "Epoch 437/500\n",
      "100/100 [==============================] - 51s 513ms/step - loss: 0.3165 - acc: 0.8894 - val_loss: 0.5867 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.84375\n",
      "Epoch 438/500\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 0.3408 - acc: 0.8755 - val_loss: 0.3742 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00438: val_acc improved from 0.84375 to 0.85938, saving model to 200106_total16\n",
      "Epoch 439/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3154 - acc: 0.8847 - val_loss: 0.5447 - val_acc: 0.8031\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.85938\n",
      "Epoch 440/500\n",
      "100/100 [==============================] - 63s 631ms/step - loss: 0.3303 - acc: 0.8781 - val_loss: 0.8111 - val_acc: 0.6306\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.85938\n",
      "Epoch 441/500\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 0.3386 - acc: 0.8780 - val_loss: 0.6145 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.85938\n",
      "Epoch 442/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3294 - acc: 0.8759 - val_loss: 0.6268 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.85938\n",
      "Epoch 443/500\n",
      "100/100 [==============================] - 60s 602ms/step - loss: 0.3070 - acc: 0.8877 - val_loss: 0.6457 - val_acc: 0.7229\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.85938\n",
      "Epoch 444/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3161 - acc: 0.8866 - val_loss: 0.7448 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.85938\n",
      "Epoch 445/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3320 - acc: 0.8797 - val_loss: 0.5874 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.85938\n",
      "Epoch 446/500\n",
      "100/100 [==============================] - 58s 578ms/step - loss: 0.3523 - acc: 0.8696 - val_loss: 0.5269 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.85938\n",
      "Epoch 447/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3177 - acc: 0.8806 - val_loss: 0.6349 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.85938\n",
      "Epoch 448/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3299 - acc: 0.8784 - val_loss: 0.6916 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.85938\n",
      "Epoch 449/500\n",
      "100/100 [==============================] - 58s 577ms/step - loss: 0.3388 - acc: 0.8765 - val_loss: 0.4691 - val_acc: 0.8089\n",
      "\n",
      "Epoch 00449: val_acc did not improve from 0.85938\n",
      "Epoch 450/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3353 - acc: 0.8772 - val_loss: 0.7425 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.85938\n",
      "Epoch 451/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3193 - acc: 0.8831 - val_loss: 0.6871 - val_acc: 0.6750\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.85938\n",
      "Epoch 452/500\n",
      "100/100 [==============================] - 60s 604ms/step - loss: 0.3206 - acc: 0.8868 - val_loss: 0.7565 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.85938\n",
      "Epoch 453/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3097 - acc: 0.8903 - val_loss: 0.5125 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.85938\n",
      "Epoch 454/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3295 - acc: 0.8800 - val_loss: 0.6514 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00454: val_acc did not improve from 0.85938\n",
      "Epoch 455/500\n",
      "100/100 [==============================] - 57s 573ms/step - loss: 0.3512 - acc: 0.8686 - val_loss: 0.4642 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.85938\n",
      "Epoch 456/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.2999 - acc: 0.8956 - val_loss: 0.4566 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.85938\n",
      "Epoch 457/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3439 - acc: 0.8775 - val_loss: 0.5222 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.85938\n",
      "Epoch 458/500\n",
      "100/100 [==============================] - 57s 575ms/step - loss: 0.3212 - acc: 0.8825 - val_loss: 0.8416 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.85938\n",
      "Epoch 459/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3240 - acc: 0.8781 - val_loss: 0.3868 - val_acc: 0.8726\n",
      "\n",
      "Epoch 00459: val_acc improved from 0.85938 to 0.87261, saving model to 200106_total16\n",
      "Epoch 460/500\n",
      "100/100 [==============================] - 57s 574ms/step - loss: 0.3279 - acc: 0.8838 - val_loss: 0.4168 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.87261\n",
      "Epoch 461/500\n",
      "100/100 [==============================] - 56s 563ms/step - loss: 0.3375 - acc: 0.8774 - val_loss: 0.6990 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.87261\n",
      "Epoch 462/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3096 - acc: 0.8863 - val_loss: 0.6115 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.87261\n",
      "Epoch 463/500\n",
      "100/100 [==============================] - 59s 592ms/step - loss: 0.3295 - acc: 0.8791 - val_loss: 0.4312 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.87261\n",
      "Epoch 464/500\n",
      "100/100 [==============================] - 50s 501ms/step - loss: 0.3171 - acc: 0.8853 - val_loss: 0.5551 - val_acc: 0.7594\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.87261\n",
      "Epoch 465/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3458 - acc: 0.8747 - val_loss: 0.5817 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.87261\n",
      "Epoch 466/500\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 0.3088 - acc: 0.8911 - val_loss: 0.5581 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.87261\n",
      "Epoch 467/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3134 - acc: 0.8884 - val_loss: 0.5407 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.87261\n",
      "Epoch 468/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3377 - acc: 0.8769 - val_loss: 0.8110 - val_acc: 0.6469\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.87261\n",
      "Epoch 469/500\n",
      "100/100 [==============================] - 57s 575ms/step - loss: 0.3118 - acc: 0.8880 - val_loss: 0.6937 - val_acc: 0.7166\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.87261\n",
      "Epoch 470/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3063 - acc: 0.8919 - val_loss: 0.5971 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.87261\n",
      "Epoch 471/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3223 - acc: 0.8847 - val_loss: 0.7116 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.87261\n",
      "Epoch 472/500\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 0.3152 - acc: 0.8834 - val_loss: 0.5706 - val_acc: 0.7197\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.87261\n",
      "Epoch 473/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3264 - acc: 0.8812 - val_loss: 0.5886 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.87261\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3344 - acc: 0.8734 - val_loss: 0.6240 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.87261\n",
      "Epoch 475/500\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.3217 - acc: 0.8850 - val_loss: 0.5201 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00475: val_acc did not improve from 0.87261\n",
      "Epoch 476/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3198 - acc: 0.8831 - val_loss: 0.7054 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.87261\n",
      "Epoch 477/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3039 - acc: 0.8903 - val_loss: 0.5631 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.87261\n",
      "Epoch 478/500\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 0.3263 - acc: 0.8784 - val_loss: 0.4162 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.87261\n",
      "Epoch 479/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3353 - acc: 0.8797 - val_loss: 0.6614 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.87261\n",
      "Epoch 480/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3131 - acc: 0.8903 - val_loss: 0.6459 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.87261\n",
      "Epoch 481/500\n",
      "100/100 [==============================] - 54s 544ms/step - loss: 0.3105 - acc: 0.8856 - val_loss: 0.6587 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.87261\n",
      "Epoch 482/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3016 - acc: 0.8909 - val_loss: 0.4147 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.87261\n",
      "Epoch 483/500\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 0.3207 - acc: 0.8903 - val_loss: 0.5319 - val_acc: 0.7875\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.87261\n",
      "Epoch 484/500\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.3198 - acc: 0.8825 - val_loss: 0.5166 - val_acc: 0.7969\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.87261\n",
      "Epoch 485/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3146 - acc: 0.8866 - val_loss: 0.6812 - val_acc: 0.7134\n",
      "\n",
      "Epoch 00485: val_acc did not improve from 0.87261\n",
      "Epoch 486/500\n",
      "100/100 [==============================] - 53s 533ms/step - loss: 0.3306 - acc: 0.8803 - val_loss: 0.5135 - val_acc: 0.7844\n",
      "\n",
      "Epoch 00486: val_acc did not improve from 0.87261\n",
      "Epoch 487/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3015 - acc: 0.8928 - val_loss: 0.6754 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00487: val_acc did not improve from 0.87261\n",
      "Epoch 488/500\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.3270 - acc: 0.8809 - val_loss: 0.5537 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00488: val_acc did not improve from 0.87261\n",
      "Epoch 489/500\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.3269 - acc: 0.8768 - val_loss: 0.6116 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00489: val_acc did not improve from 0.87261\n",
      "Epoch 490/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3247 - acc: 0.8775 - val_loss: 0.6564 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00490: val_acc did not improve from 0.87261\n",
      "Epoch 491/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3025 - acc: 0.8909 - val_loss: 0.4987 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00491: val_acc did not improve from 0.87261\n",
      "Epoch 492/500\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.3175 - acc: 0.8818 - val_loss: 0.7424 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00492: val_acc did not improve from 0.87261\n",
      "Epoch 493/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3128 - acc: 0.8891 - val_loss: 0.6888 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00493: val_acc did not improve from 0.87261\n",
      "Epoch 494/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3083 - acc: 0.8891 - val_loss: 0.7609 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00494: val_acc did not improve from 0.87261\n",
      "Epoch 495/500\n",
      "100/100 [==============================] - 53s 530ms/step - loss: 0.2980 - acc: 0.8927 - val_loss: 0.7561 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00495: val_acc did not improve from 0.87261\n",
      "Epoch 496/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3291 - acc: 0.8762 - val_loss: 0.6003 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00496: val_acc did not improve from 0.87261\n",
      "Epoch 497/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3142 - acc: 0.8800 - val_loss: 0.5211 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00497: val_acc did not improve from 0.87261\n",
      "Epoch 498/500\n",
      "100/100 [==============================] - 54s 539ms/step - loss: 0.3042 - acc: 0.8924 - val_loss: 0.6581 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00498: val_acc did not improve from 0.87261\n",
      "Epoch 499/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3145 - acc: 0.8825 - val_loss: 0.6156 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00499: val_acc did not improve from 0.87261\n",
      "Epoch 500/500\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.3047 - acc: 0.8903 - val_loss: 0.6940 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00500: val_acc did not improve from 0.87261\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"200106_total16\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# early = EarlyStopping(monitor='acc', patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=500,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x140d9c61630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.707942464226406,\n",
       " 0.72,\n",
       " 0.71375,\n",
       " 0.7132582863171896,\n",
       " 0.69875,\n",
       " 0.72625,\n",
       " 0.7191994995381997,\n",
       " 0.7153125,\n",
       " 0.7190625,\n",
       " 0.7066916821896098,\n",
       " 0.7171875,\n",
       " 0.7259375,\n",
       " 0.7263914948332824,\n",
       " 0.700625,\n",
       " 0.730625,\n",
       " 0.7120075045413267,\n",
       " 0.728125,\n",
       " 0.7265625,\n",
       " 0.729831144316186,\n",
       " 0.7175,\n",
       " 0.7309375,\n",
       " 0.7270168854043662,\n",
       " 0.75,\n",
       " 0.7185741088552875,\n",
       " 0.7271875,\n",
       " 0.731875,\n",
       " 0.7260787991004262,\n",
       " 0.725625,\n",
       " 0.74125,\n",
       " 0.7329580989608622,\n",
       " 0.7384375,\n",
       " 0.730625,\n",
       " 0.7414008756963218,\n",
       " 0.733125,\n",
       " 0.7484375,\n",
       " 0.7423389617393283,\n",
       " 0.7425,\n",
       " 0.7534375,\n",
       " 0.7420262662674055,\n",
       " 0.7446875,\n",
       " 0.7578125,\n",
       " 0.7557848654291345,\n",
       " 0.7546875,\n",
       " 0.7471875,\n",
       " 0.7523452158716785,\n",
       " 0.7446875,\n",
       " 0.7567229516957983,\n",
       " 0.7553125,\n",
       " 0.7425,\n",
       " 0.7539086930449118,\n",
       " 0.75,\n",
       " 0.766875,\n",
       " 0.7545340836532717,\n",
       " 0.7546875,\n",
       " 0.76375,\n",
       " 0.7736085054649272,\n",
       " 0.758125,\n",
       " 0.769375,\n",
       " 0.7742338963714968,\n",
       " 0.758125,\n",
       " 0.7646875,\n",
       " 0.7739212008995738,\n",
       " 0.7671875,\n",
       " 0.7603125,\n",
       " 0.7848655407767209,\n",
       " 0.7715625,\n",
       " 0.7736085054649272,\n",
       " 0.7740625,\n",
       " 0.7753125,\n",
       " 0.7798624141578603,\n",
       " 0.7759375,\n",
       " 0.775625,\n",
       " 0.7748592872035138,\n",
       " 0.779375,\n",
       " 0.778125,\n",
       " 0.7717323328943234,\n",
       " 0.789375,\n",
       " 0.7803125,\n",
       " 0.7707942463667263,\n",
       " 0.7846875,\n",
       " 0.7809375,\n",
       " 0.7792370229530812,\n",
       " 0.784375,\n",
       " 0.7834375,\n",
       " 0.7842401502056372,\n",
       " 0.7896875,\n",
       " 0.778125,\n",
       " 0.7845528456402839,\n",
       " 0.7825,\n",
       " 0.7826766728087468,\n",
       " 0.769375,\n",
       " 0.8046875,\n",
       " 0.7892432771599837,\n",
       " 0.7903125,\n",
       " 0.7846875,\n",
       " 0.7901813634639236,\n",
       " 0.7903125,\n",
       " 0.7925,\n",
       " 0.80831769837522,\n",
       " 0.786875,\n",
       " 0.8034375,\n",
       " 0.7864290179872304,\n",
       " 0.7909375,\n",
       " 0.7915625,\n",
       " 0.8045653531594601,\n",
       " 0.8075,\n",
       " 0.795625,\n",
       " 0.7964352721568567,\n",
       " 0.785625,\n",
       " 0.7934375,\n",
       " 0.7951844904182701,\n",
       " 0.8028125,\n",
       " 0.8030018762844365,\n",
       " 0.80125,\n",
       " 0.80375,\n",
       " 0.8092557846791599,\n",
       " 0.7896875,\n",
       " 0.8153125,\n",
       " 0.8089430892072371,\n",
       " 0.8084375,\n",
       " 0.8046875,\n",
       " 0.8133208255904999,\n",
       " 0.7990625,\n",
       " 0.8059375,\n",
       " 0.7983114447274605,\n",
       " 0.8121875,\n",
       " 0.81625,\n",
       " 0.8020637899804965,\n",
       " 0.808125,\n",
       " 0.808125,\n",
       " 0.8020637897195632,\n",
       " 0.8121875,\n",
       " 0.8101938710203761,\n",
       " 0.804375,\n",
       " 0.8228125,\n",
       " 0.8183239523211892,\n",
       " 0.81125,\n",
       " 0.8046875,\n",
       " 0.815196998235656,\n",
       " 0.8065625,\n",
       " 0.8165625,\n",
       " 0.8167604754088893,\n",
       " 0.8096875,\n",
       " 0.8125,\n",
       " 0.8214509069285891,\n",
       " 0.8284375,\n",
       " 0.810625,\n",
       " 0.8245778612377794,\n",
       " 0.8121875,\n",
       " 0.8203125,\n",
       " 0.8111319572870399,\n",
       " 0.831875,\n",
       " 0.8128125,\n",
       " 0.8139462161615836,\n",
       " 0.8253125,\n",
       " 0.8198874294944224,\n",
       " 0.8215625,\n",
       " 0.8303125,\n",
       " 0.8180112571474759,\n",
       " 0.82875,\n",
       " 0.8115625,\n",
       " 0.82020012511545,\n",
       " 0.8228125,\n",
       " 0.8303125,\n",
       " 0.8286429015899763,\n",
       " 0.8184375,\n",
       " 0.8303125,\n",
       " 0.8252032521070727,\n",
       " 0.8334375,\n",
       " 0.8234375,\n",
       " 0.8289555973228325,\n",
       " 0.834375,\n",
       " 0.8171875,\n",
       " 0.8330206380105153,\n",
       " 0.81875,\n",
       " 0.8334375,\n",
       " 0.8258286427154327,\n",
       " 0.830625,\n",
       " 0.8264540338456593,\n",
       " 0.83,\n",
       " 0.8265625,\n",
       " 0.8361475923569818,\n",
       " 0.823125,\n",
       " 0.834375,\n",
       " 0.8295809882666783,\n",
       " 0.83,\n",
       " 0.8203125,\n",
       " 0.8373983740955684,\n",
       " 0.8315625,\n",
       " 0.836875,\n",
       " 0.8286429019627384,\n",
       " 0.83375,\n",
       " 0.83625,\n",
       " 0.8330206379732391,\n",
       " 0.82125,\n",
       " 0.838125,\n",
       " 0.8327079423149352,\n",
       " 0.8353125,\n",
       " 0.85125,\n",
       " 0.8364602874561426,\n",
       " 0.8340625,\n",
       " 0.8433395870556453,\n",
       " 0.8309375,\n",
       " 0.845,\n",
       " 0.8398999374981893,\n",
       " 0.834375,\n",
       " 0.8353125,\n",
       " 0.8342714197118257,\n",
       " 0.8346875,\n",
       " 0.8315625,\n",
       " 0.8449030641916023,\n",
       " 0.8440625,\n",
       " 0.83625,\n",
       " 0.8408380238766816,\n",
       " 0.8353125,\n",
       " 0.84,\n",
       " 0.8380237649275856,\n",
       " 0.845,\n",
       " 0.8428125,\n",
       " 0.8467792367994823,\n",
       " 0.84375,\n",
       " 0.8308317697070553,\n",
       " 0.8496875,\n",
       " 0.8446875,\n",
       " 0.8417761098451358,\n",
       " 0.84375,\n",
       " 0.8396875,\n",
       " 0.8424015007889815,\n",
       " 0.843125,\n",
       " 0.84625,\n",
       " 0.8492808005748651,\n",
       " 0.8471875,\n",
       " 0.841875,\n",
       " 0.8364602874561426,\n",
       " 0.8384375,\n",
       " 0.839375,\n",
       " 0.851156973182745,\n",
       " 0.8390625,\n",
       " 0.8496875,\n",
       " 0.8574108819502305,\n",
       " 0.84125,\n",
       " 0.84375,\n",
       " 0.841463414745975,\n",
       " 0.8478125,\n",
       " 0.8480300185380689,\n",
       " 0.8503125,\n",
       " 0.8496875,\n",
       " 0.8536585363989848,\n",
       " 0.848125,\n",
       " 0.841875,\n",
       " 0.846153846302951,\n",
       " 0.8540625,\n",
       " 0.8528125,\n",
       " 0.841776109882412,\n",
       " 0.8434375,\n",
       " 0.8496875,\n",
       " 0.8561601001370915,\n",
       " 0.8484375,\n",
       " 0.8503125,\n",
       " 0.8511569732200213,\n",
       " 0.8546875,\n",
       " 0.8515625,\n",
       " 0.856785491043661,\n",
       " 0.8540625,\n",
       " 0.84125,\n",
       " 0.8580362727449714,\n",
       " 0.86,\n",
       " 0.8502188868788051,\n",
       " 0.8521875,\n",
       " 0.8575,\n",
       " 0.8580362727449714,\n",
       " 0.8575,\n",
       " 0.8390625,\n",
       " 0.853033145790625,\n",
       " 0.856875,\n",
       " 0.8484375,\n",
       " 0.8533458412252716,\n",
       " 0.8434375,\n",
       " 0.8725,\n",
       " 0.8564727952362523,\n",
       " 0.858125,\n",
       " 0.8509375,\n",
       " 0.8608505317313437,\n",
       " 0.860625,\n",
       " 0.85375,\n",
       " 0.8577235773103248,\n",
       " 0.859375,\n",
       " 0.8570981864783076,\n",
       " 0.8521875,\n",
       " 0.851875,\n",
       " 0.8542839272682782,\n",
       " 0.851875,\n",
       " 0.8615625,\n",
       " 0.8564727955717382,\n",
       " 0.8490625,\n",
       " 0.85875,\n",
       " 0.854596622926582,\n",
       " 0.8628125,\n",
       " 0.86,\n",
       " 0.8692933083922509,\n",
       " 0.8584375,\n",
       " 0.8609375,\n",
       " 0.861163227091438,\n",
       " 0.868125,\n",
       " 0.8578125,\n",
       " 0.8639774857050483,\n",
       " 0.8628125,\n",
       " 0.851875,\n",
       " 0.8555347092677982,\n",
       " 0.863125,\n",
       " 0.8633520950966883,\n",
       " 0.869375,\n",
       " 0.86375,\n",
       " 0.8577235773103248,\n",
       " 0.8615625,\n",
       " 0.8646875,\n",
       " 0.8589743590116352,\n",
       " 0.85375,\n",
       " 0.8590625,\n",
       " 0.8661663537102986,\n",
       " 0.87125,\n",
       " 0.8621875,\n",
       " 0.8602251407502218,\n",
       " 0.8646875,\n",
       " 0.8690625,\n",
       " 0.857410881652021,\n",
       " 0.8615625,\n",
       " 0.86125,\n",
       " 0.8611632267559521,\n",
       " 0.8575,\n",
       " 0.871875,\n",
       " 0.8652282674809111,\n",
       " 0.8621875,\n",
       " 0.8702313943607051,\n",
       " 0.8590625,\n",
       " 0.8678125,\n",
       " 0.8677298309208081,\n",
       " 0.8578125,\n",
       " 0.8675,\n",
       " 0.8683552217901014,\n",
       " 0.8671875,\n",
       " 0.8646875,\n",
       " 0.8671044403870006,\n",
       " 0.8703125,\n",
       " 0.8628125,\n",
       " 0.8642901814751807,\n",
       " 0.8721875,\n",
       " 0.8628125,\n",
       " 0.8686679175602339,\n",
       " 0.8740625,\n",
       " 0.8678125,\n",
       " 0.8664790492567739,\n",
       " 0.868125,\n",
       " 0.8621013134326542,\n",
       " 0.8759375,\n",
       " 0.858125,\n",
       " 0.8742964350483878,\n",
       " 0.8725,\n",
       " 0.8671875,\n",
       " 0.8667917448778015,\n",
       " 0.86875,\n",
       " 0.8696875,\n",
       " 0.8746091304830345,\n",
       " 0.8634375,\n",
       " 0.874375,\n",
       " 0.8739837398746746,\n",
       " 0.8659375,\n",
       " 0.8765625,\n",
       " 0.868355222088311,\n",
       " 0.8753125,\n",
       " 0.8696875,\n",
       " 0.8792995620400105,\n",
       " 0.8590625,\n",
       " 0.880625,\n",
       " 0.8724202627014414,\n",
       " 0.8675,\n",
       " 0.8749218262158908,\n",
       " 0.8865625,\n",
       " 0.8625,\n",
       " 0.8658536583129282,\n",
       " 0.8778125,\n",
       " 0.861875,\n",
       " 0.8664790494804311,\n",
       " 0.876875,\n",
       " 0.878125,\n",
       " 0.8677298312562939,\n",
       " 0.8759375,\n",
       " 0.876875,\n",
       " 0.8808630395114534,\n",
       " 0.8703125,\n",
       " 0.8703125,\n",
       " 0.8655409629155577,\n",
       " 0.8634375,\n",
       " 0.883125,\n",
       " 0.8755472171224602,\n",
       " 0.87625,\n",
       " 0.8746875,\n",
       " 0.8724202627387175,\n",
       " 0.87625,\n",
       " 0.8721075670058612,\n",
       " 0.8778125,\n",
       " 0.8796875,\n",
       " 0.8630393993638321,\n",
       " 0.8690625,\n",
       " 0.8803125,\n",
       " 0.8696060035659642,\n",
       " 0.8821875,\n",
       " 0.8746875,\n",
       " 0.8655409631019387,\n",
       " 0.8775,\n",
       " 0.8790625,\n",
       " 0.8733583490426574,\n",
       " 0.8796875,\n",
       " 0.8765625,\n",
       " 0.887742338738194,\n",
       " 0.880625,\n",
       " 0.8684375,\n",
       " 0.8771106939602077,\n",
       " 0.8765625,\n",
       " 0.8711694810374071,\n",
       " 0.871875,\n",
       " 0.8828125,\n",
       " 0.8789868665680877,\n",
       " 0.878125,\n",
       " 0.87125,\n",
       " 0.8736710441790945,\n",
       " 0.883125,\n",
       " 0.8759375,\n",
       " 0.8771106942956935,\n",
       " 0.881875,\n",
       " 0.88375,\n",
       " 0.8721075670431374,\n",
       " 0.8903125,\n",
       " 0.8740625,\n",
       " 0.888055034247393,\n",
       " 0.87375,\n",
       " 0.889375,\n",
       " 0.8755472168242506,\n",
       " 0.8846875,\n",
       " 0.878125,\n",
       " 0.8780487803014239,\n",
       " 0.8759375,\n",
       " 0.8877423387754702,\n",
       " 0.8865625,\n",
       " 0.8796875,\n",
       " 0.869606003528688,\n",
       " 0.880625,\n",
       " 0.8784375,\n",
       " 0.8764853030536383,\n",
       " 0.8771875,\n",
       " 0.883125,\n",
       " 0.8868042524715302,\n",
       " 0.8903125,\n",
       " 0.88,\n",
       " 0.8686679172620243,\n",
       " 0.895625,\n",
       " 0.8775,\n",
       " 0.8824265163119246,\n",
       " 0.878125,\n",
       " 0.88375,\n",
       " 0.8774233894321306,\n",
       " 0.88625,\n",
       " 0.8790625,\n",
       " 0.8852407755219541,\n",
       " 0.8746875,\n",
       " 0.8911819888920691,\n",
       " 0.8884375,\n",
       " 0.876875,\n",
       " 0.8880550342101168,\n",
       " 0.891875,\n",
       " 0.8846875,\n",
       " 0.8833646026531408,\n",
       " 0.88125,\n",
       " 0.8734375,\n",
       " 0.8849280797890978,\n",
       " 0.883125,\n",
       " 0.8903125,\n",
       " 0.8783614756987943,\n",
       " 0.8796875,\n",
       " 0.8903125,\n",
       " 0.8855534706956674,\n",
       " 0.8909375,\n",
       " 0.8903125,\n",
       " 0.8824265165728581,\n",
       " 0.8865625,\n",
       " 0.8803125,\n",
       " 0.8927454656552642,\n",
       " 0.8809375,\n",
       " 0.8767979985255611,\n",
       " 0.8775,\n",
       " 0.8909375,\n",
       " 0.8818011257408409,\n",
       " 0.8890625,\n",
       " 0.8890625,\n",
       " 0.89274546599075,\n",
       " 0.87625,\n",
       " 0.88,\n",
       " 0.8924327705561034,\n",
       " 0.8825,\n",
       " 0.8903125]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err = []\n",
    "val_err = []\n",
    "\n",
    "for i in range(len(hist.history['acc'])) :\n",
    "    train_err.append(1-hist.history['acc'][i])\n",
    "    \n",
    "for j in range(len(hist.history['val_acc'])) :\n",
    "    val_err.append(1-hist.history['acc'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = []\n",
    "val_ = []\n",
    "y_ = []\n",
    "\n",
    "for i in range(len(hist.history['acc'])) :\n",
    "    if i%10 == 0 :\n",
    "        train_.append(1-hist.history['acc'][i])\n",
    "        y_.append(i)\n",
    "        val_.append(1-hist.history['val_acc'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x141922135c0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "# acc_ax.plot(sample_, 'b', label='train acc')\n",
    "# acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('acc')\n",
    "\n",
    "loss_ax.legend(loc='best')\n",
    "acc_ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_[len(val_)-1] = 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23885350204577116, 0.26751592394652635, 0.29617834470833937, 0.30000000000000004, 0.24062499999999998, 0.20937499999999998, 0.19062500000000004, 0.25625, 0.3025477695616947, 0.305732484835728, 0.4140127384738558, 0.36875, 0.44999999999999996, 0.459375, 0.41874999999999996, 0.38749999999999996, 0.4363057332433713, 0.4108280252878833, 0.38535031923063245, 0.4125, 0.38125, 0.31562500000000004, 0.32499999999999996, 0.40625, 0.382165604715894, 0.39808916969663777, 0.289808917956747, 0.365625, 0.375, 0.22812500000000002, 0.375, 0.38749999999999996, 0.2929936298139536, 0.3630573252204118, 0.2834394915848021, 0.33125000000000004, 0.30625, 0.2875, 0.334375, 0.365625, 0.2611464968152867, 0.21019108128395814, 0.2866242026827138, 0.275, 0.23750000000000004, 0.32499999999999996, 0.275, 0.27812499999999996, 0.26751592204828933, 0.1]\n"
     ]
    }
   ],
   "source": [
    "print(val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_, train_, color='orange', label='train err')\n",
    "plt.plot(y_, sorted(val_, reverse=True), color='red', label='val err')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.savefig(\"D:/newFolder/2019/graph/train_val_err.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
